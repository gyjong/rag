"""Modular RAG implementation with flexible component-based architecture."""

from typing import List, Dict, Any, Optional, Callable
import time
import streamlit as st
from langchain_core.documents import Document
from enum import Enum

from ..utils.vector_store import VectorStoreManager
from ..utils.llm_manager import LLMManager


class ModuleType(Enum):
    """Types of modules in the Modular RAG system."""
    INDEXING = "indexing"
    PRE_RETRIEVAL = "pre_retrieval"
    RETRIEVAL = "retrieval"
    POST_RETRIEVAL = "post_retrieval"
    GENERATION = "generation"
    ORCHESTRATION = "orchestration"


class ModularRAG:
    """Modular RAG implementation with component-based architecture."""
    
    def __init__(self, vector_store_manager: VectorStoreManager, llm_manager: LLMManager):
        """Initialize Modular RAG system."""
        self.vector_store_manager = vector_store_manager
        self.llm_manager = llm_manager
        self.name = "Modular RAG"
        self.description = "ëª¨ë“ˆí˜• RAG ì‹œìŠ¤í…œ: ìœ ì—°í•œ ì»´í¬ë„ŒíŠ¸ ê¸°ë°˜ ì•„í‚¤í…ì²˜"
        
        # Initialize modules
        self.modules = {}
        self._setup_modules()
        
    def _setup_modules(self):
        """Setup default modules for the system."""
        self.modules = {
            ModuleType.PRE_RETRIEVAL: [
                self._query_expansion_module,
                self._query_classification_module
            ],
            ModuleType.RETRIEVAL: [
                self._semantic_retrieval_module,
                self._keyword_retrieval_module
            ],
            ModuleType.POST_RETRIEVAL: [
                self._relevance_filtering_module,
                self._diversity_module
            ],
            ModuleType.GENERATION: [
                self._answer_generation_module,
                self._confidence_estimation_module
            ],
            ModuleType.ORCHESTRATION: [
                self._routing_module,
                self._iteration_control_module
            ]
        }
    
    def _query_expansion_module(self, query: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Expand query with related terms."""
        expanded_terms = []
        
        # Simple keyword-based expansion
        keywords_map = {
            "AI": ["artificial intelligence", "machine learning", "ë”¥ëŸ¬ë‹"],
            "ì¸ê³µì§€ëŠ¥": ["AI", "ë¨¸ì‹ ëŸ¬ë‹", "neural network"],
            "íŠ¸ë Œë“œ": ["ë™í–¥", "ì „ë§", "trend", "future"],
            "ì—…ë¬´": ["work", "business", "ì§ì¥", "productivity"]
        }
        
        for keyword, expansions in keywords_map.items():
            if keyword.lower() in query.lower():
                expanded_terms.extend(expansions[:2])
        
        expanded_query = f"{query} {' '.join(expanded_terms)}" if expanded_terms else query
        
        return {
            "original_query": query,
            "expanded_query": expanded_query,
            "expansion_terms": expanded_terms
        }
    
    def _query_classification_module(self, query: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Classify query type for routing."""
        query_lower = query.lower()
        
        # Simple rule-based classification
        if any(word in query_lower for word in ["what", "ë¬´ì—‡", "ì–´ë–¤"]):
            query_type = "factual"
        elif any(word in query_lower for word in ["how", "ì–´ë–»ê²Œ", "ë°©ë²•"]):
            query_type = "procedural"
        elif any(word in query_lower for word in ["why", "ì™œ", "ì´ìœ "]):
            query_type = "causal"
        elif any(word in query_lower for word in ["when", "ì–¸ì œ", "ì‹œì "]):
            query_type = "temporal"
        else:
            query_type = "general"
            
        return {
            "query_type": query_type,
            "confidence": 0.8  # Simple confidence score
        }
    
    def _semantic_retrieval_module(self, query: str, context: Dict[str, Any]) -> List[Document]:
        """Perform semantic retrieval using vector similarity."""
        vector_store = self.vector_store_manager.get_vector_store()
        if vector_store is None:
            return []
            
        try:
            k = context.get("retrieval_k", 8)
            docs = vector_store.similarity_search(query, k=k)
            return docs
        except Exception as e:
            st.error(f"Semantic retrieval failed: {str(e)}")
            return []
    
    def _keyword_retrieval_module(self, query: str, context: Dict[str, Any]) -> List[Document]:
        """Perform keyword-based retrieval (simplified)."""
        # In a real implementation, this would use a keyword search engine
        # For now, we'll use semantic search with query modification
        vector_store = self.vector_store_manager.get_vector_store()
        if vector_store is None:
            return []
            
        try:
            # Extract keywords and create keyword-focused query
            keywords = [word for word in query.split() if len(word) > 2]
            keyword_query = " ".join(keywords)
            
            k = context.get("retrieval_k", 5)
            docs = vector_store.similarity_search(keyword_query, k=k)
            return docs
        except Exception:
            return []
    
    def _relevance_filtering_module(self, docs: List[Document], context: Dict[str, Any]) -> List[Document]:
        """Filter documents based on relevance threshold."""
        if not docs:
            return docs
            
        # Simple length-based filtering (in practice, would use relevance scores)
        min_length = context.get("min_doc_length", 50)
        filtered_docs = [doc for doc in docs if len(doc.page_content) >= min_length]
        
        return filtered_docs[:context.get("max_docs", 5)]
    
    def _diversity_module(self, docs: List[Document], context: Dict[str, Any]) -> List[Document]:
        """Ensure diversity in retrieved documents."""
        if len(docs) <= 3:
            return docs
            
        # Simple diversity based on source
        diverse_docs = []
        seen_sources = set()
        
        for doc in docs:
            source = doc.metadata.get("source", "unknown")
            if source not in seen_sources or len(diverse_docs) < 2:
                diverse_docs.append(doc)
                seen_sources.add(source)
                
            if len(diverse_docs) >= context.get("max_diverse_docs", 4):
                break
                
        return diverse_docs
    
    def _answer_generation_module(self, query: str, docs: List[Document], context: Dict[str, Any]) -> str:
        """Generate answer using retrieved documents."""
        if not docs:
            return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        doc_context = "\n\n".join([doc.page_content for doc in docs])
        
        # Limit context length
        max_length = context.get("max_context_length", 3500)
        if len(doc_context) > max_length:
            doc_context = doc_context[:max_length] + "..."
            
        query_type = context.get("query_type", "general")
        
        # Customize prompt based on query type
        if query_type == "factual":
            prompt_template = """ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì‚¬ì‹¤ì  ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”.
ì •í™•í•œ ì •ë³´ë§Œ ì œê³µí•˜ê³ , í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µí•´ì£¼ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸: {context}
ì§ˆë¬¸: {question}
ë‹µë³€:"""
        elif query_type == "procedural":
            prompt_template = """ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¨ê³„ë³„ ë°©ë²•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.
ê°€ëŠ¥í•˜ë©´ ìˆœì„œëŒ€ë¡œ ì •ë¦¬í•´ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸: {context}
ì§ˆë¬¸: {question}
ë‹µë³€:"""
        else:
            prompt_template = """ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”.
ë‹µë³€ì€ ëª…í™•í•˜ê³  ë„ì›€ì´ ë˜ë„ë¡ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸: {context}
ì§ˆë¬¸: {question}
ë‹µë³€:"""
        
        st.write("**ëª¨ë“ˆí˜• ë‹µë³€ ìƒì„± ì¤‘...**")
        answer_placeholder = st.empty()
        
        start_time = time.time()
        full_response = ""
        
        # Stream the response
        for chunk in self.llm_manager.generate_response_stream(
            prompt=query,
            context=doc_context
        ):
            full_response += chunk
            answer_placeholder.markdown(full_response + "â–Œ")
        
        generation_time = time.time() - start_time
        answer_placeholder.markdown(full_response)
        
        st.success(f"ëª¨ë“ˆí˜• ë‹µë³€ ìƒì„± ì™„ë£Œ ({generation_time:.2f}ì´ˆ)")
        return full_response
    
    def _confidence_estimation_module(self, answer: str, context: Dict[str, Any]) -> float:
        """Estimate confidence in the generated answer."""
        # Simple heuristic-based confidence estimation
        confidence = 0.5  # Base confidence
        
        # Increase confidence based on answer length and structure
        if len(answer) > 50:
            confidence += 0.2
        if len(answer) > 100:
            confidence += 0.1
            
        # Decrease confidence for uncertainty indicators
        uncertainty_words = ["ëª¨ë¥¸ë‹¤", "í™•ì‹¤í•˜ì§€", "ì•„ë§ˆë„", "maybe", "uncertain"]
        if any(word in answer.lower() for word in uncertainty_words):
            confidence -= 0.3
            
        # Increase confidence if answer has specific details
        if any(word in answer for word in ["2024", "2025", "êµ¬ì²´ì ", "specific"]):
            confidence += 0.1
            
        return max(0.0, min(1.0, confidence))
    
    def _routing_module(self, query: str, context: Dict[str, Any]) -> str:
        """Route query to appropriate processing path."""
        query_type = context.get("query_type", "general")
        
        if query_type == "factual":
            return "precise_path"
        elif query_type == "procedural":
            return "step_by_step_path"
        elif query_type == "causal":
            return "reasoning_path"
        else:
            return "standard_path"
    
    def _iteration_control_module(self, context: Dict[str, Any]) -> bool:
        """Control whether to iterate or stop."""
        confidence = context.get("confidence", 0.0)
        iteration_count = context.get("iteration_count", 0)
        max_iterations = context.get("max_iterations", 2)
        
        # Continue if confidence is low and we haven't exceeded max iterations
        return confidence < 0.7 and iteration_count < max_iterations
    
    def query(self, question: str, max_iterations: int = 2) -> Dict[str, Any]:
        """Process query using modular approach."""
        start_time = time.time()
        context = {
            "iteration_count": 0,
            "max_iterations": max_iterations,
            "retrieval_k": 8,
            "max_docs": 5,
            "max_context_length": 3500
        }
        
        st.subheader("ğŸ§© ëª¨ë“ˆí˜• RAG ì²˜ë¦¬ ê³¼ì •")
        
        # Step 1: Pre-retrieval processing
        st.write("**1ë‹¨ê³„: ì‚¬ì „ ê²€ìƒ‰ ì²˜ë¦¬**")
        
        # Query expansion
        expansion_result = self._query_expansion_module(question, context)
        context.update(expansion_result)
        if expansion_result["expansion_terms"]:
            st.info(f"ì¿¼ë¦¬ í™•ì¥: {expansion_result['expanded_query']}")
        
        # Query classification
        classification_result = self._query_classification_module(question, context)
        context.update(classification_result)
        st.info(f"ì¿¼ë¦¬ ìœ í˜•: {classification_result['query_type']}")
        
        # Routing
        processing_path = self._routing_module(question, context)
        context["processing_path"] = processing_path
        st.info(f"ì²˜ë¦¬ ê²½ë¡œ: {processing_path}")
        
        final_answer = ""
        all_retrieved_docs = []
        
        # Iterative processing
        while context["iteration_count"] < max_iterations:
            iteration = context["iteration_count"] + 1
            st.write(f"**ë°˜ë³µ {iteration}:**")
            
            # Step 2: Retrieval
            with st.spinner("ë¬¸ì„œ ê²€ìƒ‰ ì¤‘..."):
                query_to_use = context.get("expanded_query", question)
                
                # Combine semantic and keyword retrieval
                semantic_docs = self._semantic_retrieval_module(query_to_use, context)
                keyword_docs = self._keyword_retrieval_module(query_to_use, context)
                
                # Merge and deduplicate
                all_docs = semantic_docs + keyword_docs
                seen_content = set()
                unique_docs = []
                for doc in all_docs:
                    content_hash = hash(doc.page_content[:100])
                    if content_hash not in seen_content:
                        unique_docs.append(doc)
                        seen_content.add(content_hash)
                
                st.success(f"ê²€ìƒ‰ ì™„ë£Œ: {len(unique_docs)}ê°œ ë¬¸ì„œ")
            
            # Step 3: Post-retrieval processing
            filtered_docs = self._relevance_filtering_module(unique_docs, context)
            diverse_docs = self._diversity_module(filtered_docs, context)
            all_retrieved_docs.extend(diverse_docs)
            
            st.info(f"í›„ì²˜ë¦¬ ì™„ë£Œ: {len(diverse_docs)}ê°œ ë¬¸ì„œ ì„ íƒ")
            
            # Step 4: Generation
            with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                answer = self._answer_generation_module(question, diverse_docs, context)
                
            # Step 5: Confidence estimation
            confidence = self._confidence_estimation_module(answer, context)
            context["confidence"] = confidence
            context["iteration_count"] = iteration
            
            st.info(f"ì‹ ë¢°ë„: {confidence:.2f}")
            
            # Check if we should continue iterating
            if not self._iteration_control_module(context):
                final_answer = answer
                break
            else:
                st.warning(f"ì‹ ë¢°ë„ê°€ ë‚®ì•„ ë‹¤ìŒ ë°˜ë³µì„ ì‹œë„í•©ë‹ˆë‹¤ (ì‹ ë¢°ë„: {confidence:.2f})")
                # Adjust parameters for next iteration
                context["retrieval_k"] = min(context["retrieval_k"] + 2, 15)
                final_answer = answer
        
        # Display final results
        with st.expander(f"ìµœì¢… ê²€ìƒ‰ëœ ë¬¸ì„œ ({len(all_retrieved_docs)}ê°œ)"):
            for i, doc in enumerate(all_retrieved_docs[-5:]):  # Show last 5
                st.write(f"**ë¬¸ì„œ {i+1}:**")
                st.write(f"ì¶œì²˜: {doc.metadata.get('source', 'Unknown')}")
                st.write(f"ë‚´ìš©: {doc.page_content[:200]}...")
                st.divider()
        
        total_time = time.time() - start_time
        
        return {
            "question": question,
            "answer": final_answer,
            "retrieved_docs": all_retrieved_docs,
            "total_time": total_time,
            "rag_type": self.name,
            "metadata": {
                "iterations": context["iteration_count"],
                "final_confidence": context.get("confidence", 0.0),
                "query_type": context.get("query_type", "general"),
                "processing_path": context.get("processing_path", "standard"),
                "total_retrieved": len(all_retrieved_docs),
                "expansion_terms": context.get("expansion_terms", [])
            }
        }
    
    def get_system_info(self) -> Dict[str, Any]:
        """Get information about the Modular RAG system."""
        return {
            "name": self.name,
            "description": self.description,
            "components": [
                "Pre-retrieval Modules (Query Expansion, Classification)",
                "Retrieval Modules (Semantic + Keyword)",
                "Post-retrieval Modules (Filtering, Diversity)",
                "Generation Modules (Answer Generation, Confidence)",
                "Orchestration Modules (Routing, Iteration Control)"
            ],
            "features": [
                "ëª¨ë“ˆí˜• ì•„í‚¤í…ì²˜",
                "ì¿¼ë¦¬ ìœ í˜•ë³„ ë¼ìš°íŒ…",
                "ë°˜ë³µì  ê°œì„ ",
                "ì‹ ë¢°ë„ ê¸°ë°˜ ì œì–´",
                "ë‹¤ì¤‘ ê²€ìƒ‰ ì „ëµ"
            ],
            "advantages": [
                "ë†’ì€ ìœ ì—°ì„±ê³¼ í™•ì¥ì„±",
                "ìƒí™©ë³„ ìµœì í™”",
                "ì ì§„ì  í’ˆì§ˆ ê°œì„ ",
                "íˆ¬ëª…í•œ ì²˜ë¦¬ ê³¼ì •"
            ]
        } 