"""Modular RAG utilities with a flexible component-based architecture."""

import logging
from typing import List, Dict, Any, Iterator, Tuple
import re
import numpy as np
from langchain_core.documents import Document
from enum import Enum

from ..utils.vector_store import VectorStoreManager
from ..utils.llm_manager import LLMManager
from ..config.settings import CONFIDENCE_THRESHOLD

logger = logging.getLogger(__name__)

# --- BM25 Implementation (Utility Class) ---
class BM25:
    """BM25 ranking algorithm implementation."""
    def __init__(self, corpus: List[str], k1: float = 1.5, b: float = 0.75):
        self.k1 = k1
        self.b = b
        self.corpus = corpus or []
        self.corpus_size = len(self.corpus)
        
        if self.corpus_size == 0:
            self._initialize_empty()
            return

        try:
            self.doc_tokens = [self._tokenize(doc) if doc else [] for doc in self.corpus]
            self.doc_len = [len(tokens) for tokens in self.doc_tokens]
            self.avgdl = sum(self.doc_len) / self.corpus_size if self.corpus_size > 0 else 0
            
            self.vocab = list(set(token for tokens in self.doc_tokens for token in tokens))
            
            self.df = {term: sum(1 for tokens in self.doc_tokens if term in tokens) for term in self.vocab}
            self.idf = {term: np.log((self.corpus_size - self.df.get(term, 0) + 0.5) / (self.df.get(term, 0) + 0.5) + 1) for term in self.vocab}

        except Exception as e:
            logger.error(f"BM25 initialization failed: {e}", exc_info=True)
            self._initialize_empty()
            raise ValueError(f"BM25 ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")

    def _initialize_empty(self):
        self.doc_tokens = []
        self.doc_len = []
        self.avgdl = 0
        self.vocab = []
        self.df = {}
        self.idf = {}

    def _tokenize(self, text: str) -> List[str]:
        if not text or not isinstance(text, str): return []
        text = text.lower()
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        return [token for token in text.split() if len(token) > 1]

    def get_scores(self, query: str) -> List[float]:
        if self.corpus_size == 0: return []
        query_tokens = self._tokenize(query)
        if not query_tokens: return [0.0] * self.corpus_size

        scores = np.zeros(self.corpus_size)
        for term in query_tokens:
            if term not in self.vocab: continue
            
            q_freq = np.array([doc.count(term) for doc in self.doc_tokens])
            
            numerator = q_freq * (self.k1 + 1)
            denominator = q_freq + self.k1 * (1 - self.b + self.b * np.array(self.doc_len) / (self.avgdl + 1e-8))
            
            scores += self.idf.get(term, 0) * (numerator / (denominator + 1e-8))
            
        return scores.tolist()

    def get_top_k_indices(self, query: str, k: int = 5) -> List[Tuple[int, float]]:
        scores = self.get_scores(query)
        if not scores: return []
        
        top_indices = np.argsort(scores)[::-1][:k]
        return [(i, scores[i]) for i in top_indices if scores[i] > 0]

# --- RAG Module Definitions ---
class ModuleType(Enum):
    INDEXING = "indexing"
    PRE_RETRIEVAL = "pre_retrieval"
    RETRIEVAL = "retrieval"
    POST_RETRIEVAL = "post_retrieval"
    GENERATION = "generation"
    ORCHESTRATION = "orchestration"

# --- Pre-retrieval Modules ---
def expand_query(query: str) -> Dict[str, Any]:
    """Expand query with related terms."""
    keywords_map = {
        # Technology & AI
        "AI": ["artificial intelligence", "machine learning", "deep learning", "neural networks", "automation"],
        "ì¸ê³µì§€ëŠ¥": ["AI", "ë¨¸ì‹ ëŸ¬ë‹", "ë”¥ëŸ¬ë‹", "ì‹ ê²½ë§", "ìë™í™”"],
        "ë¨¸ì‹ ëŸ¬ë‹": ["machine learning", "ML", "AI", "ì¸ê³µì§€ëŠ¥", "ë°ì´í„° ë¶„ì„"],
        "ë”¥ëŸ¬ë‹": ["deep learning", "neural networks", "AI", "ë¨¸ì‹ ëŸ¬ë‹"],
        
        # Business & Trends
        "íŠ¸ë Œë“œ": ["ë™í–¥", "ì „ë§", "ì¶”ì„¸", "ë°©í–¥ì„±", "ë¯¸ë˜"],
        "ë™í–¥": ["trend", "íŠ¸ë Œë“œ", "ì „ë§", "ì¶”ì„¸", "ë°©í–¥ì„±"],
        "ì „ë§": ["outlook", "forecast", "ì˜ˆì¸¡", "ë¯¸ë˜", "íŠ¸ë Œë“œ"],
        "ì‹œì¥": ["market", "business", "ì‚°ì—…", "ê²½ì œ", "ìƒì—…"],
        
        # Time & Future
        "ë¯¸ë˜": ["future", "ì „ë§", "í–¥í›„", "ì•ìœ¼ë¡œ", "ë‹¤ê°€ì˜¬"],
        "2025ë…„": ["2025", "ì˜¬í•´", "í˜„ì¬", "ìµœì‹ ", "ìµœê·¼"],
        "ì˜¬í•´": ["2025", "í˜„ì¬", "ìµœì‹ ", "ìµœê·¼", "ì´ë²ˆë…„ë„"],
        
        # Work & Productivity
        "ì—…ë¬´": ["work", "business", "ì§ë¬´", "ì¼", "ì—…ë¬´í™˜ê²½"],
        "ìƒì‚°ì„±": ["productivity", "efficiency", "íš¨ìœ¨ì„±", "ì„±ê³¼", "ì—…ë¬´íš¨ìœ¨"],
        "ì§ì¥": ["workplace", "office", "íšŒì‚¬", "ì§ì¥í™˜ê²½", "ì—…ë¬´í™˜ê²½"],
        
        # Industry & Sectors
        "ê¸°ì—…": ["company", "corporation", "business", "íšŒì‚¬", "ê¸°ì—…í™˜ê²½"],
        "ì‚°ì—…": ["industry", "sector", "ë¶„ì•¼", "ì—…ê³„", "ì‚°ì—…ê³„"],
        "ìŠ¤íƒ€íŠ¸ì—…": ["startup", "ì‹ ìƒê¸°ì—…", "ë²¤ì²˜", "ì°½ì—…", "ì‹ ê·œê¸°ì—…"],
        
        # Digital & Technology
        "ë””ì§€í„¸": ["digital", "ì˜¨ë¼ì¸", "ì „ì", "ì •ë³´ê¸°ìˆ ", "IT"],
        "ì˜¨ë¼ì¸": ["online", "ì¸í„°ë„·", "ì›¹", "ë””ì§€í„¸", "ê°€ìƒ"],
        "ëª¨ë°”ì¼": ["mobile", "ìŠ¤ë§ˆíŠ¸í°", "ì•±", "íœ´ëŒ€ìš©", "ì´ë™"],
        
        # Data & Analytics
        "ë°ì´í„°": ["data", "ì •ë³´", "ë¶„ì„", "í†µê³„", "ì¸ì‚¬ì´íŠ¸"],
        "ë¶„ì„": ["analysis", "analytics", "ë°ì´í„°ë¶„ì„", "í†µê³„", "ì—°êµ¬"],
        "í†µê³„": ["statistics", "ë°ì´í„°", "ìˆ˜ì¹˜", "ë¶„ì„", "ì¡°ì‚¬"],
        
        # Innovation & Development
        "í˜ì‹ ": ["innovation", "ì°½ì˜ì„±", "ìƒˆë¡œìš´", "ë°œì „", "ê°œì„ "],
        "ê°œë°œ": ["development", "ì—°êµ¬", "ê°œë°œ", "ê¸°ìˆ ê°œë°œ", "ì œí’ˆê°œë°œ"],
        "ì—°êµ¬": ["research", "ì¡°ì‚¬", "ë¶„ì„", "ê°œë°œ", "íƒêµ¬"],
        
        # Communication & Collaboration
        "ì†Œí†µ": ["communication", "ì˜ì‚¬ì†Œí†µ", "ëŒ€í™”", "í˜‘ì—…", "íŒ€ì›Œí¬"],
        "í˜‘ì—…": ["collaboration", "íŒ€ì›Œí¬", "í˜‘ë ¥", "ê³µë™ì‘ì—…", "ì†Œí†µ"],
        "íšŒì˜": ["meeting", "ì»¨í¼ëŸ°ìŠ¤", "í† ë¡ ", "í˜‘ì˜", "ì†Œí†µ"],
        
        # Skills & Learning
        "ê¸°ìˆ ": ["skill", "ëŠ¥ë ¥", "ì „ë¬¸ì„±", "ì—­ëŸ‰", "ì‹¤ë ¥"],
        "í•™ìŠµ": ["learning", "êµìœ¡", "í›ˆë ¨", "ê°œë°œ", "ì„±ì¥"],
        "êµìœ¡": ["education", "í•™ìŠµ", "í›ˆë ¨", "ê°•ì˜", "êµìœ¡ê³¼ì •"],
        
        # Environment & Culture
        "í™˜ê²½": ["environment", "ìƒí™©", "ì¡°ê±´", "ë¶„ìœ„ê¸°", "ë¬¸í™”"],
        "ë¬¸í™”": ["culture", "ì „í†µ", "ê°€ì¹˜ê´€", "ìŠµê´€", "í™˜ê²½"],
        "ë³€í™”": ["change", "ë³€í™”", "ì „í™˜", "ë°œì „", "í˜ì‹ "]
    }
    expanded_terms = []
    for keyword, expansions in keywords_map.items():
        if keyword.lower() in query.lower():
            expanded_terms.extend(expansions)
            if len(expanded_terms) >= 4: break
    
    unique_terms = list(dict.fromkeys(expanded_terms))[:4]
    enhanced_query = f"{query} {' '.join(unique_terms)}" if unique_terms else query
    
    return {"expanded_query": enhanced_query, "expansion_terms": unique_terms}

def classify_query(query: str) -> Dict[str, Any]:
    """Classify the query type with confidence scoring."""
    query_lower = query.lower().strip()
    patterns = {
        "factual": (["ë¬´ì—‡", "what", "ì–´ë–¤"], ["ì •ì˜", "ì„¤ëª…"], 1.0),
        "procedural": (["ì–´ë–»ê²Œ", "how", "ë°©ë²•"], ["ê³¼ì •", "ì ˆì°¨"], 1.0),
        "causal": (["ì™œ", "why", "ì´ìœ "], ["ì›ì¸", "ê²°ê³¼"], 1.0),
        "temporal": (["ì–¸ì œ", "when"], ["ë…„ë„", "ë¯¸ë˜", "ê³¼ê±°"], 1.0),
        "comparative": (["ë¹„êµ", "ì°¨ì´", "vs"], ["ì¥ë‹¨ì "], 1.0),
        "quantitative": (["ì–¼ë§ˆ", "ìˆ˜ëŸ‰", "ê°œìˆ˜"], ["í†µê³„", "%"], 1.0),
        "general": (["ëŒ€í•´", "ê´€ë ¨", "about"], ["ì„¤ëª…", "ì•Œë ¤ì¤˜"], 0.8)
    }
    scores = {}
    for q_type, (primary, secondary, weight) in patterns.items():
        score = sum(1.0 for p in primary if p in query_lower) + sum(0.5 for p in secondary if p in query_lower)
        scores[q_type] = score * weight

    if max(scores.values()) > 0:
        query_type = max(scores, key=scores.get)
        confidence = min(0.95, 0.3 + (scores[query_type] * 0.3))
    else:
        query_type = "general"
        confidence = 0.5
        
    return {"query_type": query_type, "confidence": confidence, "classification_scores": scores}

# --- Retrieval Modules ---
def retrieve_semantic(vector_store_manager: VectorStoreManager, query: str, k: int) -> List[Document]:
    """Perform semantic retrieval using vector similarity."""
    vector_store = vector_store_manager.get_vector_store()
    if not vector_store: return []
    try:
        return vector_store.similarity_search(query, k=k)
    except Exception as e:
        logger.error(f"Semantic retrieval failed: {e}", exc_info=True)
        return []

def retrieve_keyword(bm25_index: BM25, all_bm25_docs: List[Document], query: str, k: int) -> List[Document]:
    """Perform BM25-based keyword retrieval."""
    if not bm25_index or not all_bm25_docs: return []
    try:
        top_results = bm25_index.get_top_k_indices(query, k=k)
        return [all_bm25_docs[i] for i, score in top_results]
    except Exception as e:
        logger.error(f"Keyword retrieval failed: {e}", exc_info=True)
        return []

# --- Post-retrieval Modules ---
def filter_and_diversify(docs: List[Document], max_docs: int = 5) -> List[Document]:
    """Filter by relevance (simple length check) and ensure diversity by source."""
    if not docs: return []
    
    # Filter by min length
    filtered = [doc for doc in docs if len(doc.page_content) >= 50]
    
    # Diversify by source
    if len(filtered) <= 3: return filtered[:max_docs]
        
    diverse_docs, seen_sources = [], set()
    for doc in filtered:
        source = doc.metadata.get("source", "unknown")
        if source not in seen_sources or len(diverse_docs) < 2:
            diverse_docs.append(doc)
            seen_sources.add(source)
        if len(diverse_docs) >= max_docs: break
            
    # If not enough diverse docs, fill with remaining filtered docs
    if len(diverse_docs) < max_docs:
        remaining_docs = [doc for doc in filtered if doc not in diverse_docs]
        diverse_docs.extend(remaining_docs[:max_docs - len(diverse_docs)])

    return diverse_docs

# --- Generation Modules ---
def generate_answer_stream(llm_manager: LLMManager, query: str, docs: List[Document], query_type: str, max_context_length: int = 3500) -> Iterator[str]:
    """Generate an answer stream based on the query type and context."""
    if not docs:
        yield "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        return
        
    context = "\\n\\n".join([doc.page_content for doc in docs])
    if len(context) > max_context_length:
        context = context[:max_context_length] + "..."
        
    prompt_templates = {
        "factual": "ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì‚¬ì‹¤ ì •ë³´ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”. ì •í™•í•˜ì§€ ì•Šìœ¼ë©´ 'í™•ì‹¤í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤'ë¼ê³  ë‹µí•˜ì„¸ìš”.",
        "procedural": "ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¨ê³„ë³„ ë°©ë²•ì„ ì²´ê³„ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "causal": "ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì›ì¸ê³¼ ì´ìœ ë¥¼ ë…¼ë¦¬ì ìœ¼ë¡œ ë¶„ì„í•´ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”.",
        "default": "ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…í•©ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."
    }
    system_prompt = prompt_templates.get(query_type, prompt_templates["default"])
    enhanced_prompt = f"{system_prompt}\\n\\nì»¨í…ìŠ¤íŠ¸:\\n{context}\\n\\nì§ˆë¬¸: {query}\\n\\në‹µë³€:"
    
    try:
        for chunk in llm_manager.generate_response_stream(prompt=enhanced_prompt, context=""):
            yield chunk
    except Exception as e:
        logger.error(f"Answer generation failed: {e}", exc_info=True)
        yield "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

def estimate_confidence(llm_manager: LLMManager, answer: str, docs: List[Document], classification_confidence: float) -> float:
    """
    Estimate confidence by checking for uncertainty and factual consistency (groundedness).
    """
    logger.info("ğŸ” === CONFIDENCE CALCULATION DEBUG ===")
    
    # 1. Base score from query classification and answer length
    base_score = 0.3
    classification_bonus = classification_confidence * 0.1
    length_bonus = 0.1 if len(answer) > 50 else 0.0
    
    confidence = base_score + classification_bonus + length_bonus
    
    logger.info(f"ğŸ“Š Base Score Calculation:")
    logger.info(f"   â”œâ”€ Base score: {base_score}")
    logger.info(f"   â”œâ”€ Classification confidence: {classification_confidence:.3f} â†’ bonus: {classification_bonus:.3f}")
    logger.info(f"   â”œâ”€ Answer length: {len(answer)} chars â†’ bonus: {length_bonus}")
    logger.info(f"   â””â”€ Subtotal: {confidence:.3f}")

    # 2. Penalize for uncertainty keywords
    uncertainty_keywords = ["ëª¨ë¥¸ë‹¤", "í™•ì‹¤í•˜ì§€", "ì•„ë§ˆë„"]
    found_uncertainty_words = [word for word in uncertainty_keywords if word in answer.lower()]
    uncertainty_penalty = 0.3 if found_uncertainty_words else 0.0
    
    confidence -= uncertainty_penalty
    
    logger.info(f"ğŸš« Uncertainty Penalty:")
    logger.info(f"   â”œâ”€ Found uncertainty words: {found_uncertainty_words}")
    logger.info(f"   â”œâ”€ Penalty: -{uncertainty_penalty}")
    logger.info(f"   â”œâ”€ Answer preview: '{answer[:100]}{'...' if len(answer) > 100 else ''}'")
    logger.info(f"   â””â”€ After penalty: {confidence:.3f}")

    # 3. Check for factual consistency using an LLM call
    faithfulness_bonus = 0.0
    if docs:
        context = "\\n\\n".join([doc.page_content for doc in docs])
        prompt = f"""
        Please act as a fact-checker. You will be given a context and a statement.
        Your task is to determine if the statement is fully supported by the information in the context.
        Answer with only "Yes" or "No".

        Context:
        ---
        {context}
        ---

        Statement:
        ---
        {answer}
        ---

        Is the statement fully supported by the context? Answer "Yes" or "No".
        """
        try:
            faithfulness_response = llm_manager.generate_response(prompt=prompt, context="").strip().lower()
            logger.info(f"Faithfulness check response: {faithfulness_response}")
            if "yes" in faithfulness_response:
                faithfulness_bonus = 0.3  # Big bonus for being grounded
            else:
                faithfulness_bonus = -0.4 # Big penalty for hallucination
        except Exception as e:
            logger.error(f"Faithfulness check failed: {e}")
            faithfulness_bonus = -0.1 # Small penalty if check fails
    else:
        logger.info("No documents available for faithfulness check")

    confidence += faithfulness_bonus
    final_confidence = max(0.0, min(1.0, confidence))
    
    logger.info(f"âœ… Faithfulness Check:")
    logger.info(f"   â”œâ”€ Documents available: {len(docs) if docs else 0}")
    logger.info(f"   â”œâ”€ Faithfulness bonus/penalty: {faithfulness_bonus:+.3f}")
    logger.info(f"   â””â”€ After faithfulness: {confidence:.3f}")
    
    logger.info(f"ğŸ¯ Final Confidence Calculation:")
    logger.info(f"   â”œâ”€ Before clamping: {confidence:.3f}")
    logger.info(f"   â”œâ”€ After clamping [0.0, 1.0]: {final_confidence:.3f}")
    logger.info(f"   â””â”€ Formula: 0.3 + ({classification_confidence:.3f}*0.1) + {length_bonus} - {uncertainty_penalty} + {faithfulness_bonus:+.3f} = {final_confidence:.3f}")
    
    logger.info("ğŸ” === END CONFIDENCE DEBUG ===")
    
    return final_confidence

# --- Orchestration Modules ---
def check_iteration_stop(confidence: float, iteration: int, max_iterations: int) -> bool:
    """
    Control whether to iterate or stop.
    Stops if confidence is high OR if the number of completed iterations reaches max_iterations - 1.
    """
    # iteration is 0-indexed (0 means 1st iteration completed)
    return confidence >= CONFIDENCE_THRESHOLD or iteration >= max_iterations - 1

# --- System Info ---
def get_modular_rag_system_info() -> Dict[str, Any]:
    """Get information about the Modular RAG system."""
    return {
        "name": "Modular RAG",
        "description": "ëª¨ë“ˆí˜• RAG ì‹œìŠ¤í…œ: ìœ ì—°í•œ ì»´í¬ë„ŒíŠ¸ ê¸°ë°˜ ì•„í‚¤í…ì²˜",
        "components": [
            "Pre-retrieval (Query Expansion, Classification)",
            "Retrieval (Semantic + BM25 Keyword)",
            "Post-retrieval (Filtering, Diversity)",
            "Generation (Answer Generation, Confidence)",
            "Orchestration (Iteration Control)"
        ],
        "features": [
            "ëª¨ë“ˆí˜• ì•„í‚¤í…ì²˜", "7ê°€ì§€ ì¿¼ë¦¬ ìœ í˜• ë¶„ë¥˜", "í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë²¡í„° + BM25)",
            "ë°˜ë³µì  ê°œì„ ", "ì‹ ë¢°ë„ ê¸°ë°˜ ì œì–´"
        ],
        "advantages": [
            "ë†’ì€ ìœ ì—°ì„±ê³¼ í™•ì¥ì„±", "ì§ˆë¬¸ ìœ í˜•ë³„ ë§ì¶¤ ì²˜ë¦¬", "ì˜ë¯¸ì  + í‚¤ì›Œë“œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰",
            "ì ì§„ì  í’ˆì§ˆ ê°œì„ ", "íˆ¬ëª…í•œ ì²˜ë¦¬ ê³¼ì •"
        ],
        "retrieval_methods": {
            "semantic": "Dense vector similarity (embeddings)",
            "keyword": "BM25 sparse retrieval",
            "hybrid": "Combination of both methods"
        }
    } 