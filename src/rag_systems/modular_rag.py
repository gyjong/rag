"""Modular RAG implementation with flexible component-based architecture."""

from typing import List, Dict, Any, Optional, Callable, Tuple
import time
import re
import numpy as np
import streamlit as st
import pandas as pd
from langchain_core.documents import Document
from enum import Enum

from ..utils.vector_store import VectorStoreManager
from ..utils.llm_manager import LLMManager

# BM25 Implementation (standalone to avoid external dependencies)
class BM25:
    """BM25 ranking algorithm implementation."""
    
    def __init__(self, corpus: List[str], k1: float = 1.5, b: float = 0.75):
        """Initialize BM25 with corpus.
        
        Args:
            corpus: List of documents (strings)
            k1: Term frequency normalization parameter (default: 1.5)
            b: Length normalization parameter (default: 0.75)
        """
        self.k1 = k1
        self.b = b
        self.corpus = corpus
        self.corpus_size = len(corpus)
        
        # Tokenize and process corpus
        self.doc_tokens = [self._tokenize(doc) for doc in corpus]
        self.doc_len = [len(tokens) for tokens in self.doc_tokens]
        self.avgdl = sum(self.doc_len) / self.corpus_size if self.corpus_size > 0 else 0
        
        # Build vocabulary and document frequency
        self.vocab = set()
        for tokens in self.doc_tokens:
            self.vocab.update(tokens)
        self.vocab = list(self.vocab)
        
        # Calculate document frequencies
        self.df = {}
        for term in self.vocab:
            self.df[term] = sum(1 for tokens in self.doc_tokens if term in tokens)
        
        # Pre-compute IDF scores
        self.idf = {}
        for term in self.vocab:
            self.idf[term] = np.log((self.corpus_size - self.df[term] + 0.5) / (self.df[term] + 0.5))
    
    def _tokenize(self, text: str) -> List[str]:
        """Simple tokenization (can be enhanced with proper NLP tokenizer)."""
        # Basic tokenization: lowercase, remove punctuation, split by spaces
        text = text.lower()
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)  # Keep Korean characters
        tokens = text.split()
        # Filter short tokens
        tokens = [token for token in tokens if len(token) > 1]
        return tokens
    
    def get_scores(self, query: str) -> List[float]:
        """Calculate BM25 scores for query against all documents."""
        query_tokens = self._tokenize(query)
        scores = []
        
        for i, doc_tokens in enumerate(self.doc_tokens):
            score = 0.0
            doc_len = self.doc_len[i]
            
            # Count term frequencies in document
            tf = {}
            for token in doc_tokens:
                tf[token] = tf.get(token, 0) + 1
            
            # Calculate BM25 score
            for term in query_tokens:
                if term in tf:
                    # TF component
                    tf_component = tf[term] * (self.k1 + 1)
                    tf_component /= (tf[term] + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl))
                    
                    # IDF component
                    idf_component = self.idf.get(term, 0)
                    
                    score += tf_component * idf_component
            
            scores.append(score)
        
        return scores
    
    def get_top_k(self, query: str, k: int = 5) -> List[Tuple[int, float]]:
        """Get top-k documents with scores."""
        scores = self.get_scores(query)
        # Get indices and scores, sort by score (descending)
        scored_docs = [(i, score) for i, score in enumerate(scores)]
        scored_docs.sort(key=lambda x: x[1], reverse=True)
        return scored_docs[:k]


class ModuleType(Enum):
    """Types of modules in the Modular RAG system."""
    INDEXING = "indexing"
    PRE_RETRIEVAL = "pre_retrieval"
    RETRIEVAL = "retrieval"
    POST_RETRIEVAL = "post_retrieval"
    GENERATION = "generation"
    ORCHESTRATION = "orchestration"


class ModularRAG:
    """Modular RAG implementation with component-based architecture."""
    
    def __init__(self, vector_store_manager: VectorStoreManager, llm_manager: LLMManager):
        """Initialize Modular RAG system."""
        self.vector_store_manager = vector_store_manager
        self.llm_manager = llm_manager
        self.name = "Modular RAG"
        self.description = "ëª¨ë“ˆí˜• RAG ì‹œìŠ¤í…œ: ìœ ì—°í•œ ì»´í¬ë„ŒíŠ¸ ê¸°ë°˜ ì•„í‚¤í…ì²˜"
        
        # BM25 index for keyword-based retrieval (lazy initialization)
        self.bm25_index = None
        self.bm25_documents = []
        
        # Initialize modules
        self.modules = {}
        self._setup_modules()
    
    def _initialize_bm25_index(self):
        """Initialize BM25 index for keyword-based retrieval."""
        if self.bm25_index is not None:
            return  # Already initialized
            
        # Check if we have a cached index in session state
        if "bm25_index" in st.session_state and "bm25_documents" in st.session_state:
            self.bm25_index = st.session_state.bm25_index
            self.bm25_documents = st.session_state.bm25_documents
            return
        
        vector_store = self.vector_store_manager.get_vector_store()
        if vector_store is None:
            st.warning("âš ï¸ ë²¡í„° ìŠ¤í† ì–´ê°€ ì—†ì–´ BM25 ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        try:
            with st.spinner("ğŸ” BM25 í‚¤ì›Œë“œ ê²€ìƒ‰ ì¸ë±ìŠ¤ ìƒì„± ì¤‘..."):
                # Get all documents from vector store
                # Note: This is a simplified approach - in production, you'd want a more efficient method
                sample_docs = vector_store.similarity_search("AI", k=100)  # Get many docs
                
                if not sample_docs:
                    st.warning("âš ï¸ ë¬¸ì„œê°€ ì—†ì–´ BM25 ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return
                
                # Extract text content
                corpus = [doc.page_content for doc in sample_docs]
                self.bm25_documents = sample_docs
                
                # Create BM25 index
                self.bm25_index = BM25(corpus)
                
                # Cache in session state
                st.session_state.bm25_index = self.bm25_index
                st.session_state.bm25_documents = self.bm25_documents
                
                st.success(f"âœ… BM25 ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ! ({len(corpus)}ê°œ ë¬¸ì„œ ì¸ë±ì‹±)")
                
        except Exception as e:
            st.error(f"âŒ BM25 ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        
    def _setup_modules(self):
        """Setup default modules for the system."""
        self.modules = {
            ModuleType.PRE_RETRIEVAL: [
                self._query_expansion_module,
                self._query_classification_module
            ],
            ModuleType.RETRIEVAL: [
                self._semantic_retrieval_module,
                self._keyword_retrieval_module
            ],
            ModuleType.POST_RETRIEVAL: [
                self._relevance_filtering_module,
                self._diversity_module
            ],
            ModuleType.GENERATION: [
                self._answer_generation_module,
                self._confidence_estimation_module
            ],
            ModuleType.ORCHESTRATION: [
                self._routing_module,
                self._iteration_control_module
            ]
        }
    
    def _query_expansion_module(self, query: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Expand query with related terms."""
        expanded_terms = []
        
        # Simple keyword-based expansion
        keywords_map = {
            # AI & Technology
            "AI": ["artificial intelligence", "machine learning", "ë”¥ëŸ¬ë‹", "neural networks", "automation", "intelligent systems"],
            "ì¸ê³µì§€ëŠ¥": ["AI", "ë¨¸ì‹ ëŸ¬ë‹", "neural network", "ìë™í™”", "ì§€ëŠ¥í˜• ì‹œìŠ¤í…œ", "algorithm"],
            "ë¨¸ì‹ ëŸ¬ë‹": ["machine learning", "AI", "ì¸ê³µì§€ëŠ¥", "data science", "predictive modeling", "ML"],
            "ë”¥ëŸ¬ë‹": ["deep learning", "neural networks", "AI", "ì¸ê³µì§€ëŠ¥", "computer vision", "NLP"],
            
            # Trends & Future
            "íŠ¸ë Œë“œ": ["ë™í–¥", "ì „ë§", "trend", "future", "ë°©í–¥ì„±", "íë¦„", "tendency", "outlook"],
            "ë™í–¥": ["trend", "íŠ¸ë Œë“œ", "í˜„í™©", "ìƒí™©", "direction", "movement"],
            "ì „ë§": ["outlook", "prospect", "ì˜ˆì¸¡", "ë¯¸ë˜", "forecast", "prediction"],
            "ë¯¸ë˜": ["future", "prospect", "ì „ë§", "outlook", "upcoming", "forthcoming"],
            
            # Business & Work
            "ì—…ë¬´": ["work", "business", "ì§ì¥", "productivity", "task", "operation", "job", "profession"],
            "ì§ì¥": ["workplace", "office", "business", "ì—…ë¬´", "career", "employment"],
            "ë¹„ì¦ˆë‹ˆìŠ¤": ["business", "enterprise", "commerce", "ì—…ë¬´", "corporate", "commercial"],
            "ìƒì‚°ì„±": ["productivity", "efficiency", "performance", "output", "effectiveness"],
            
            # Technology Applications
            "ìë™í™”": ["automation", "ìë™", "streamlining", "optimization", "efficiency"],
            "ë””ì§€í„¸í™”": ["digitalization", "digital transformation", "digitization", "modernization"],
            "í˜ì‹ ": ["innovation", "breakthrough", "advancement", "progress", "development"],
            "ê¸°ìˆ ": ["technology", "tech", "technique", "method", "approach"],
            
            # Industry & Sectors
            "ì‚°ì—…": ["industry", "sector", "field", "domain", "market"],
            "ì‹œì¥": ["market", "industry", "sector", "business", "commercial"],
            "ê¸°ì—…": ["company", "enterprise", "corporation", "business", "organization"],
            
            # Analysis & Research
            "ë¶„ì„": ["analysis", "research", "study", "investigation", "examination"],
            "ì—°êµ¬": ["research", "study", "investigation", "analysis", "exploration"],
            "ì¡°ì‚¬": ["survey", "investigation", "research", "study", "examination"],
            
            # Impact & Effects
            "ì˜í–¥": ["impact", "effect", "influence", "consequence", "result"],
            "íš¨ê³¼": ["effect", "impact", "result", "outcome", "consequence"],
            "ë³€í™”": ["change", "transformation", "shift", "evolution", "development"],
            
            # Implementation & Strategy
            "ë„ì…": ["implementation", "adoption", "introduction", "deployment", "integration"],
            "ì „ëµ": ["strategy", "plan", "approach", "method", "tactic"],
            "ë°©ì•ˆ": ["plan", "strategy", "approach", "solution", "method"],
            "ë°©ë²•": ["method", "approach", "technique", "way", "procedure"],
            
            # Performance & Quality
            "ì„±ëŠ¥": ["performance", "efficiency", "capability", "effectiveness", "quality"],
            "í’ˆì§ˆ": ["quality", "standard", "excellence", "performance", "caliber"],
            "íš¨ìœ¨ì„±": ["efficiency", "productivity", "effectiveness", "performance", "optimization"]
        }
        
        # Limit expansion to avoid overwhelming the query
        for keyword, expansions in keywords_map.items():
            if keyword.lower() in query.lower():
                expanded_terms.extend(expansions[:2])
                # Limit total expansion terms to prevent query bloat
                if len(expanded_terms) >= 4:
                    break
        
        expanded_query = f"{query} {' '.join(expanded_terms[:4])}" if expanded_terms else query
        
        return {
            "original_query": query,
            "expanded_query": expanded_query,
            "expansion_terms": expanded_terms
        }
    
    def _query_classification_module(self, query: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Enhanced query classification with detailed analysis and confidence scoring."""
        query_lower = query.lower().strip()
        
        # Enhanced pattern matching with scoring
        classification_patterns = {
            "factual": {
                "primary": ["ë¬´ì—‡", "what", "ì–´ë–¤", "ì–´ë– í•œ", "ë­”ê°€", "ë­ê°€"],
                "secondary": ["ì •ì˜", "definition", "ì˜ë¯¸", "meaning", "ê°œë…", "ì„¤ëª…"],
                "weight": 1.0
            },
            "procedural": {
                "primary": ["ì–´ë–»ê²Œ", "how", "ë°©ë²•", "ë°©ì‹", "ê³¼ì •", "ì ˆì°¨"],
                "secondary": ["ë‹¨ê³„", "step", "ìˆœì„œ", "í”„ë¡œì„¸ìŠ¤", "process", "êµ¬í˜„"],
                "weight": 1.0
            },
            "causal": {
                "primary": ["ì™œ", "why", "ì´ìœ ", "ì›ì¸", "cause", "reason"],
                "secondary": ["ë•Œë¬¸", "because", "ê²°ê³¼", "ì˜í–¥", "effect", "impact"],
                "weight": 1.0
            },
            "temporal": {
                "primary": ["ì–¸ì œ", "when", "ì‹œì ", "ì‹œê¸°", "timing"],
                "secondary": ["ë…„ë„", "year", "ë¯¸ë˜", "future", "ê³¼ê±°", "past", "í˜„ì¬"],
                "weight": 1.0
            },
            "comparative": {
                "primary": ["ë¹„êµ", "ì°¨ì´", "compare", "difference", "vs", "ëŒ€ë¹„"],
                "secondary": ["ì¥ë‹¨ì ", "pros", "cons", "ì¢‹ì€", "ë‚˜ìœ", "better", "worse"],
                "weight": 1.0
            },
            "quantitative": {
                "primary": ["ì–¼ë§ˆ", "how much", "how many", "ìˆ˜ëŸ‰", "ê°œìˆ˜", "ë¹„ìœ¨"],
                "secondary": ["í¼ì„¼íŠ¸", "percent", "%", "í†µê³„", "statistics", "ìˆ˜ì¹˜"],
                "weight": 1.0
            }
        }
        
        # Calculate scores for each query type
        type_scores = {}
        matched_keywords = {}
        
        for q_type, patterns in classification_patterns.items():
            score = 0.0
            keywords = []
            
            # Check primary patterns (higher weight)
            for pattern in patterns["primary"]:
                if pattern in query_lower:
                    score += 1.0
                    keywords.append(f"[ì£¼ìš”] {pattern}")
            
            # Check secondary patterns (lower weight)  
            for pattern in patterns["secondary"]:
                if pattern in query_lower:
                    score += 0.5
                    keywords.append(f"[ë³´ì¡°] {pattern}")
            
            # Question mark bonus
            if "?" in query or "?" in query:
                score += 0.2
                
            type_scores[q_type] = score * patterns["weight"]
            matched_keywords[q_type] = keywords
        
        # Determine best classification
        if max(type_scores.values()) > 0:
            query_type = max(type_scores, key=type_scores.get)
            confidence = min(0.95, 0.3 + (type_scores[query_type] * 0.3))
        else:
            query_type = "general"
            confidence = 0.5
        
        # Display detailed classification analysis
        st.write("**ğŸ” ì§ˆë¬¸ ë¶„ë¥˜ ìƒì„¸ ë¶„ì„:**")
        
        # Create classification table
        classification_data = []
        for q_type, score in type_scores.items():
            status = "âœ… ì„ íƒë¨" if q_type == query_type else ""
            keywords_str = ", ".join(matched_keywords[q_type]) if matched_keywords[q_type] else "ë§¤ì¹­ ì—†ìŒ"
            
            classification_data.append({
                "ìœ í˜•": q_type,
                "ì ìˆ˜": f"{score:.2f}",
                "ë§¤ì¹­ í‚¤ì›Œë“œ": keywords_str,
                "ìƒíƒœ": status
            })
        
        # Sort by score
        classification_data.sort(key=lambda x: float(x["ì ìˆ˜"]), reverse=True)
        
        import pandas as pd
        df = pd.DataFrame(classification_data)
        st.dataframe(df, use_container_width=True)
        
        # Show confidence meter
        col1, col2 = st.columns([3, 1])
        with col1:
            st.write(f"**ìµœì¢… ë¶„ë¥˜:** `{query_type}` (ì‹ ë¢°ë„: {confidence:.2%})")
        with col2:
            # Simple confidence visualization
            if confidence >= 0.8:
                st.success(f"ë†’ìŒ {confidence:.1%}")
            elif confidence >= 0.6:
                st.warning(f"ë³´í†µ {confidence:.1%}")
            else:
                st.error(f"ë‚®ìŒ {confidence:.1%}")
        
        # Show classification impact
        classification_effects = {
            "factual": "ğŸ¯ ì •í™•í•œ ì‚¬ì‹¤ ì •ë³´ ìœ„ì£¼ë¡œ ë‹µë³€",
            "procedural": "ğŸ“‹ ë‹¨ê³„ë³„ ë°©ë²•ë¡  ì¤‘ì‹¬ìœ¼ë¡œ ë‹µë³€", 
            "causal": "ğŸ¤” ì›ì¸ê³¼ ì´ìœ  ë¶„ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë‹µë³€",
            "temporal": "â° ì‹œê°„ìˆœ ì •ë³´ ì¤‘ì‹¬ìœ¼ë¡œ ë‹µë³€",
            "comparative": "âš–ï¸ ë¹„êµ ë¶„ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë‹µë³€",
            "quantitative": "ğŸ“Š ìˆ˜ì¹˜ì™€ í†µê³„ ì¤‘ì‹¬ìœ¼ë¡œ ë‹µë³€",
            "general": "ğŸ“– ì¢…í•©ì ì´ê³  í¬ê´„ì ìœ¼ë¡œ ë‹µë³€"
        }
        
        st.info(f"**ì²˜ë¦¬ ë°©ì‹:** {classification_effects.get(query_type, 'ì¼ë°˜ì  ì²˜ë¦¬')}")
        
        return {
            "query_type": query_type,
            "confidence": confidence,
            "all_scores": type_scores,
            "matched_keywords": matched_keywords[query_type],
            "classification_details": classification_data
        }
    
    def _semantic_retrieval_module(self, query: str, context: Dict[str, Any]) -> List[Document]:
        """Perform semantic retrieval using vector similarity."""
        vector_store = self.vector_store_manager.get_vector_store()
        if vector_store is None:
            return []
            
        try:
            k = context.get("retrieval_k", 8)
            docs = vector_store.similarity_search(query, k=k)
            return docs
        except Exception as e:
            st.error(f"Semantic retrieval failed: {str(e)}")
            return []
    
    def _keyword_retrieval_module(self, query: str, context: Dict[str, Any]) -> List[Document]:
        """Perform BM25-based keyword retrieval."""
        st.write("**ğŸ” BM25 í‚¤ì›Œë“œ ê²€ìƒ‰:**")
        
        # Initialize BM25 index if not already done
        self._initialize_bm25_index()
        
        if self.bm25_index is None or not self.bm25_documents:
            st.warning("âš ï¸ BM25 ì¸ë±ìŠ¤ê°€ ì—†ì–´ ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            # Fallback to vector search
            vector_store = self.vector_store_manager.get_vector_store()
            if vector_store:
                k = context.get("retrieval_k", 5)
                return vector_store.similarity_search(query, k=k)
            return []
            
        try:
            k = context.get("retrieval_k", 5)
            
            # Perform BM25 search
            with st.spinner(f"BM25 ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìƒìœ„ {k}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘..."):
                start_time = time.time()
                top_docs = self.bm25_index.get_top_k(query, k=k)
                search_time = time.time() - start_time
            
            # Extract documents and scores
            retrieved_docs = []
            search_results = []
            
            for rank, (doc_idx, score) in enumerate(top_docs, 1):
                if score > 0:  # Only include documents with positive scores
                    doc = self.bm25_documents[doc_idx]
                    retrieved_docs.append(doc)
                    search_results.append({
                        "ìˆœìœ„": rank,
                        "BM25 ì ìˆ˜": f"{score:.3f}",
                        "ë¬¸ì„œ ì¶œì²˜": doc.metadata.get('source', 'Unknown')[:30] + "...",
                        "ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°": doc.page_content[:100] + "..."
                    })
            
            # Display search results
            if search_results:
                st.write("**ğŸ“Š BM25 ê²€ìƒ‰ ê²°ê³¼:**")
                df = pd.DataFrame(search_results)
                st.dataframe(df, use_container_width=True)
                
                # Search statistics
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("ê²€ìƒ‰ ì‹œê°„", f"{search_time:.3f}ì´ˆ")
                with col2:
                    st.metric("ê²€ìƒ‰ëœ ë¬¸ì„œ", f"{len(retrieved_docs)}ê°œ")
                with col3:
                    st.metric("ìµœê³  ì ìˆ˜", f"{top_docs[0][1]:.3f}" if top_docs else "0.000")
                with col4:
                    st.metric("í‰ê·  ì ìˆ˜", f"{np.mean([score for _, score in top_docs]):.3f}" if top_docs else "0.000")
                
                # Query analysis
                query_tokens = self.bm25_index._tokenize(query)
                st.info(f"**ë¶„ì„ëœ ì¿¼ë¦¬ í† í°:** {', '.join(query_tokens)}")
                
                # Show detailed results for top documents
                with st.expander(f"ğŸ” ìƒìœ„ {min(3, len(retrieved_docs))}ê°œ ë¬¸ì„œ ìƒì„¸ë³´ê¸°"):
                    for i, (doc, (_, score)) in enumerate(zip(retrieved_docs[:3], top_docs[:3])):
                        st.write(f"**#{i+1} ë¬¸ì„œ (BM25: {score:.3f})**")
                        st.write(f"**ì¶œì²˜:** {doc.metadata.get('source', 'Unknown')}")
                        st.write(f"**ë‚´ìš©:** {doc.page_content[:200]}...")
                        st.divider()
                
                st.success(f"âœ… BM25 í‚¤ì›Œë“œ ê²€ìƒ‰ ì™„ë£Œ: {len(retrieved_docs)}ê°œ ë¬¸ì„œ ({search_time:.3f}ì´ˆ)")
                
            else:
                st.warning("âš ï¸ ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                
            return retrieved_docs
            
        except Exception as e:
            st.error(f"âŒ BM25 ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            # Fallback to vector search
            vector_store = self.vector_store_manager.get_vector_store()
            if vector_store:
                k = context.get("retrieval_k", 5)
                return vector_store.similarity_search(query, k=k)
            return []
    
    def _relevance_filtering_module(self, docs: List[Document], context: Dict[str, Any]) -> List[Document]:
        """Filter documents based on relevance threshold."""
        if not docs:
            return docs
            
        # Simple length-based filtering (in practice, would use relevance scores)
        min_length = context.get("min_doc_length", 50)
        filtered_docs = [doc for doc in docs if len(doc.page_content) >= min_length]
        
        return filtered_docs[:context.get("max_docs", 5)]
    
    def _diversity_module(self, docs: List[Document], context: Dict[str, Any]) -> List[Document]:
        """Ensure diversity in retrieved documents."""
        if len(docs) <= 3:
            return docs
            
        # Simple diversity based on source
        diverse_docs = []
        seen_sources = set()
        
        for doc in docs:
            source = doc.metadata.get("source", "unknown")
            if source not in seen_sources or len(diverse_docs) < 2:
                diverse_docs.append(doc)
                seen_sources.add(source)
                
            if len(diverse_docs) >= context.get("max_diverse_docs", 4):
                break
                
        return diverse_docs
    
    def _answer_generation_module(self, query: str, docs: List[Document], context: Dict[str, Any]) -> str:
        """Enhanced answer generation with query-type specific processing."""
        if not docs:
            return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        doc_context = "\n\n".join([doc.page_content for doc in docs])
        
        # Limit context length
        max_length = context.get("max_context_length", 3500)
        if len(doc_context) > max_length:
            doc_context = doc_context[:max_length] + "..."
            
        query_type = context.get("query_type", "general")
        
        # Enhanced prompt templates with clear differentiation
        prompt_templates = {
            "factual": {
                "instruction": "ì‚¬ì‹¤ì  ì •ë³´ ì¤‘ì‹¬ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.",
                "system_prompt": """ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì •í™•í•œ ì‚¬ì‹¤ ì •ë³´ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”.
- í™•ì‹¤í•œ ì •ë³´ë§Œ ì œê³µí•˜ì„¸ìš”
- ì¶”ì¸¡ì´ë‚˜ ì˜ê²¬ì€ ë°°ì œí•˜ì„¸ìš”  
- ì •í™•í•˜ì§€ ì•Šìœ¼ë©´ 'í™•ì‹¤í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤'ë¼ê³  ë‹µí•´ì£¼ì„¸ìš”
- êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ë°ì´í„°ê°€ ìˆìœ¼ë©´ í¬í•¨í•˜ì„¸ìš”""",
                "style": "ì •í™•ì„± ìš°ì„ "
            },
            "procedural": {
                "instruction": "ë‹¨ê³„ë³„ ë°©ë²•ë¡  ì¤‘ì‹¬ìœ¼ë¡œ ì²´ê³„ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.",
                "system_prompt": """ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¨ê³„ë³„ ë°©ë²•ì„ ì²´ê³„ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
- ìˆœì„œëŒ€ë¡œ ë²ˆí˜¸ë¥¼ ë§¤ê²¨ ì„¤ëª…í•˜ì„¸ìš”
- ê° ë‹¨ê³„ë³„ë¡œ êµ¬ì²´ì ì¸ ë°©ë²•ì„ ì œì‹œí•˜ì„¸ìš”
- ì£¼ì˜ì‚¬í•­ì´ë‚˜ íŒì´ ìˆìœ¼ë©´ í¬í•¨í•˜ì„¸ìš”
- ì‹¤í–‰ ê°€ëŠ¥í•œ êµ¬ì²´ì ì¸ ë°©ë²•ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”""",
                "style": "ë‹¨ê³„ë³„ ì²´ê³„í™”"
            },
            "causal": {
                "instruction": "ì›ì¸ê³¼ ì´ìœ  ë¶„ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë…¼ë¦¬ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.",
                "system_prompt": """ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì›ì¸ê³¼ ì´ìœ ë¥¼ ë…¼ë¦¬ì ìœ¼ë¡œ ë¶„ì„í•´ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”.
- 'ì™œëƒí•˜ë©´...', 'ê·¸ ì´ìœ ëŠ”...' ë“±ì˜ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”
- ì›ì¸ê³¼ ê²°ê³¼ì˜ ê´€ê³„ë¥¼ ëª…í™•íˆ ì„¤ëª…í•˜ì„¸ìš”
- ë°°ê²½ ìƒí™©ê³¼ ë§¥ë½ì„ í¬í•¨í•˜ì„¸ìš”
- ë‹¤ì–‘í•œ ìš”ì¸ë“¤ ê°„ì˜ ì—°ê´€ì„±ì„ ì„¤ëª…í•˜ì„¸ìš”""",
                "style": "ë…¼ë¦¬ì  ì¸ê³¼ê´€ê³„"
            },
            "temporal": {
                "instruction": "ì‹œê°„ìˆœ ì •ë³´ì™€ ë°œì „ ê³¼ì • ì¤‘ì‹¬ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.",
                "system_prompt": """ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë¦¬í•´ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”.
- ì—°ë„, ì‹œê¸°, ìˆœì„œë¥¼ ëª…í™•íˆ í‘œì‹œí•˜ì„¸ìš”
- ê³¼ê±°â†’í˜„ì¬â†’ë¯¸ë˜ ìˆœìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”  
- ë°œì „ ê³¼ì •ì´ë‚˜ ë³€í™” ì¶”ì´ë¥¼ í¬í•¨í•˜ì„¸ìš”
- íŠ¹ì • ì‹œì ì˜ ì¤‘ìš”í•œ ì‚¬ê±´ì´ë‚˜ ë³€í™”ë¥¼ ê°•ì¡°í•˜ì„¸ìš”""",
                "style": "ì‹œê°„ìˆœ ì •ë¦¬"
            },
            "comparative": {
                "instruction": "ë¹„êµ ë¶„ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì¥ë‹¨ì ì„ ëª…í™•íˆ ë‹µë³€í•´ì£¼ì„¸ìš”.",
                "system_prompt": """ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¹„êµ ë¶„ì„í•´ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”.
- ê³µí†µì ê³¼ ì°¨ì´ì ì„ ëª…í™•íˆ êµ¬ë¶„í•˜ì„¸ìš”
- ì¥ì ê³¼ ë‹¨ì ì„ ê· í˜•ìˆê²Œ ì œì‹œí•˜ì„¸ìš”
- í‘œë‚˜ ëª©ë¡ í˜•íƒœë¡œ ì •ë¦¬í•´ì„œ ì„¤ëª…í•˜ì„¸ìš”
- ê°ê°ì˜ íŠ¹ì§•ì„ ìƒëŒ€ì ìœ¼ë¡œ ë¹„êµí•´ì£¼ì„¸ìš”""",
                "style": "ë¹„êµ ë¶„ì„"
            },
            "quantitative": {
                "instruction": "ìˆ˜ì¹˜ì™€ í†µê³„ ì •ë³´ ì¤‘ì‹¬ìœ¼ë¡œ ë°ì´í„° ê¸°ë°˜ ë‹µë³€í•´ì£¼ì„¸ìš”.",
                "system_prompt": """ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ìˆ˜ì¹˜, í†µê³„, ë°ì´í„° ì¤‘ì‹¬ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
- êµ¬ì²´ì ì¸ ìˆ«ìì™€ í¼ì„¼íŠ¸ë¥¼ í¬í•¨í•˜ì„¸ìš”
- í†µê³„ ë°ì´í„°ê°€ ìˆìœ¼ë©´ í™œìš©í•˜ì„¸ìš”
- ì–‘ì  ë³€í™”ë‚˜ ê·œëª¨ë¥¼ ê°•ì¡°í•˜ì„¸ìš”
- ê·¸ë˜í”„ë‚˜ ì°¨íŠ¸ë¡œ í‘œí˜„ ê°€ëŠ¥í•œ ì •ë³´ë¥¼ ì œì‹œí•˜ì„¸ìš”""",
                "style": "ë°ì´í„° ê¸°ë°˜"
            },
            "general": {
                "instruction": "ì¢…í•©ì ì´ê³  í¬ê´„ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.",
                "system_prompt": """ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…í•©ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
- ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì„¤ëª…í•˜ì„¸ìš”
- ì „ì²´ì ì¸ ê°œìš”ë¶€í„° ì„¸ë¶€ì‚¬í•­ê¹Œì§€ í¬í•¨í•˜ì„¸ìš”
- ê· í˜•ìˆê³  í¬ê´„ì ì¸ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”""",
                "style": "ì¢…í•©ì  ì„¤ëª…"
            }
        }
        
        # Get prompt configuration for current query type
        prompt_config = prompt_templates.get(query_type, prompt_templates["general"])
        
        # Display answer generation strategy
        st.write("**ğŸ¤– ë‹µë³€ ìƒì„± ì „ëµ:**")
        col1, col2 = st.columns([2, 1])
        with col1:
            st.info(f"ğŸ“ {prompt_config['instruction']}")
        with col2:
            st.success(f"ğŸ¯ {prompt_config['style']}")
        
        # Create enhanced prompt
        enhanced_prompt = f"""
{prompt_config['system_prompt']}

ì»¨í…ìŠ¤íŠ¸:
{doc_context}

ì§ˆë¬¸: {query}

ë‹µë³€:"""
        
        st.write("**ëª¨ë“ˆí˜• ë‹µë³€ ìƒì„± ì¤‘...**")
        answer_placeholder = st.empty()
        
        start_time = time.time()
        full_response = ""
        
        # Stream the response with enhanced prompt
        for chunk in self.llm_manager.generate_response_stream(
            prompt=enhanced_prompt,
            context=""  # Context already included in enhanced_prompt
        ):
            full_response += chunk
            answer_placeholder.markdown(full_response + "â–Œ")
        
        generation_time = time.time() - start_time
        answer_placeholder.markdown(full_response)
        
        # Show generation summary
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ìƒì„± ì‹œê°„", f"{generation_time:.2f}ì´ˆ")
        with col2:
            st.metric("ë‹µë³€ ê¸¸ì´", f"{len(full_response)}ì")
        with col3:
            st.metric("ì§ˆë¬¸ ìœ í˜•", query_type)
            
        st.success(f"âœ… {prompt_config['style']} ë°©ì‹ìœ¼ë¡œ ë‹µë³€ ìƒì„± ì™„ë£Œ!")
        
        return full_response
    
    def _confidence_estimation_module(self, answer: str, context: Dict[str, Any]) -> float:
        """Estimate confidence in the generated answer."""
        # Simple heuristic-based confidence estimation
        confidence = 0.5  # Base confidence
        
        # Increase confidence based on answer length and structure
        if len(answer) > 50:
            confidence += 0.2
        if len(answer) > 100:
            confidence += 0.1
            
        # Decrease confidence for uncertainty indicators
        uncertainty_words = ["ëª¨ë¥¸ë‹¤", "í™•ì‹¤í•˜ì§€", "ì•„ë§ˆë„", "maybe", "uncertain"]
        if any(word in answer.lower() for word in uncertainty_words):
            confidence -= 0.3
            
        # Increase confidence if answer has specific details
        if any(word in answer for word in ["2024", "2025", "êµ¬ì²´ì ", "specific"]):
            confidence += 0.1
            
        return max(0.0, min(1.0, confidence))
    
    def _routing_module(self, query: str, context: Dict[str, Any]) -> str:
        """Route query to appropriate processing path."""
        query_type = context.get("query_type", "general")
        
        if query_type == "factual":
            return "precise_path"
        elif query_type == "procedural":
            return "step_by_step_path"
        elif query_type == "causal":
            return "reasoning_path"
        else:
            return "standard_path"
    
    def _iteration_control_module(self, context: Dict[str, Any]) -> bool:
        """Control whether to iterate or stop."""
        confidence = context.get("confidence", 0.0)
        iteration_count = context.get("iteration_count", 0)
        max_iterations = context.get("max_iterations", 2)
        
        # Continue if confidence is low and we haven't exceeded max iterations
        return confidence < 0.7 and iteration_count < max_iterations
    
    def query(self, question: str, max_iterations: int = 2) -> Dict[str, Any]:
        """Process query using modular approach."""
        start_time = time.time()
        context = {
            "iteration_count": 0,
            "max_iterations": max_iterations,
            "retrieval_k": 8,
            "max_docs": 5,
            "max_context_length": 3500
        }
        
        st.subheader("ğŸ§© ëª¨ë“ˆí˜• RAG ì²˜ë¦¬ ê³¼ì •")
        
        # Step 1: Pre-retrieval processing
        st.write("**1ë‹¨ê³„: ì‚¬ì „ ê²€ìƒ‰ ì²˜ë¦¬**")
        
        # Query expansion
        expansion_result = self._query_expansion_module(question, context)
        context.update(expansion_result)
        if expansion_result["expansion_terms"]:
            st.info(f"ì¿¼ë¦¬ í™•ì¥: {expansion_result['expanded_query']}")
        
        # Query classification
        classification_result = self._query_classification_module(question, context)
        context.update(classification_result)
        st.info(f"ì¿¼ë¦¬ ìœ í˜•: {classification_result['query_type']}")
        
        # Routing
        processing_path = self._routing_module(question, context)
        context["processing_path"] = processing_path
        st.info(f"ì²˜ë¦¬ ê²½ë¡œ: {processing_path}")
        
        final_answer = ""
        all_retrieved_docs = []
        
        # Iterative processing
        while context["iteration_count"] < max_iterations:
            iteration = context["iteration_count"] + 1
            st.write(f"**ë°˜ë³µ {iteration}:**")
            
            # Step 2: Retrieval
            with st.spinner("ë¬¸ì„œ ê²€ìƒ‰ ì¤‘..."):
                query_to_use = context.get("expanded_query", question)
                
                # Combine semantic and keyword retrieval
                semantic_docs = self._semantic_retrieval_module(query_to_use, context)
                keyword_docs = self._keyword_retrieval_module(query_to_use, context)
                
                # Merge and deduplicate
                all_docs = semantic_docs + keyword_docs
                seen_content = set()
                unique_docs = []
                for doc in all_docs:
                    content_hash = hash(doc.page_content[:100])
                    if content_hash not in seen_content:
                        unique_docs.append(doc)
                        seen_content.add(content_hash)
                
                st.success(f"ê²€ìƒ‰ ì™„ë£Œ: {len(unique_docs)}ê°œ ë¬¸ì„œ")
            
            # Step 3: Post-retrieval processing
            filtered_docs = self._relevance_filtering_module(unique_docs, context)
            diverse_docs = self._diversity_module(filtered_docs, context)
            all_retrieved_docs.extend(diverse_docs)
            
            st.info(f"í›„ì²˜ë¦¬ ì™„ë£Œ: {len(diverse_docs)}ê°œ ë¬¸ì„œ ì„ íƒ")
            
            # Step 4: Generation
            with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                answer = self._answer_generation_module(question, diverse_docs, context)
                
            # Step 5: Confidence estimation
            confidence = self._confidence_estimation_module(answer, context)
            context["confidence"] = confidence
            context["iteration_count"] = iteration
            
            st.info(f"ì‹ ë¢°ë„: {confidence:.2f}")
            
            # Check if we should continue iterating
            if not self._iteration_control_module(context):
                final_answer = answer
                break
            else:
                st.warning(f"ì‹ ë¢°ë„ê°€ ë‚®ì•„ ë‹¤ìŒ ë°˜ë³µì„ ì‹œë„í•©ë‹ˆë‹¤ (ì‹ ë¢°ë„: {confidence:.2f})")
                # Adjust parameters for next iteration
                context["retrieval_k"] = min(context["retrieval_k"] + 2, 15)
                final_answer = answer
        
        # Display final results
        with st.expander(f"ìµœì¢… ê²€ìƒ‰ëœ ë¬¸ì„œ ({len(all_retrieved_docs)}ê°œ)"):
            for i, doc in enumerate(all_retrieved_docs[-5:]):  # Show last 5
                st.write(f"**ë¬¸ì„œ {i+1}:**")
                st.write(f"ì¶œì²˜: {doc.metadata.get('source', 'Unknown')}")
                st.write(f"ë‚´ìš©: {doc.page_content[:200]}...")
                st.divider()
        
        total_time = time.time() - start_time
        
        return {
            "question": question,
            "answer": final_answer,
            "retrieved_docs": all_retrieved_docs,
            "total_time": total_time,
            "rag_type": self.name,
            "metadata": {
                "iterations": context["iteration_count"],
                "final_confidence": context.get("confidence", 0.0),
                "query_type": context.get("query_type", "general"),
                "processing_path": context.get("processing_path", "standard"),
                "total_retrieved": len(all_retrieved_docs),
                "expansion_terms": context.get("expansion_terms", [])
            }
        }
    
    def get_system_info(self) -> Dict[str, Any]:
        """Get information about the Modular RAG system."""
        # Get BM25 index status
        bm25_status = "âœ… í™œì„±í™”" if self.bm25_index is not None else "â³ ëŒ€ê¸°ì¤‘"
        bm25_docs_count = len(self.bm25_documents) if self.bm25_documents else 0
        
        return {
            "name": self.name,
            "description": self.description,
            "components": [
                "Pre-retrieval Modules (Query Expansion, Classification)",
                "Retrieval Modules (Semantic + BM25 Keyword)",
                "Post-retrieval Modules (Filtering, Diversity)",
                "Generation Modules (Answer Generation, Confidence)",
                "Orchestration Modules (Routing, Iteration Control)"
            ],
            "features": [
                "ëª¨ë“ˆí˜• ì•„í‚¤í…ì²˜",
                "7ê°€ì§€ ì¿¼ë¦¬ ìœ í˜• ë¶„ë¥˜ (factual, procedural, causal, temporal, comparative, quantitative, general)",
                "í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë²¡í„° + BM25)",
                "ë°˜ë³µì  ê°œì„ ",
                "ì‹ ë¢°ë„ ê¸°ë°˜ ì œì–´",
                f"BM25 í‚¤ì›Œë“œ ê²€ìƒ‰ {bm25_status} ({bm25_docs_count}ê°œ ë¬¸ì„œ)"
            ],
            "advantages": [
                "ë†’ì€ ìœ ì—°ì„±ê³¼ í™•ì¥ì„±",
                "ì§ˆë¬¸ ìœ í˜•ë³„ ë§ì¶¤ ì²˜ë¦¬",
                "ì˜ë¯¸ì  + í‚¤ì›Œë“œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰",
                "ì ì§„ì  í’ˆì§ˆ ê°œì„ ",
                "íˆ¬ëª…í•œ ì²˜ë¦¬ ê³¼ì •"
            ],
            "retrieval_methods": {
                "semantic": "Dense vector similarity (embeddings)",
                "keyword": f"BM25 sparse retrieval ({bm25_docs_count} indexed docs)",
                "hybrid": "Combination of both methods"
            }
        } 