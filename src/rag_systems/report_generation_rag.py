"""Report Generation RAG implementation."""

from typing import List, Dict, Any, Optional
import time
import streamlit as st
from langchain_core.documents import Document
from langchain_core.language_models import BaseLLM

from ..utils.vector_store import VectorStoreManager
from ..utils.llm_manager import LLMManager


class ReportGenerationRAG:
    """Report Generation RAG implementation for structured report creation."""
    
    def __init__(self, vector_store_manager: VectorStoreManager, llm_manager: LLMManager):
        """Initialize Report Generation RAG system.
        
        Args:
            vector_store_manager: Vector store manager instance
            llm_manager: LLM manager instance
        """
        self.vector_store_manager = vector_store_manager
        self.llm_manager = llm_manager
        self.name = "Report Generation RAG"
        self.description = "êµ¬ì¡°í™”ëœ ë³´ê³ ì„œ ìƒì„±ì„ ìœ„í•œ ì „ë¬¸ RAG ì‹œìŠ¤í…œ"
        
    def retrieve_for_topic(self, topic: str, k: int = 10) -> List[Document]:
        """Retrieve relevant documents for a specific topic.
        
        Args:
            topic: Topic to search for
            k: Number of documents to retrieve
            
        Returns:
            List of retrieved documents
        """
        vector_store = self.vector_store_manager.get_vector_store()
        if vector_store is None:
            st.error("ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
            
        try:
            # Create multiple search queries for better coverage
            search_queries = [
                topic,
                f"{topic} ë™í–¥",
                f"{topic} ë¶„ì„",
                f"{topic} í˜„í™©",
                f"{topic} ì „ë§"
            ]
            
            all_docs = []
            seen_content = set()
            
            for query in search_queries:
                try:
                    docs = vector_store.similarity_search(query, k=k//len(search_queries)+1)
                    for doc in docs:
                        # Remove duplicates based on content
                        content_hash = hash(doc.page_content[:500])
                        if content_hash not in seen_content:
                            all_docs.append(doc)
                            seen_content.add(content_hash)
                except Exception as e:
                    st.warning(f"ê²€ìƒ‰ ì¿¼ë¦¬ '{query}' ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    continue
            
            # Sort by relevance and return top k
            return all_docs[:k]
            
        except Exception as e:
            st.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def generate_report_section(self, section_title: str, section_content_guide: str, 
                              context_docs: List[Document], report_config: Dict[str, Any], 
                              streaming_container=None) -> str:
        """Generate a specific section of the report.
        
        Args:
            section_title: Title of the section
            section_content_guide: Guide for what to include in this section
            context_docs: Relevant documents for this section
            report_config: Configuration for the report
            streaming_container: Streamlit container for live updates
            
        Returns:
            Generated section content in markdown format
        """
        if not context_docs:
            context = "ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
        else:
            context = "\n\n".join([f"ë¬¸ì„œ: {doc.metadata.get('source', 'Unknown')}\në‚´ìš©: {doc.page_content}" 
                                 for doc in context_docs])
        
        # Limit context length
        max_context_length = 6000
        if len(context) > max_context_length:
            context = context[:max_context_length] + "..."
        
        # Create section generation prompt
        prompt = f"""ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ë³´ê³ ì„œ ì‘ì„±ìì…ë‹ˆë‹¤. ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë³´ê³ ì„œì˜ í•œ ì„¹ì…˜ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

**ë³´ê³ ì„œ ì •ë³´:**
- ë³´ê³ ì„œ ìœ í˜•: {report_config.get('report_type', 'N/A')}
- ì „ì²´ ì£¼ì œ: {report_config.get('topic', 'N/A')}
- ëª©ì : {report_config.get('purpose', 'N/A')}
- ëŒ€ìƒ ë…ì: {report_config.get('audience', 'N/A')}
- ì–¸ì–´: {report_config.get('language', 'í•œêµ­ì–´')}

**í˜„ì¬ ì‘ì„±í•  ì„¹ì…˜:**
- ì„¹ì…˜ ì œëª©: {section_title}
- ì„¹ì…˜ ë‚´ìš© ê°€ì´ë“œ: {section_content_guide}

**ì°¸ê³  ë¬¸ì„œ:**
{context}

**ì‘ì„± ì§€ì¹¨:**
1. ì œê³µëœ ì°¸ê³  ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”
2. ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”
3. ëŒ€ìƒ ë…ìì— ë§ëŠ” ì ì ˆí•œ ì–¸ì–´ì™€ ì„¤ëª… ìˆ˜ì¤€ì„ ì‚¬ìš©í•˜ì„¸ìš”
4. êµ¬ì²´ì ì¸ ë°ì´í„°ë‚˜ ì‚¬ì‹¤ì´ ìˆìœ¼ë©´ í¬í•¨í•˜ì„¸ìš”
5. í•„ìš”ì‹œ í•˜ìœ„ ì„¹ì…˜ì„ ë§Œë“¤ì–´ êµ¬ì¡°í™”í•˜ì„¸ìš”
6. ì°¸ê³ í•œ ë¬¸ì„œê°€ ìˆìœ¼ë©´ ë‚´ìš© ë§ë¯¸ì— ê°„ë‹¨íˆ ì–¸ê¸‰í•˜ì„¸ìš”

**ì¶œë ¥ í˜•ì‹:**
## {section_title}

[ì„¹ì…˜ ë‚´ìš©ì„ ì—¬ê¸°ì— ì‘ì„±]

ì„¹ì…˜ ë‚´ìš©ë§Œ ì‘ì„±í•˜ê³ , ì¶”ê°€ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”."""

        try:
            # Generate section content with streaming
            section_content = ""
            
            if streaming_container:
                # Show section header immediately
                with streaming_container:
                    st.markdown(f"### ğŸ”„ ìƒì„± ì¤‘: {section_title}")
                    section_placeholder = st.empty()
                    
                # Stream the content
                for chunk in self.llm_manager.generate_response_stream(
                    prompt=prompt,
                    context=""  # Context is already included in prompt
                ):
                    section_content += chunk
                    # Update the display in real-time with a small delay for smoother experience
                    if len(section_content) % 50 == 0:  # Update every 50 characters
                        with section_placeholder.container():
                            st.markdown(section_content + "â–Œ")  # Add cursor
                            time.sleep(0.1)  # Small delay for better UX
                
                # Final update without cursor
                with section_placeholder.container():
                    st.markdown(section_content)
                    
            else:
                # Non-streaming fallback
                for chunk in self.llm_manager.generate_response_stream(
                    prompt=prompt,
                    context=""
                ):
                    section_content += chunk
            
            return section_content.strip()
            
        except Exception as e:
            error_msg = f"ì„¹ì…˜ '{section_title}' ìƒì„± ì‹¤íŒ¨: {str(e)}"
            st.error(error_msg)
            return f"## {section_title}\n\nì„¹ì…˜ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def generate_report(self, report_config: Dict[str, Any], streaming_container=None) -> str:
        """Generate a complete report based on configuration.
        
        Args:
            report_config: Configuration dictionary containing report parameters
            streaming_container: Streamlit container for live streaming display
            
        Returns:
            Generated report in markdown format
        """
        start_time = time.time()
        
        # Extract configuration
        report_type = report_config.get('report_type', 'ì—°êµ¬ë³´ê³ ì„œ')
        topic = report_config.get('topic', '')
        purpose = report_config.get('purpose', '')
        outline = report_config.get('outline', [])
        target_length = report_config.get('target_length', 'medium')
        audience = report_config.get('audience', 'ì¼ë°˜ì¸')
        language = report_config.get('language', 'í•œêµ­ì–´')
        include_visuals = report_config.get('include_visuals', False)
        citation_style = report_config.get('citation_style', 'simple')
        
        # Create containers for different parts of the UI
        progress_container = st.container()
        
        if streaming_container:
            # Separate containers for progress and streaming content
            with progress_container:
                st.subheader("ğŸ“Š ë³´ê³ ì„œ ìƒì„± ì§„í–‰ ìƒí™©")
                progress_bar = st.progress(0)
                status_text = st.empty()
            
            # Live report display container
            with streaming_container:
                st.subheader("ğŸ“ ì‹¤ì‹œê°„ ë³´ê³ ì„œ ìƒì„±")
                live_report_container = st.container()
        else:
            st.subheader("ğŸ“Š ë³´ê³ ì„œ ìƒì„± ì§„í–‰ ìƒí™©")
            progress_bar = st.progress(0)
            status_text = st.empty()
            live_report_container = None
        
        # Step 1: Retrieve relevant documents for the main topic
        status_text.text("1. ì£¼ì œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
        progress_bar.progress(10)
        
        main_docs = self.retrieve_for_topic(topic, k=20)
        
        if not main_docs:
            st.warning("ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ë³´ê³ ì„œ êµ¬ì¡°ë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        
        # Step 2: Generate report header
        status_text.text("2. ë³´ê³ ì„œ í—¤ë” ìƒì„± ì¤‘...")
        progress_bar.progress(20)
        
        report_content = self._generate_report_header(report_config)
        
        # Display header immediately if streaming
        if live_report_container:
            with live_report_container:
                st.markdown(report_content)
                st.markdown("---")
        
        # Step 3: Generate each section
        total_sections = len(outline)
        if total_sections == 0:
            st.error("ëª©ì°¨ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return "ëª©ì°¨ê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        for i, section_info in enumerate(outline):
            section_title = section_info.get('title', f'ì„¹ì…˜ {i+1}')
            section_guide = section_info.get('content_guide', 'ì´ ì„¹ì…˜ì˜ ë‚´ìš©ì„ ì‘ì„±í•˜ì„¸ìš”.')
            
            status_text.text(f"3. ì„¹ì…˜ ìƒì„± ì¤‘: {section_title} ({i+1}/{total_sections})")
            progress = 20 + (i + 1) * 60 // total_sections
            progress_bar.progress(progress)
            
            # Get relevant documents for this specific section
            section_docs = self.retrieve_for_topic(f"{topic} {section_title}", k=10)
            
            # Use main docs if no specific docs found
            if not section_docs:
                section_docs = main_docs[:5]
            
            # Create section-specific streaming container
            section_streaming_container = None
            if live_report_container:
                section_streaming_container = live_report_container.container()
            
            # Generate section content with streaming
            section_content = self.generate_report_section(
                section_title, section_guide, section_docs, report_config,
                streaming_container=section_streaming_container
            )
            
            report_content += "\n\n" + section_content
            
            # Add separator between sections
            if live_report_container:
                with live_report_container:
                    st.markdown("---")
        
        # Step 4: Generate conclusion and references
        status_text.text("4. ê²°ë¡  ë° ì°¸ê³ ìë£Œ ìƒì„± ì¤‘...")
        progress_bar.progress(90)
        
        # Generate conclusion with streaming
        conclusion_container = None
        if live_report_container:
            conclusion_container = live_report_container.container()
        
        conclusion = self._generate_conclusion(
            topic, purpose, main_docs, report_config, 
            streaming_container=conclusion_container
        )
        report_content += "\n\n" + conclusion
        
        # Add references if requested
        if citation_style != 'none':
            references = self._generate_references(main_docs, citation_style)
            report_content += "\n\n" + references
            
            # Display references immediately if streaming
            if live_report_container:
                with live_report_container:
                    st.markdown("---")
                    st.markdown(references)
        
        # Step 5: Final formatting
        status_text.text("5. ìµœì¢… í¬ë§·íŒ… ì¤‘...")
        progress_bar.progress(100)
        
        # Add visual elements placeholder if requested
        if include_visuals:
            report_content = self._add_visual_placeholders(report_content)
        
        total_time = time.time() - start_time
        status_text.text(f"âœ… ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {total_time:.1f}ì´ˆ)")
        
        return report_content
    
    def _generate_report_header(self, report_config: Dict[str, Any]) -> str:
        """Generate report header with title and metadata."""
        report_type = report_config.get('report_type', 'ë³´ê³ ì„œ')
        topic = report_config.get('topic', '')
        purpose = report_config.get('purpose', '')
        
        from datetime import datetime
        current_date = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
        
        header = f"""# {topic} {report_type}

**ì‘ì„±ì¼:** {current_date}  
**ëª©ì :** {purpose}  
**ëŒ€ìƒ ë…ì:** {report_config.get('audience', 'ì¼ë°˜ì¸')}  

---

## ê°œìš”

ë³¸ ë³´ê³ ì„œëŠ” {topic}ì— ëŒ€í•œ {report_type.lower()}ë¡œ, {purpose}ë¥¼ ëª©ì ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
"""
        
        return header
    
    def _generate_conclusion(self, topic: str, purpose: str, context_docs: List[Document], 
                           report_config: Dict[str, Any], streaming_container=None) -> str:
        """Generate conclusion section."""
        context = "\n\n".join([doc.page_content for doc in context_docs[:5]])
        max_context_length = 3000
        if len(context) > max_context_length:
            context = context[:max_context_length] + "..."
        
        prompt = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë³´ê³ ì„œì˜ ê²°ë¡  ì„¹ì…˜ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

**ë³´ê³ ì„œ ì •ë³´:**
- ì£¼ì œ: {topic}
- ëª©ì : {purpose}
- ë³´ê³ ì„œ ìœ í˜•: {report_config.get('report_type', 'N/A')}

**ì°¸ê³  ë¬¸ì„œ:**
{context}

**ìš”êµ¬ì‚¬í•­:**
1. ë³´ê³ ì„œì˜ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•˜ì„¸ìš”
2. í•µì‹¬ ë°œê²¬ì‚¬í•­ì´ë‚˜ ê²°ê³¼ë¥¼ ì œì‹œí•˜ì„¸ìš”
3. í–¥í›„ ì „ë§ì´ë‚˜ ì œì–¸ì„ í¬í•¨í•˜ì„¸ìš”
4. ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”

**ì¶œë ¥ í˜•ì‹:**
## ê²°ë¡ 

[ê²°ë¡  ë‚´ìš©ì„ ì—¬ê¸°ì— ì‘ì„±]

### ì£¼ìš” ë°œê²¬ì‚¬í•­
- [ë°œê²¬ì‚¬í•­ 1]
- [ë°œê²¬ì‚¬í•­ 2]
- [ë°œê²¬ì‚¬í•­ 3]

### í–¥í›„ ì „ë§ ë° ì œì–¸
[ì „ë§ ë° ì œì–¸ ë‚´ìš©]
"""
        
        try:
            conclusion = ""
            
            if streaming_container:
                # Show conclusion header immediately
                with streaming_container:
                    st.markdown("### ğŸ”„ ìƒì„± ì¤‘: ê²°ë¡ ")
                    conclusion_placeholder = st.empty()
                    
                # Stream the content
                for chunk in self.llm_manager.generate_response_stream(
                    prompt=prompt,
                    context=""
                ):
                    conclusion += chunk
                    # Update the display in real-time with a small delay for smoother experience
                    if len(conclusion) % 50 == 0:  # Update every 50 characters
                        with conclusion_placeholder.container():
                            st.markdown(conclusion + "â–Œ")  # Add cursor
                            time.sleep(0.1)  # Small delay for better UX
                
                # Final update without cursor
                with conclusion_placeholder.container():
                    st.markdown(conclusion)
                    
            else:
                # Non-streaming fallback
                for chunk in self.llm_manager.generate_response_stream(
                    prompt=prompt,
                    context=""
                ):
                    conclusion += chunk
            
            return conclusion.strip()
            
        except Exception as e:
            error_msg = f"ê²°ë¡  ìƒì„± ì‹¤íŒ¨: {str(e)}"
            st.error(error_msg)
            return "## ê²°ë¡ \n\nê²°ë¡  ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def _generate_references(self, docs: List[Document], citation_style: str) -> str:
        """Generate references section."""
        if not docs:
            return ""
        
        references = "## ì°¸ê³ ìë£Œ\n\n"
        
        # Get unique sources
        sources = set()
        for doc in docs:
            source = doc.metadata.get('source', 'Unknown')
            if source != 'Unknown':
                sources.add(source)
        
        if citation_style == 'detailed':
            for i, source in enumerate(sorted(sources), 1):
                references += f"{i}. {source}\n"
        else:  # simple
            references += "ë³¸ ë³´ê³ ì„œëŠ” ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤:\n\n"
            for source in sorted(sources):
                references += f"- {source}\n"
        
        return references
    
    def _add_visual_placeholders(self, content: str) -> str:
        """Add placeholders for visual elements."""
        # Add chart placeholders after certain sections
        visual_sections = ['í˜„í™©', 'ë¶„ì„', 'ë™í–¥', 'í†µê³„', 'ë°ì´í„°']
        
        for section in visual_sections:
            if section in content:
                content = content.replace(
                    f"## {section}",
                    f"## {section}\n\n*[ì—¬ê¸°ì— {section} ê´€ë ¨ ì°¨íŠ¸ë‚˜ ê·¸ë˜í”„ ì‚½ì…]*\n"
                )
        
        return content
    
    def get_system_info(self) -> Dict[str, Any]:
        """Get information about the Report Generation RAG system."""
        return {
            "name": self.name,
            "description": self.description,
            "components": [
                "Vector Store (Multi-query Search)",
                "LLM (Structured Generation)",
                "Report Formatter",
                "Citation Manager"
            ],
            "features": [
                "êµ¬ì¡°í™”ëœ ë³´ê³ ì„œ ìƒì„±",
                "ì‚¬ìš©ì ì •ì˜ ëª©ì°¨ ì§€ì›",
                "ë‹¤ì–‘í•œ ë³´ê³ ì„œ ìœ í˜• ì§€ì›",
                "ìë™ ì¸ìš© ë° ì°¸ê³ ìë£Œ ìƒì„±",
                "ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ì¶œë ¥",
                "ì‹œê° ìš”ì†Œ í”Œë ˆì´ìŠ¤í™€ë”"
            ],
            "limitations": [
                "LLM í† í° ê¸¸ì´ ì œí•œ",
                "ë³µì¡í•œ ìˆ˜ì‹ì´ë‚˜ í‘œ ìƒì„± ì œí•œ",
                "ì‹¤ì‹œê°„ ë°ì´í„° ì—…ë°ì´íŠ¸ ë¶ˆê°€"
            ]
        } 