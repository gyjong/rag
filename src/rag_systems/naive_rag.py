"""Naive RAG implementation."""

from typing import List, Dict, Any, Optional
import time
import streamlit as st
from langchain_core.documents import Document
from langchain_core.language_models import BaseLLM

from ..utils.vector_store import VectorStoreManager
from ..utils.llm_manager import LLMManager


class NaiveRAG:
    """Naive RAG implementation with simple retrieval and generation."""
    
    def __init__(self, vector_store_manager: VectorStoreManager, llm_manager: LLMManager):
        """Initialize Naive RAG system.
        
        Args:
            vector_store_manager: Vector store manager instance
            llm_manager: LLM manager instance
        """
        self.vector_store_manager = vector_store_manager
        self.llm_manager = llm_manager
        self.name = "Naive RAG"
        self.description = "ê¸°ë³¸ì ì¸ RAG ì‹œìŠ¤í…œ: ë‹¨ìˆœ ìœ ì‚¬ë„ ê²€ìƒ‰ + ìƒì„±"
        
    def retrieve(self, query: str, k: int = 5) -> List[Document]:
        """Retrieve relevant documents using simple similarity search.
        
        Args:
            query: User query
            k: Number of documents to retrieve
            
        Returns:
            List of retrieved documents
        """
        vector_store = self.vector_store_manager.get_vector_store()
        if vector_store is None:
            st.error("ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
            
        try:
            with st.spinner("ë¬¸ì„œ ê²€ìƒ‰ ì¤‘..."):
                start_time = time.time()
                documents = vector_store.similarity_search(query, k=k)
                retrieval_time = time.time() - start_time
                
            st.success(f"ê²€ìƒ‰ ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ ({retrieval_time:.2f}ì´ˆ)")
            return documents
            
        except Exception as e:
            st.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def generate(self, query: str, context_docs: List[Document]) -> str:
        """Generate answer using retrieved context.
        
        Args:
            query: User query
            context_docs: Retrieved context documents
            
        Returns:
            Generated answer
        """
        if not context_docs:
            return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        # Combine context from retrieved documents
        context = "\n\n".join([doc.page_content for doc in context_docs])
        
        # Limit context length to avoid token limits
        max_context_length = 4000
        if len(context) > max_context_length:
            context = context[:max_context_length] + "..."
            
        st.write("**ë‹µë³€ ìƒì„± ì¤‘...**")
        answer_placeholder = st.empty()
        
        start_time = time.time()
        full_response = ""
        
        # Stream the response
        for chunk in self.llm_manager.generate_response_stream(
            prompt=query,
            context=context
        ):
            full_response += chunk
            answer_placeholder.markdown(full_response + "â–Œ")
        
        generation_time = time.time() - start_time
        answer_placeholder.markdown(full_response)
        
        st.success(f"ë‹µë³€ ìƒì„± ì™„ë£Œ ({generation_time:.2f}ì´ˆ)")
        return full_response
    
    def query(self, question: str, k: int = 5) -> Dict[str, Any]:
        """Process a query end-to-end.
        
        Args:
            question: User question
            k: Number of documents to retrieve
            
        Returns:
            Dictionary with results and metadata
        """
        start_time = time.time()
        
        # Step 1: Retrieve relevant documents
        st.subheader("ğŸ” 1ë‹¨ê³„: ë¬¸ì„œ ê²€ìƒ‰")
        retrieved_docs = self.retrieve(question, k=k)
        
        if not retrieved_docs:
            return {
                "question": question,
                "answer": "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "retrieved_docs": [],
                "total_time": time.time() - start_time,
                "rag_type": self.name
            }
        
        # Display retrieved documents
        with st.expander(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ({len(retrieved_docs)}ê°œ)"):
            for i, doc in enumerate(retrieved_docs):
                st.write(f"**ë¬¸ì„œ {i+1}:**")
                st.write(f"ì¶œì²˜: {doc.metadata.get('source', 'Unknown')}")
                st.write(f"ë‚´ìš©: {doc.page_content[:200]}...")
                st.divider()
        
        # Step 2: Generate answer
        st.subheader("ğŸ¤– 2ë‹¨ê³„: ë‹µë³€ ìƒì„±")
        answer = self.generate(question, retrieved_docs)
        
        total_time = time.time() - start_time
        
        return {
            "question": question,
            "answer": answer,
            "retrieved_docs": retrieved_docs,
            "total_time": total_time,
            "rag_type": self.name,
            "metadata": {
                "num_retrieved": len(retrieved_docs),
                "retrieval_method": "similarity_search",
                "generation_method": "simple"
            }
        }
    
    def get_system_info(self) -> Dict[str, Any]:
        """Get information about the Naive RAG system.
        
        Returns:
            Dictionary with system information
        """
        return {
            "name": self.name,
            "description": self.description,
            "components": [
                "Vector Store (Similarity Search)",
                "LLM (Direct Generation)"
            ],
            "features": [
                "ë‹¨ìˆœ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰",
                "ì§ì ‘ì ì¸ ë‹µë³€ ìƒì„±",
                "ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„"
            ],
            "limitations": [
                "ê²€ìƒ‰ í’ˆì§ˆ ì œí•œ",
                "ì»¨í…ìŠ¤íŠ¸ ì¬ì •ë ¬ ì—†ìŒ",
                "ì¿¼ë¦¬ ìµœì í™” ì—†ìŒ"
            ]
        } 