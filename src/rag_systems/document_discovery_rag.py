"""
Document Discovery RAG System
ë¬¸ì„œ ë°œê²¬ ë° ìƒì„¸ ê²€ìƒ‰ì„ ìœ„í•œ 2ë‹¨ê³„ RAG ì‹œìŠ¤í…œ
"""

import streamlit as st
from typing import List, Dict, Any, Tuple, Optional
from pathlib import Path
import json
from datetime import datetime

from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

from src.components.document_loader import DocumentLoader
from src.utils.llm_manager import LLMManager
from src.utils.vector_store import VectorStoreManager
from src.config import DOCS_FOLDER as DOCS_DIR


class DocumentDiscoveryRAG:
    """ë¬¸ì„œ ë°œê²¬ ë° ìƒì„¸ ê²€ìƒ‰ì„ ìœ„í•œ 2ë‹¨ê³„ RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self, llm_manager: LLMManager, embedding_manager, vector_store_manager: VectorStoreManager):
        self.llm_manager = llm_manager
        self.embedding_manager = embedding_manager
        self.vector_store_manager = vector_store_manager
        self.document_loader = DocumentLoader()
        
        # ë¬¸ì„œ ìš”ì•½ ìºì‹œ íŒŒì¼ ê²½ë¡œ
        self.cache_dir = Path("vector_stores/document_summaries")
        self.cache_dir.mkdir(exist_ok=True)
        self.summary_cache_file = self.cache_dir / "document_summaries.json"
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
        self.summary_prompt = PromptTemplate(
            input_variables=["document_content", "document_name"],
            template="""ë‹¤ìŒ ë¬¸ì„œì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ìš”ì•½í•´ì£¼ì„¸ìš”:

ë¬¸ì„œëª…: {document_name}

ë¬¸ì„œ ë‚´ìš©:
{document_content}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:
1. ì£¼ìš” ì£¼ì œ: (ë¬¸ì„œì˜ í•µì‹¬ ì£¼ì œ)
2. í‚¤ì›Œë“œ: (ì¤‘ìš”í•œ í‚¤ì›Œë“œë“¤ì„ ì‰¼í‘œë¡œ êµ¬ë¶„)
3. ë‚´ìš© ìš”ì•½: (3-5ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ ë‚´ìš© ìš”ì•½)
4. ë¬¸ì„œ ìœ í˜•: (ë³´ê³ ì„œ/ë…¼ë¬¸/ì •ì±…ë¬¸ì„œ/ê¸°ìˆ ë¬¸ì„œ ë“±)
5. ëŒ€ìƒ ë…ì: (ì¼ë°˜ì¸/ì „ë¬¸ê°€/ì •ì±…ê²°ì •ì ë“±)

ìš”ì•½:"""
        )
        
        self.relevance_prompt = PromptTemplate(
            input_variables=["query", "document_summaries"],
            template="""ì‚¬ìš©ì ì§ˆë¬¸: {query}

ë‹¤ìŒì€ ì‚¬ìš© ê°€ëŠ¥í•œ ë¬¸ì„œë“¤ì˜ ìš”ì•½ì…ë‹ˆë‹¤:
{document_summaries}

ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê° ë¬¸ì„œì˜ ê´€ë ¨ì„±ì„ 0-100ì ìœ¼ë¡œ í‰ê°€í•˜ê³ , ê´€ë ¨ì„±ì´ ë†’ì€ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
[ë¬¸ì„œëª…] ì ìˆ˜: XXì , ê´€ë ¨ì„± ì„¤ëª…: (ì™œ ê´€ë ¨ì´ ìˆëŠ”ì§€ ê°„ë‹¨íˆ ì„¤ëª…)

ì˜ˆì‹œ:
[ë¬¸ì„œ1.pdf] ì ìˆ˜: 85ì , ê´€ë ¨ì„± ì„¤ëª…: ì‚¬ìš©ìê°€ ë¬»ëŠ” AI ì •ì±…ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ì •ë¶€ ì •ì±… ë¬¸ì„œ
[ë¬¸ì„œ2.pdf] ì ìˆ˜: 70ì , ê´€ë ¨ì„± ì„¤ëª…: AI ê¸°ìˆ  ë™í–¥ì„ ë‹¤ë£¨ì–´ ì§ˆë¬¸ê³¼ ë¶€ë¶„ì ìœ¼ë¡œ ê´€ë ¨"""
        )
        
        self.detail_qa_prompt = PromptTemplate(
            input_variables=["context", "question"],
            template="""ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ê³ ë ¤í•´ì£¼ì„¸ìš”:
1. ë¬¸ì„œì—ì„œ ì§ì ‘ì ìœ¼ë¡œ ì–¸ê¸‰ëœ ë‚´ìš©ì„ ìš°ì„ ì ìœ¼ë¡œ í™œìš©
2. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ë°ì´í„°ê°€ ìˆìœ¼ë©´ í¬í•¨
3. ê´€ë ¨ëœ ë§¥ë½ì´ë‚˜ ë°°ê²½ ì •ë³´ë„ í•¨ê»˜ ì œê³µ
4. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  "ë¬¸ì„œì— ëª…ì‹œë˜ì§€ ì•ŠìŒ"ì´ë¼ê³  ëª…ì‹œ

ë‹µë³€:"""
        )
    
    def get_available_documents(self) -> List[Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë¬¸ì„œ ëª©ë¡ê³¼ ê¸°ë³¸ ì •ë³´ ë°˜í™˜"""
        documents = []
        pdf_files = list(DOCS_DIR.glob("*.pdf"))
        
        for pdf_file in pdf_files:
            doc_info = self.document_loader.get_document_info(pdf_file.name)
            if doc_info:
                documents.append(doc_info)
        
        return documents
    
    def load_document_summaries(self) -> Dict[str, Dict[str, Any]]:
        """ìºì‹œëœ ë¬¸ì„œ ìš”ì•½ ë¡œë“œ"""
        if self.summary_cache_file.exists():
            try:
                with open(self.summary_cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return {}
        return {}
    
    def save_document_summaries(self, summaries: Dict[str, Dict[str, Any]]):
        """ë¬¸ì„œ ìš”ì•½ì„ ìºì‹œì— ì €ì¥"""
        try:
            with open(self.summary_cache_file, 'w', encoding='utf-8') as f:
                json.dump(summaries, f, ensure_ascii=False, indent=2)
        except Exception as e:
            st.error(f"ìš”ì•½ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
    
    def generate_document_summary(self, filename: str) -> Optional[Dict[str, Any]]:
        """ë‹¨ì¼ ë¬¸ì„œ ìš”ì•½ ìƒì„±"""
        try:
            # ë¬¸ì„œ ë¡œë“œ
            documents = self.document_loader.load_document(filename)
            if not documents:
                return None
            
            # ë¬¸ì„œ ë‚´ìš© í•©ì¹˜ê¸° (ì²˜ìŒ 3í˜ì´ì§€ ì •ë„ë§Œ ì‚¬ìš©)
            content_parts = []
            for i, doc in enumerate(documents[:3]):  # ì²˜ìŒ 3í˜ì´ì§€ë§Œ
                content_parts.append(doc.page_content)
            
            combined_content = "\n\n".join(content_parts)
            
            # ë„ˆë¬´ ê¸´ ê²½ìš° ì˜ë¼ë‚´ê¸° (ìš”ì•½ ìƒì„±ì„ ìœ„í•´)
            if len(combined_content) > 4000:
                combined_content = combined_content[:4000] + "..."
            
            # LLMì„ ì‚¬ìš©í•´ ìš”ì•½ ìƒì„±
            llm = self.llm_manager.get_llm()
            chain = self.summary_prompt | llm | StrOutputParser()
            
            summary_text = chain.invoke({
                "document_content": combined_content,
                "document_name": filename
            })
            
            # ë¬¸ì„œ ê¸°ë³¸ ì •ë³´
            doc_info = self.document_loader.get_document_info(filename)
            
            summary_data = {
                "filename": filename,
                "summary": summary_text,
                "generated_at": datetime.now().isoformat(),
                "pages": doc_info.get("pages", 0),
                "size_mb": doc_info.get("size_mb", 0),
                "preview": doc_info.get("preview", ""),
                "title": doc_info.get("title", filename)
            }
            
            return summary_data
            
        except Exception as e:
            st.error(f"ë¬¸ì„œ ìš”ì•½ ìƒì„± ì‹¤íŒ¨ ({filename}): {str(e)}")
            return None
    
    def generate_all_summaries(self, progress_callback=None) -> Dict[str, Dict[str, Any]]:
        """ëª¨ë“  ë¬¸ì„œì˜ ìš”ì•½ ìƒì„±"""
        summaries = self.load_document_summaries()
        available_docs = self.get_available_documents()
        
        total_docs = len(available_docs)
        processed = 0
        
        for doc_info in available_docs:
            filename = doc_info["filename"]
            
            # ì´ë¯¸ ìš”ì•½ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            if filename in summaries:
                processed += 1
                if progress_callback:
                    progress_callback(processed, total_docs, f"âœ… {filename} (ìºì‹œë¨)")
                continue
            
            if progress_callback:
                progress_callback(processed, total_docs, f"ğŸ“ {filename} ìš”ì•½ ìƒì„± ì¤‘...")
            
            # ìƒˆ ìš”ì•½ ìƒì„±
            summary = self.generate_document_summary(filename)
            if summary:
                summaries[filename] = summary
                # ì§„í–‰ ì¤‘ì—ë„ ì €ì¥ (ì¤‘ë‹¨ë˜ì–´ë„ ì§„í–‰ì‚¬í•­ ë³´ì¡´)
                self.save_document_summaries(summaries)
            
            processed += 1
            if progress_callback:
                status = "âœ… ì™„ë£Œ" if summary else "âŒ ì‹¤íŒ¨"
                progress_callback(processed, total_docs, f"{status} {filename}")
        
        return summaries
    
    def find_relevant_documents(self, query: str, top_k: int = 5) -> List[Tuple[str, int, str]]:
        """ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë“¤ì„ ì°¾ê¸°"""
        summaries = self.load_document_summaries()
        
        if not summaries:
            return []
        
        # ë¬¸ì„œ ìš”ì•½ë“¤ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        summary_text_parts = []
        for filename, summary_data in summaries.items():
            summary_text_parts.append(f"[{filename}]\n{summary_data['summary']}\n")
        
        combined_summaries = "\n".join(summary_text_parts)
        
        try:
            # LLMì„ ì‚¬ìš©í•´ ê´€ë ¨ì„± í‰ê°€
            llm = self.llm_manager.get_llm()
            chain = self.relevance_prompt | llm | StrOutputParser()
            
            relevance_result = chain.invoke({
                "query": query,
                "document_summaries": combined_summaries
            })
            
            # ê²°ê³¼ íŒŒì‹±
            relevant_docs = self._parse_relevance_result(relevance_result)
            
            # ìƒìœ„ kê°œë§Œ ë°˜í™˜
            return relevant_docs[:top_k]
            
        except Exception as e:
            st.error(f"ê´€ë ¨ì„± í‰ê°€ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def _parse_relevance_result(self, result_text: str) -> List[Tuple[str, int, str]]:
        """ê´€ë ¨ì„± í‰ê°€ ê²°ê³¼ íŒŒì‹±"""
        relevant_docs = []
        lines = result_text.strip().split('\n')
        
        for line in lines:
            line = line.strip()
            if '[' in line and ']' in line and 'ì ìˆ˜:' in line:
                try:
                    # [ë¬¸ì„œëª…] ì¶”ì¶œ
                    start_bracket = line.find('[')
                    end_bracket = line.find(']')
                    if start_bracket >= 0 and end_bracket > start_bracket:
                        filename = line[start_bracket+1:end_bracket]
                        
                        # ì ìˆ˜ ì¶”ì¶œ
                        score_start = line.find('ì ìˆ˜:') + 3
                        score_end = line.find('ì ', score_start)
                        if score_start > 2 and score_end > score_start:
                            score_str = line[score_start:score_end].strip()
                            score = int(score_str)
                            
                            # ì„¤ëª… ì¶”ì¶œ
                            explanation_start = line.find('ê´€ë ¨ì„± ì„¤ëª…:')
                            explanation = ""
                            if explanation_start >= 0:
                                explanation = line[explanation_start+7:].strip()
                            
                            relevant_docs.append((filename, score, explanation))
                
                except Exception:
                    continue
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        relevant_docs.sort(key=lambda x: x[1], reverse=True)
        return relevant_docs
    
    def detailed_search(self, filename: str, query: str, top_k: int = 5) -> Dict[str, Any]:
        """ì„ íƒëœ ë¬¸ì„œì— ëŒ€í•œ ìƒì„¸ ê²€ìƒ‰"""
        try:
            # ë¬¸ì„œ ë¡œë“œ ë° ì²­í¬ ìƒì„±
            documents = self.document_loader.process_documents_for_rag([filename])
            
            if not documents:
                return {"error": "ë¬¸ì„œë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
            
            # ì„ì‹œ ë²¡í„° ìŠ¤í† ì–´ ìƒì„± (í•´ë‹¹ ë¬¸ì„œë§Œ)
            embeddings = self.embedding_manager.get_embeddings()
            
            # ìƒˆë¡œìš´ ì„ì‹œ VectorStoreManager ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            from src.utils.vector_store import VectorStoreManager
            temp_collection_name = f"temp_{filename.replace('.', '_').replace(' ', '_')}"
            temp_vector_store_manager = VectorStoreManager(
                embeddings=embeddings,
                vector_store_type=self.vector_store_manager.vector_store_type,
                collection_name=temp_collection_name
            )
            
            # ì„ì‹œ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
            temp_vector_store = temp_vector_store_manager.create_vector_store(documents)
            
            # ê´€ë ¨ ì²­í¬ ê²€ìƒ‰
            retriever = temp_vector_store.as_retriever(search_kwargs={"k": top_k})
            relevant_chunks = retriever.get_relevant_documents(query)
            
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = "\n\n".join([chunk.page_content for chunk in relevant_chunks])
            
            # LLMì„ ì‚¬ìš©í•´ ë‹µë³€ ìƒì„±  
            llm = self.llm_manager.get_llm()
            chain = self.detail_qa_prompt | llm | StrOutputParser()
            
            answer = chain.invoke({
                "context": context,
                "question": query
            })
            
            # ì„ì‹œ ë²¡í„° ìŠ¤í† ì–´ ì •ë¦¬
            try:
                temp_vector_store_manager.cleanup()
            except:
                pass  # ì •ë¦¬ ì‹¤íŒ¨ëŠ” ë¬´ì‹œ
            
            return {
                "filename": filename,
                "answer": answer,
                "relevant_chunks": [
                    {
                        "content": chunk.page_content,
                        "metadata": chunk.metadata
                    }
                    for chunk in relevant_chunks
                ],
                "total_chunks_found": len(relevant_chunks)
            }
            
        except Exception as e:
            return {"error": f"ìƒì„¸ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}"}
    
    def get_document_overview(self, filename: str) -> Dict[str, Any]:
        """ë¬¸ì„œ ê°œìš” ì •ë³´ ë°˜í™˜"""
        summaries = self.load_document_summaries()
        
        if filename in summaries:
            return summaries[filename]
        else:
            # ìºì‹œì— ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
            return self.generate_document_summary(filename) 