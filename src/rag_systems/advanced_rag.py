"""Advanced RAG implementation with enhanced retrieval and processing."""

from typing import List, Dict, Any, Optional, Tuple
import time
import re
import streamlit as st
from langchain_core.documents import Document
from langchain_core.language_models import BaseLLM
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

from ..utils.vector_store import VectorStoreManager
from ..utils.llm_manager import LLMManager


class AdvancedRAG:
    """Advanced RAG implementation with pre-retrieval optimization and post-retrieval processing."""
    
    def __init__(self, vector_store_manager: VectorStoreManager, llm_manager: LLMManager):
        """Initialize Advanced RAG system.
        
        Args:
            vector_store_manager: Vector store manager instance
            llm_manager: LLM manager instance
        """
        self.vector_store_manager = vector_store_manager
        self.llm_manager = llm_manager
        self.name = "Advanced RAG"
        self.description = "í–¥ìƒëœ RAG ì‹œìŠ¤í…œ: ì¿¼ë¦¬ ìµœì í™” + ì¬ìˆœìœ„í™” + ì»¨í…ìŠ¤íŠ¸ ì••ì¶•"
        
    def preprocess_query(self, query: str) -> str:
        """Preprocess and optimize the query with comprehensive expansion and enhancement.
        
        Args:
            query: Original user query
            
        Returns:
            Optimized and expanded query
        """
        # Basic query cleaning
        query = re.sub(r'\s+', ' ', query.strip())
        
        # Comprehensive query expansion with multiple strategies
        expanded_terms = []
        
        # 1. AI & Technology Domain Expansion
        ai_keywords = ["AI", "ì¸ê³µì§€ëŠ¥", "artificial intelligence", "ë¨¸ì‹ ëŸ¬ë‹", "machine learning"]
        if any(keyword.lower() in query.lower() for keyword in ai_keywords):
            expanded_terms.extend([
                "ë”¥ëŸ¬ë‹", "deep learning", "neural network", "ì‹ ê²½ë§", 
                "ìë™í™”", "automation", "ì•Œê³ ë¦¬ì¦˜", "algorithm",
                "ë°ì´í„° ë¶„ì„", "data analysis", "ì˜ˆì¸¡ ëª¨ë¸", "predictive modeling"
            ])
        
        # 2. Business & Work Domain Expansion
        business_keywords = ["ì—…ë¬´", "work", "ì§ì¥", "business", "ë¹„ì¦ˆë‹ˆìŠ¤", "íšŒì‚¬"]
        if any(keyword.lower() in query.lower() for keyword in business_keywords):
            expanded_terms.extend([
                "ìƒì‚°ì„±", "productivity", "íš¨ìœ¨ì„±", "efficiency",
                "ì—…ë¬´ í”„ë¡œì„¸ìŠ¤", "work process", "ìë™í™”", "automation",
                "ë””ì§€í„¸ ì „í™˜", "digital transformation", "í˜ì‹ ", "innovation"
            ])
        
        # 3. Trend & Future Domain Expansion
        trend_keywords = ["íŠ¸ë Œë“œ", "trend", "ë™í–¥", "ì „ë§", "ë¯¸ë˜", "future"]
        if any(keyword.lower() in query.lower() for keyword in trend_keywords):
            expanded_terms.extend([
                "ì‹œì¥ ë™í–¥", "market trend", "ê¸°ìˆ  ë™í–¥", "technology trend",
                "ë°œì „ ë°©í–¥", "development direction", "ë³€í™”", "change",
                "í˜ì‹ ", "innovation", "ì§„í™”", "evolution"
            ])
        
        # 4. Industry & Market Domain Expansion
        industry_keywords = ["ì‚°ì—…", "industry", "ì‹œì¥", "market", "ê¸°ì—…", "company"]
        if any(keyword.lower() in query.lower() for keyword in industry_keywords):
            expanded_terms.extend([
                "ì‹œì¥ ë¶„ì„", "market analysis", "ê²½ìŸ", "competition",
                "ì„±ì¥", "growth", "íˆ¬ì", "investment", "ì „ëµ", "strategy"
            ])
        
        # 5. Analysis & Research Domain Expansion
        analysis_keywords = ["ë¶„ì„", "analysis", "ì—°êµ¬", "research", "ì¡°ì‚¬", "survey"]
        if any(keyword.lower() in query.lower() for keyword in analysis_keywords):
            expanded_terms.extend([
                "ë°ì´í„° ë¶„ì„", "data analysis", "í†µê³„", "statistics",
                "ì¡°ì‚¬ ê²°ê³¼", "survey results", "ì—°êµ¬ ë³´ê³ ì„œ", "research report"
            ])
        
        # 6. Implementation & Strategy Domain Expansion
        strategy_keywords = ["ë„ì…", "implementation", "ì „ëµ", "strategy", "ë°©ì•ˆ", "plan"]
        if any(keyword.lower() in query.lower() for keyword in strategy_keywords):
            expanded_terms.extend([
                "ì‹¤í–‰ ê³„íš", "execution plan", "ë¡œë“œë§µ", "roadmap",
                "ë‹¨ê³„ë³„ ì ‘ê·¼", "step-by-step approach", "ì„±ê³µ ì‚¬ë¡€", "success case"
            ])
        
        # 7. Performance & Quality Domain Expansion
        performance_keywords = ["ì„±ëŠ¥", "performance", "í’ˆì§ˆ", "quality", "íš¨ìœ¨ì„±", "efficiency"]
        if any(keyword.lower() in query.lower() for keyword in performance_keywords):
            expanded_terms.extend([
                "ìµœì í™”", "optimization", "ê°œì„ ", "improvement",
                "ì¸¡ì •", "measurement", "í‰ê°€", "evaluation", "ë²¤ì¹˜ë§ˆí¬", "benchmark"
            ])
        
        # 8. Impact & Effect Domain Expansion
        impact_keywords = ["ì˜í–¥", "impact", "íš¨ê³¼", "effect", "ë³€í™”", "change"]
        if any(keyword.lower() in query.lower() for keyword in impact_keywords):
            expanded_terms.extend([
                "ê²°ê³¼", "result", "ì„±ê³¼", "outcome", "ê°œì„  íš¨ê³¼", "improvement effect",
                "ë³€í™” ë¶„ì„", "change analysis", "ì˜í–¥ í‰ê°€", "impact assessment"
            ])
        
        # 9. Technology Application Domain Expansion
        tech_app_keywords = ["ìë™í™”", "automation", "ë””ì§€í„¸í™”", "digitalization", "í˜ì‹ ", "innovation"]
        if any(keyword.lower() in query.lower() for keyword in tech_app_keywords):
            expanded_terms.extend([
                "ìŠ¤ë§ˆíŠ¸ íŒ©í† ë¦¬", "smart factory", "IoT", "ì¸í„°ë„· of things",
                "í´ë¼ìš°ë“œ", "cloud", "ë¹…ë°ì´í„°", "big data", "ë¸”ë¡ì²´ì¸", "blockchain"
            ])
        
        # 10. Temporal & Comparative Domain Expansion
        temporal_keywords = ["í˜„ì¬", "current", "ë¯¸ë˜", "future", "ê³¼ê±°", "past", "ë¹„êµ", "compare"]
        if any(keyword.lower() in query.lower() for keyword in temporal_keywords):
            expanded_terms.extend([
                "ì‹œê³„ì—´ ë¶„ì„", "time series analysis", "íŠ¸ë Œë“œ ë¹„êµ", "trend comparison",
                "ë³€í™” ì¶”ì´", "change trend", "ì˜ˆì¸¡", "prediction", "ì „ë§", "outlook"
            ])
        
        # Remove duplicates and limit expansion terms to prevent query bloat
        unique_terms = list(dict.fromkeys(expanded_terms))  # Preserve order while removing duplicates
        
        # Debug: Show expansion process
        st.write("**ğŸ” ì¿¼ë¦¬ í™•ì¥ ë¶„ì„:**")
        st.write(f"- ì›ë³¸ ì¿¼ë¦¬: `{query}`")
        st.write(f"- ë°œê²¬ëœ í™•ì¥ ìš©ì–´ ({len(expanded_terms)}ê°œ): {expanded_terms[:10]}{'...' if len(expanded_terms) > 10 else ''}")
        st.write(f"- ì¤‘ë³µ ì œê±° í›„ ({len(unique_terms)}ê°œ): {unique_terms[:10]}{'...' if len(unique_terms) > 10 else ''}")
        
        # Smart expansion: ë„ë©”ì¸ ë§¤ì¹­ê³¼ ì§ˆë¬¸ ë³µì¡ë„ ê¸°ë°˜ ë™ì  í™•ì¥
        query_lower = query.lower()
        
        # ë§¤ì¹­ëœ ë„ë©”ì¸ ìˆ˜ ê³„ì‚°
        domain_keywords = {
            "AI": ["ai", "ì¸ê³µì§€ëŠ¥", "artificial intelligence", "ë¨¸ì‹ ëŸ¬ë‹", "machine learning"],
            "ë¹„ì¦ˆë‹ˆìŠ¤": ["ì—…ë¬´", "work", "ì§ì¥", "business", "ë¹„ì¦ˆë‹ˆìŠ¤", "íšŒì‚¬", "ì·¨ì—…", "job"],
            "íŠ¸ë Œë“œ": ["íŠ¸ë Œë“œ", "trend", "ë™í–¥", "ì „ë§", "ë¯¸ë˜", "future"],
            "ì „ëµ": ["ë„ì…", "implementation", "ì „ëµ", "strategy", "ë°©ì•ˆ", "plan", "ì¤€ë¹„"],
            "ë¶„ì„": ["ë¶„ì„", "analysis", "ì—°êµ¬", "research", "ì¡°ì‚¬", "survey"],
            "ê¸°ìˆ ": ["ìë™í™”", "automation", "ë””ì§€í„¸í™”", "digitalization", "í˜ì‹ ", "innovation"]
        }
        
        matched_domains = 0
        for domain, keywords in domain_keywords.items():
            if any(keyword in query_lower for keyword in keywords):
                matched_domains += 1
        
        # ì§ˆë¬¸ ë³µì¡ë„ ì§€í‘œë“¤
        question_marks = query.count('?') + query.count('ï¼Ÿ')
        sentence_count = len([s for s in query.split('.') if s.strip()])
        word_count = len(query.split())
        
        # ë™ì  í™•ì¥ ìˆ˜ ê³„ì‚°
        base_expansion = min(matched_domains * 2, 6)  # ë„ë©”ì¸ë‹¹ 2ê°œ, ìµœëŒ€ 6ê°œ
        
        # ë³µì¡í•œ ì§ˆë¬¸ì— ëŒ€í•œ ë³´ë„ˆìŠ¤
        if question_marks > 1:  # ë³µìˆ˜ ì§ˆë¬¸
            base_expansion += 2
        if word_count > 15:  # ê¸´ ì§ˆë¬¸
            base_expansion += 1
        if sentence_count > 1:  # ë³µí•© ë¬¸ì¥
            base_expansion += 1
            
        # ìµœì†Œ/ìµœëŒ€ ì œí•œ
        max_terms = max(3, min(base_expansion, 8))  # ìµœì†Œ 3ê°œ, ìµœëŒ€ 8ê°œ
        
        selected_terms = unique_terms[:max_terms]
        st.write(f"- ë§¤ì¹­ëœ ë„ë©”ì¸: {matched_domains}ê°œ")
        st.write(f"- ì§ˆë¬¸ ë³µì¡ë„: ì§ˆë¬¸ìˆ˜({question_marks}), ë¬¸ì¥ìˆ˜({sentence_count}), ë‹¨ì–´ìˆ˜({word_count})")
        st.write(f"- ë™ì  ê³„ì‚°ëœ í™•ì¥ ìˆ˜: {max_terms}ê°œ")
        st.write(f"- ì„ íƒëœ ìš©ì–´ ({len(selected_terms)}ê°œ): {selected_terms}")
        
        # Construct enhanced query
        if selected_terms:
            enhanced_query = f"{query} {' '.join(selected_terms)}"
            st.success(f"ğŸ” **ìµœì¢… í™•ì¥ ì¿¼ë¦¬:** `{enhanced_query}`")
            return enhanced_query
        else:
            st.info("ğŸ” **í™•ì¥ ìš©ì–´ ì—†ìŒ** - ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©")
        
        return query
    
    def retrieve_with_scores(self, query: str, k: int = 10) -> List[Tuple[Document, float]]:
        """Retrieve documents with similarity scores.
        
        Args:
            query: Search query
            k: Number of documents to retrieve (before reranking)
            
        Returns:
            List of (document, score) tuples
        """
        vector_store = self.vector_store_manager.get_vector_store()
        if vector_store is None:
            st.error("ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
            
        try:
            docs_with_scores = vector_store.similarity_search_with_score(query, k=k)
            return docs_with_scores
            
        except Exception as e:
            st.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def rerank_documents(self, query: str, docs_with_scores: List[Tuple[Document, float]], top_k: int = 5) -> List[Document]:
        """Rerank documents using TF-IDF similarity.
        
        Args:
            query: Original query
            docs_with_scores: Documents with initial scores
            top_k: Number of top documents to return after reranking
            
        Returns:
            Reranked documents
        """
        if not docs_with_scores:
            return []
            
        documents = [doc for doc, _ in docs_with_scores]
        
        # Prepare texts for TF-IDF
        texts = [doc.page_content for doc in documents]
        texts.append(query)  # Add query as the last document
        
        try:
            # Calculate TF-IDF similarity
            vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)
            tfidf_matrix = vectorizer.fit_transform(texts)
            
            # Calculate similarity between query and documents
            query_vector = tfidf_matrix[-1]  # Last vector is the query
            doc_vectors = tfidf_matrix[:-1]  # All vectors except the last one
            
            similarities = cosine_similarity(query_vector, doc_vectors).flatten()
            
            # Combine original scores with TF-IDF scores
            combined_scores = []
            for i, (doc, original_score) in enumerate(docs_with_scores):
                tfidf_score = similarities[i]
                # Weighted combination (70% TF-IDF, 30% original embedding score)
                combined_score = 0.7 * tfidf_score + 0.3 * (1 - original_score)  # Lower distance = higher score
                combined_scores.append((doc, combined_score))
            
            # Sort by combined score (descending)
            combined_scores.sort(key=lambda x: x[1], reverse=True)
            
            # Return top-k documents
            reranked_docs = [doc for doc, _ in combined_scores[:top_k]]
            return reranked_docs
            
        except Exception as e:
            st.warning(f"ì¬ìˆœìœ„í™” ì‹¤íŒ¨, ì›ë³¸ ìˆœì„œ ì‚¬ìš©: {str(e)}")
            return [doc for doc, _ in docs_with_scores[:top_k]]
    
    def compress_context(self, docs: List[Document], max_length: int = 3000) -> str:
        """Compress context by selecting most relevant sentences.
        
        Args:
            docs: List of documents
            max_length: Maximum context length
            
        Returns:
            Compressed context
        """
        if not docs:
            return ""
            
        # Combine all text
        full_context = "\n\n".join([doc.page_content for doc in docs])
        
        if len(full_context) <= max_length:
            return full_context
            
        # Split into sentences
        sentences = re.split(r'[.!?]\s+', full_context)
        
        # Simple sentence scoring based on length and content
        scored_sentences = []
        for sentence in sentences:
            if len(sentence.strip()) < 20:  # Skip very short sentences
                continue
                
            # Score based on keywords and length
            score = len(sentence) * 0.1
            if any(keyword in sentence.lower() for keyword in ['ai', 'ì¸ê³µì§€ëŠ¥', 'íŠ¸ë Œë“œ', 'ë¯¸ë˜', 'ì—…ë¬´']):
                score *= 1.5
                
            scored_sentences.append((sentence.strip(), score))
        
        # Sort by score and select top sentences
        scored_sentences.sort(key=lambda x: x[1], reverse=True)
        
        compressed_context = ""
        for sentence, _ in scored_sentences:
            if len(compressed_context) + len(sentence) <= max_length:
                compressed_context += sentence + ". "
            else:
                break
                
        return compressed_context.strip()
    
    def generate_with_reasoning(self, query: str, context: str) -> str:
        """Generate answer with explicit reasoning steps.
        
        Args:
            query: User query
            context: Retrieved and processed context
            
        Returns:
            Generated answer with reasoning
        """
        st.write("**ë‹µë³€ ìƒì„± ì¤‘ (ì¶”ë¡  ê¸°ë°˜)...**")
        answer_placeholder = st.empty()
        
        start_time = time.time()
        full_response = ""
        
        # Stream the response
        for chunk in self.llm_manager.generate_response_stream(
            prompt=query,
            context=context
        ):
            full_response += chunk
            answer_placeholder.markdown(full_response + "â–Œ")
        
        generation_time = time.time() - start_time
        answer_placeholder.markdown(full_response)
        
        st.success(f"ì¶”ë¡  ê¸°ë°˜ ë‹µë³€ ìƒì„± ì™„ë£Œ ({generation_time:.2f}ì´ˆ)")
        return full_response
    
    def query(self, question: str, k: int = 10, rerank_top_k: int = 5) -> Dict[str, Any]:
        """Process a query end-to-end with advanced techniques.
        
        Args:
            question: User question
            k: Number of documents to retrieve initially
            rerank_top_k: Number of documents after reranking
            
        Returns:
            Dictionary with results and metadata
        """
        start_time = time.time()
        
        # Step 1: Query preprocessing
        st.subheader("ğŸ”§ 1ë‹¨ê³„: ì¿¼ë¦¬ ì „ì²˜ë¦¬")
        optimized_query = self.preprocess_query(question)
        if optimized_query != question:
            st.info(f"ìµœì í™”ëœ ì¿¼ë¦¬: {optimized_query}")
        else:
            st.info("ì¿¼ë¦¬ ìµœì í™”: ë³€ê²½ì‚¬í•­ ì—†ìŒ")
        
        # Step 2: Retrieve with scores
        st.subheader("ğŸ” 2ë‹¨ê³„: ë¬¸ì„œ ê²€ìƒ‰")
        docs_with_scores = self.retrieve_with_scores(optimized_query, k=k)
        
        if not docs_with_scores:
            return {
                "question": question,
                "answer": "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "retrieved_docs": [],
                "total_time": time.time() - start_time,
                "rag_type": self.name
            }
        
        st.success(f"ì´ˆê¸° ê²€ìƒ‰: {len(docs_with_scores)}ê°œ ë¬¸ì„œ")
        
        # Step 3: Rerank documents
        st.subheader("ğŸ“Š 3ë‹¨ê³„: ë¬¸ì„œ ì¬ìˆœìœ„í™”")
        reranked_docs = self.rerank_documents(optimized_query, docs_with_scores, rerank_top_k)
        st.success(f"ì¬ìˆœìœ„í™” ì™„ë£Œ: ìƒìœ„ {len(reranked_docs)}ê°œ ë¬¸ì„œ ì„ íƒ")
        
        # Display reranked documents
        with st.expander(f"ì¬ìˆœìœ„í™”ëœ ë¬¸ì„œ ({len(reranked_docs)}ê°œ)"):
            for i, doc in enumerate(reranked_docs):
                st.write(f"**ë¬¸ì„œ {i+1}:**")
                st.write(f"ì¶œì²˜: {doc.metadata.get('source', 'Unknown')}")
                st.write(f"ë‚´ìš©: {doc.page_content[:200]}...")
                st.divider()
        
        # Step 4: Context compression
        st.subheader("ğŸ—œï¸ 4ë‹¨ê³„: ì»¨í…ìŠ¤íŠ¸ ì••ì¶•")
        compressed_context = self.compress_context(reranked_docs)
        compression_ratio = len(compressed_context) / sum(len(doc.page_content) for doc in reranked_docs) if reranked_docs else 0
        st.info(f"ì••ì¶•ë¥ : {compression_ratio:.2%}")
        
        # Step 5: Generate answer with reasoning
        st.subheader("ğŸ¤– 5ë‹¨ê³„: ë‹µë³€ ìƒì„±")
        answer = self.generate_with_reasoning(question, compressed_context)
        
        total_time = time.time() - start_time
        
        return {
            "question": question,
            "optimized_query": optimized_query,
            "answer": answer,
            "retrieved_docs": reranked_docs,
            "total_time": total_time,
            "rag_type": self.name,
            "metadata": {
                "initial_retrieved": len(docs_with_scores),
                "final_retrieved": len(reranked_docs),
                "compression_ratio": compression_ratio,
                "retrieval_method": "similarity_search + reranking",
                "generation_method": "reasoning-based"
            }
        }
    
    def get_system_info(self) -> Dict[str, Any]:
        """Get information about the Advanced RAG system.
        
        Returns:
            Dictionary with system information
        """
        return {
            "name": self.name,
            "description": self.description,
            "components": [
                "Query Preprocessor",
                "Vector Store (Enhanced Search)",
                "Document Reranker (TF-IDF)",
                "Context Compressor",
                "LLM (Reasoning-based Generation)"
            ],
            "features": [
                "ì¿¼ë¦¬ í™•ì¥ ë° ìµœì í™”",
                "TF-IDF ê¸°ë°˜ ë¬¸ì„œ ì¬ìˆœìœ„í™”",
                "ì»¨í…ìŠ¤íŠ¸ ì••ì¶•",
                "ì¶”ë¡  ê¸°ë°˜ ë‹µë³€ ìƒì„±"
            ],
            "advantages": [
                "í–¥ìƒëœ ê²€ìƒ‰ ì •í™•ë„",
                "íš¨ìœ¨ì ì¸ ì»¨í…ìŠ¤íŠ¸ í™œìš©",
                "ë” ì •í™•í•œ ë‹µë³€ ìƒì„±"
            ]
        } 