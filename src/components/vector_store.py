"""
ë²¡í„° ìŠ¤í† ì–´ ê´€ë¦¬ ì»´í¬ë„ŒíŠ¸
"""
import streamlit as st
from typing import List, Dict, Any, Optional, Tuple
import faiss
import numpy as np
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_community.vectorstores import Chroma
from langchain_core.vectorstores import VectorStore

try:
    from langchain_milvus import Milvus
    MILVUS_AVAILABLE = True
except ImportError:
    MILVUS_AVAILABLE = False
    Milvus = None

from src.components.embeddings import EmbeddingManager

try:
    from src.config.settings import TOP_K, MILVUS_URI, MILVUS_TOKEN, MILVUS_COLLECTION_NAME
except ImportError:
    # ê¸°ë³¸ê°’ ì‚¬ìš©
    TOP_K = 5
    MILVUS_URI = "./milvus_local.db"
    MILVUS_TOKEN = ""
    MILVUS_COLLECTION_NAME = "rag_documents"


class VectorStoreManager:
    """ë²¡í„° ìŠ¤í† ì–´ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, embedding_manager: EmbeddingManager, store_type: str = "faiss"):
        self.embedding_manager = embedding_manager
        self.store_type = store_type
        self.vector_store: Optional[VectorStore] = None
        self.documents: List[Document] = []
    
    def create_vector_store(self, documents: List[Document]) -> VectorStore:
        """ë²¡í„° ìŠ¤í† ì–´ ìƒì„±"""
        if not documents:
            raise ValueError("ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        self.documents = documents
        embeddings = self.embedding_manager.get_embeddings()
        
        try:
            if self.store_type == "faiss":
                self.vector_store = FAISS.from_documents(documents, embeddings)
            elif self.store_type == "chroma":
                self.vector_store = Chroma.from_documents(documents, embeddings)
            elif self.store_type == "milvus":
                if not MILVUS_AVAILABLE:
                    raise ImportError(
                        "Milvus ì‚¬ìš©ì„ ìœ„í•´ langchain-milvusì™€ pymilvusë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: "
                        "pip install langchain-milvus pymilvus"
                    )
                
                # Milvus ì—°ê²° ì¸ì ì„¤ì •
                connection_args = {"uri": MILVUS_URI}
                if MILVUS_TOKEN:
                    connection_args["token"] = MILVUS_TOKEN
                
                self.vector_store = Milvus.from_documents(
                    documents,
                    embeddings,
                    collection_name=MILVUS_COLLECTION_NAME,
                    connection_args=connection_args,
                    drop_old=False  # ê¸°ì¡´ ì»¬ë ‰ì…˜ ìœ ì§€
                )
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë²¡í„° ìŠ¤í† ì–´ íƒ€ì…: {self.store_type}")
            
            return self.vector_store
            
        except Exception as e:
            st.error(f"ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise
    
    def similarity_search(self, query: str, k: int = TOP_K) -> List[Document]:
        """ìœ ì‚¬ë„ ê²€ìƒ‰"""
        if not self.vector_store:
            raise ValueError("ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        return self.vector_store.similarity_search(query, k=k)
    
    def similarity_search_with_score(self, query: str, k: int = TOP_K) -> List[Tuple[Document, float]]:
        """ì ìˆ˜ì™€ í•¨ê»˜ ìœ ì‚¬ë„ ê²€ìƒ‰"""
        if not self.vector_store:
            raise ValueError("ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        return self.vector_store.similarity_search_with_score(query, k=k)
    
    def get_retriever(self, search_type: str = "similarity", search_kwargs: Optional[Dict] = None):
        """ë¦¬íŠ¸ë¦¬ë²„ ë°˜í™˜"""
        if not self.vector_store:
            raise ValueError("ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        if search_kwargs is None:
            search_kwargs = {"k": TOP_K}
        
        return self.vector_store.as_retriever(
            search_type=search_type,
            search_kwargs=search_kwargs
        )
    
    def add_documents(self, documents: List[Document]):
        """ë¬¸ì„œ ì¶”ê°€"""
        if not self.vector_store:
            raise ValueError("ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        self.vector_store.add_documents(documents)
        self.documents.extend(documents)
    
    def get_store_info(self) -> Dict[str, Any]:
        """ë²¡í„° ìŠ¤í† ì–´ ì •ë³´ ë°˜í™˜"""
        info = {
            "store_type": self.store_type,
            "document_count": len(self.documents),
            "embedding_model": self.embedding_manager.model_name
        }
        
        if self.vector_store and hasattr(self.vector_store, 'index'):
            if hasattr(self.vector_store.index, 'ntotal'):
                info["vector_count"] = self.vector_store.index.ntotal
        
        return info


class HybridVectorStore:
    """í•˜ì´ë¸Œë¦¬ë“œ ë²¡í„° ìŠ¤í† ì–´ (ë°€ì§‘ + í¬ì†Œ)"""
    
    def __init__(self, embedding_manager: EmbeddingManager):
        self.embedding_manager = embedding_manager
        self.dense_store: Optional[VectorStore] = None
        self.sparse_index = None
        self.documents: List[Document] = []
        self._init_sparse_index()
    
    def _init_sparse_index(self):
        """í¬ì†Œ ì¸ë±ìŠ¤ ì´ˆê¸°í™”"""
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import cosine_similarity
        
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=5000,
            stop_words='english',
            ngram_range=(1, 2)
        )
    
    def create_hybrid_store(self, documents: List[Document]) -> None:
        """í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤í† ì–´ ìƒì„±"""
        self.documents = documents
        
        # ë°€ì§‘ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        embeddings = self.embedding_manager.get_embeddings()
        self.dense_store = FAISS.from_documents(documents, embeddings)
        
        # í¬ì†Œ ì¸ë±ìŠ¤ ìƒì„±
        texts = [doc.page_content for doc in documents]
        self.sparse_index = self.tfidf_vectorizer.fit_transform(texts)
    
    def hybrid_search(self, query: str, k: int = TOP_K, alpha: float = 0.7) -> List[Tuple[Document, float]]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (alpha: ë°€ì§‘ ë²¡í„° ê°€ì¤‘ì¹˜)"""
        if not self.dense_store or self.sparse_index is None:
            raise ValueError("í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ë°€ì§‘ ë²¡í„° ê²€ìƒ‰
        dense_results = self.dense_store.similarity_search_with_score(query, k=k*2)
        
        # í¬ì†Œ ë²¡í„° ê²€ìƒ‰
        query_sparse = self.tfidf_vectorizer.transform([query])
        sparse_scores = cosine_similarity(query_sparse, self.sparse_index).flatten()
        
        # ê²°ê³¼ ê²°í•©
        combined_results = []
        
        for doc, dense_score in dense_results:
            # ë¬¸ì„œ ì¸ë±ìŠ¤ ì°¾ê¸°
            doc_idx = next((i for i, d in enumerate(self.documents) 
                           if d.page_content == doc.page_content), -1)
            
            if doc_idx >= 0:
                sparse_score = sparse_scores[doc_idx]
                # ì ìˆ˜ ì •ê·œí™” (0-1 ë²”ìœ„)
                normalized_dense = 1 / (1 + dense_score) if dense_score > 0 else 0
                normalized_sparse = sparse_score
                
                # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
                hybrid_score = alpha * normalized_dense + (1 - alpha) * normalized_sparse
                combined_results.append((doc, hybrid_score))
        
        # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        combined_results.sort(key=lambda x: x[1], reverse=True)
        
        return combined_results[:k]


class MultiModalVectorStore:
    """ë©€í‹°ëª¨ë‹¬ ë²¡í„° ìŠ¤í† ì–´ (í–¥í›„ í™•ì¥ìš©)"""
    
    def __init__(self, embedding_manager: EmbeddingManager):
        self.embedding_manager = embedding_manager
        self.text_store: Optional[VectorStore] = None
        # í–¥í›„ ì´ë¯¸ì§€, í…Œì´ë¸” ë“± ë‹¤ë¥¸ ëª¨ë‹¬ë¦¬í‹° ì¶”ê°€ ê°€ëŠ¥
    
    def create_text_store(self, documents: List[Document]) -> VectorStore:
        """í…ìŠ¤íŠ¸ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±"""
        embeddings = self.embedding_manager.get_embeddings()
        self.text_store = FAISS.from_documents(documents, embeddings)
        return self.text_store
    
    # TODO: ì´ë¯¸ì§€, í…Œì´ë¸” ë“± ë‹¤ë¥¸ ëª¨ë‹¬ë¦¬í‹° ì§€ì› ì¶”ê°€


def create_vector_store(documents: List[Document], 
                       embedding_manager: EmbeddingManager,
                       store_type: str = "faiss") -> VectorStoreManager:
    """ë²¡í„° ìŠ¤í† ì–´ íŒ©í† ë¦¬ í•¨ìˆ˜"""
    manager = VectorStoreManager(embedding_manager, store_type)
    manager.create_vector_store(documents)
    return manager


@st.cache_data
def get_cached_vector_store_info(store_type: str, doc_count: int) -> Dict[str, Any]:
    """ìºì‹œëœ ë²¡í„° ìŠ¤í† ì–´ ì •ë³´"""
    return {
        "store_type": store_type,
        "document_count": doc_count,
        "status": "ready"
    }


def display_vector_store_info(vector_store_manager: VectorStoreManager):
    """ë²¡í„° ìŠ¤í† ì–´ ì •ë³´ í‘œì‹œ"""
    info = vector_store_manager.get_store_info()
    
    st.sidebar.markdown("### ğŸ“Š ë²¡í„° ìŠ¤í† ì–´ ì •ë³´")
    st.sidebar.markdown(f"**íƒ€ì…**: {info['store_type']}")
    st.sidebar.markdown(f"**ë¬¸ì„œ ìˆ˜**: {info['document_count']}")
    
    if "vector_count" in info:
        st.sidebar.markdown(f"**ë²¡í„° ìˆ˜**: {info['vector_count']}") 