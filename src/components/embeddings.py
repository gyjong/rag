"""
ì„ë² ë”© ìƒì„± ë° ê´€ë¦¬ ì»´í¬ë„ŒíŠ¸
"""
import streamlit as st
from typing import List, Optional
from pathlib import Path
from langchain_core.embeddings import Embeddings
from langchain_huggingface import HuggingFaceEmbeddings
import numpy as np

from src.config.settings import EMBEDDING_MODEL
from src.config import MODELS_FOLDER


class CustomEmbeddings(Embeddings):
    """ì»¤ìŠ¤í…€ ì„ë² ë”© í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str = EMBEDDING_MODEL):
        self.model_name = model_name
        # Use HuggingFace embeddings instead of SentenceTransformer
        cache_folder = str(MODELS_FOLDER / "embeddings")
        self.model = HuggingFaceEmbeddings(
            model_name=model_name,
            cache_folder=cache_folder,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ì— ëŒ€í•œ ì„ë² ë”© ìƒì„±"""
        return self.model.embed_documents(texts)
    
    def embed_query(self, text: str) -> List[float]:
        """ë‹¨ì¼ ì¿¼ë¦¬ì— ëŒ€í•œ ì„ë² ë”© ìƒì„±"""
        return self.model.embed_query(text)


class EmbeddingManager:
    """ì„ë² ë”© ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str = EMBEDDING_MODEL):
        self.model_name = model_name
        self.embeddings = self._load_embedding_model()
    
    @st.cache_resource
    def _load_embedding_model(_self) -> Embeddings:
        """ì„ë² ë”© ëª¨ë¸ ë¡œë”© (ìºì‹œë¨)"""
        try:
            # Set cache directory to local models folder
            cache_folder = str(MODELS_FOLDER / "embeddings")
            MODELS_FOLDER.mkdir(exist_ok=True)
            
            # HuggingFace Embeddings ì‚¬ìš©
            embeddings = HuggingFaceEmbeddings(
                model_name=_self.model_name,
                cache_folder=cache_folder,
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
            return embeddings
        except Exception as e:
            st.error(f"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            # í´ë°±ìœ¼ë¡œ ì»¤ìŠ¤í…€ ì„ë² ë”© ì‚¬ìš©
            return CustomEmbeddings(_self.model_name)
    
    def get_embeddings(self) -> Embeddings:
        """ì„ë² ë”© ê°ì²´ ë°˜í™˜"""
        return self.embeddings
    
    def compute_similarity(self, query_embedding: List[float], 
                          doc_embeddings: List[List[float]]) -> List[float]:
        """ì¿¼ë¦¬ì™€ ë¬¸ì„œë“¤ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°"""
        query_vec = np.array(query_embedding)
        doc_vecs = np.array(doc_embeddings)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = np.dot(doc_vecs, query_vec) / (
            np.linalg.norm(doc_vecs, axis=1) * np.linalg.norm(query_vec)
        )
        
        return similarities.tolist()
    
    def get_model_info(self) -> dict:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "model_name": self.model_name,
            "dimension": self._get_embedding_dimension(),
            "max_sequence_length": self._get_max_sequence_length()
        }
    
    def _get_embedding_dimension(self) -> int:
        """ì„ë² ë”© ì°¨ì› ìˆ˜ ë°˜í™˜"""
        try:
            test_embedding = self.embeddings.embed_query("test")
            return len(test_embedding)
        except:
            return 384  # ê¸°ë³¸ê°’
    
    def _get_max_sequence_length(self) -> int:
        """ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ ë°˜í™˜"""
        try:
            if hasattr(self.embeddings, 'client') and hasattr(self.embeddings.client, 'max_seq_length'):
                return self.embeddings.client.max_seq_length
            return 512  # ê¸°ë³¸ê°’
        except:
            return 512


class HybridEmbeddings:
    """í•˜ì´ë¸Œë¦¬ë“œ ì„ë² ë”© (ë°€ì§‘ + í¬ì†Œ)"""
    
    def __init__(self, dense_model: str = EMBEDDING_MODEL):
        self.dense_embeddings = EmbeddingManager(dense_model)
        self.sparse_embeddings = self._init_sparse_embeddings()
    
    def _init_sparse_embeddings(self):
        """í¬ì†Œ ì„ë² ë”© ì´ˆê¸°í™” (TF-IDF ë“±)"""
        from sklearn.feature_extraction.text import TfidfVectorizer
        return TfidfVectorizer(
            max_features=1000,
            stop_words='english',
            ngram_range=(1, 2)
        )
    
    def fit_sparse(self, texts: List[str]):
        """í¬ì†Œ ì„ë² ë”© í•™ìŠµ"""
        self.sparse_embeddings.fit(texts)
    
    def get_dense_embeddings(self, texts: List[str]) -> List[List[float]]:
        """ë°€ì§‘ ì„ë² ë”© ìƒì„±"""
        return self.dense_embeddings.get_embeddings().embed_documents(texts)
    
    def get_sparse_embeddings(self, texts: List[str]) -> np.ndarray:
        """í¬ì†Œ ì„ë² ë”© ìƒì„±"""
        return self.sparse_embeddings.transform(texts).toarray()
    
    def get_hybrid_scores(self, query: str, texts: List[str], 
                         alpha: float = 0.7) -> List[float]:
        """í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚° (alpha: ë°€ì§‘ ê°€ì¤‘ì¹˜)"""
        # ë°€ì§‘ ì„ë² ë”© ìœ ì‚¬ë„
        query_dense = self.dense_embeddings.get_embeddings().embed_query(query)
        doc_dense = self.get_dense_embeddings(texts)
        dense_scores = self.dense_embeddings.compute_similarity(query_dense, doc_dense)
        
        # í¬ì†Œ ì„ë² ë”© ìœ ì‚¬ë„
        query_sparse = self.sparse_embeddings.transform([query]).toarray()[0]
        doc_sparse = self.get_sparse_embeddings(texts)
        sparse_scores = np.dot(doc_sparse, query_sparse) / (
            np.linalg.norm(doc_sparse, axis=1) * np.linalg.norm(query_sparse)
        )
        
        # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜
        hybrid_scores = alpha * np.array(dense_scores) + (1 - alpha) * sparse_scores
        return hybrid_scores.tolist()


def get_embedding_manager(model_name: Optional[str] = None) -> EmbeddingManager:
    """ì„ë² ë”© ë§¤ë‹ˆì € íŒ©í† ë¦¬ í•¨ìˆ˜"""
    if model_name is None:
        model_name = EMBEDDING_MODEL
    
    return EmbeddingManager(model_name)


def display_embedding_info(embedding_manager: EmbeddingManager):
    """ì„ë² ë”© ëª¨ë¸ ì •ë³´ í‘œì‹œ"""
    info = embedding_manager.get_model_info()
    
    st.sidebar.markdown("### ğŸ”§ ì„ë² ë”© ëª¨ë¸ ì •ë³´")
    st.sidebar.markdown(f"**ëª¨ë¸**: {info['model_name']}")
    st.sidebar.markdown(f"**ì°¨ì›**: {info['dimension']}")
    st.sidebar.markdown(f"**ìµœëŒ€ ê¸¸ì´**: {info['max_sequence_length']}") 