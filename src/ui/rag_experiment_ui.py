"""RAG experiment UI module for the RAG application."""

import streamlit as st
import time
from typing import Dict, Any, Optional, List

from ..config import *
from ..config.settings import MAX_ITERATIONS, RERANK_TOP_K
from ..utils.llm_manager import LLMManager
from ..utils.vector_store import VectorStoreManager
from ..rag_systems.naive_rag import get_naive_rag_system_info
from ..rag_systems.advanced_rag import get_advanced_rag_system_info
from ..rag_systems.modular_rag import get_modular_rag_system_info, BM25
from ..graphs.naive_rag_graph import create_naive_rag_graph
from ..graphs.advanced_rag_graph import create_advanced_rag_graph
from ..graphs.modular_rag_graph import create_modular_rag_graph


class RAGExperimentUI:
    """UI components for RAG experiment functionality."""
    
    @staticmethod
    def display_rag_experiment_tab():
        """RAG experiment tab with various systems."""
        st.header("ğŸ§ª RAG ì‹œìŠ¤í…œ ì‹¤í—˜")
        
        # ë²¡í„° ìŠ¤í† ì–´ ìƒíƒœë¥¼ í™•ì¸í•˜ê³  UIì— í‘œì‹œ
        is_ready = RAGExperimentUI._check_vector_store()
        
        # ë²¡í„° ìŠ¤í† ì–´ê°€ ì¤€ë¹„ë˜ì—ˆì„ ë•Œë§Œ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        if is_ready:
            RAGExperimentUI._initialize_rag_systems()
        
        # í•­ìƒ ì‹¤í—˜ ì¸í„°í˜ì´ìŠ¤ë¥¼ í‘œì‹œ
        RAGExperimentUI._display_experiment_interface(is_ready)
    
    @staticmethod
    def _check_vector_store():
        """ë²¡í„° ìŠ¤í† ì–´ ìƒíƒœë¥¼ í™•ì¸í•˜ê³  UIì— í‘œì‹œí•˜ë©°, ì¤€ë¹„ ì—¬ë¶€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        vector_store_manager = st.session_state.get("vector_store_manager")
        vector_store = None
        
        if vector_store_manager:
            try:
                vector_store = vector_store_manager.get_vector_store()
            except Exception as e:
                st.warning(f"âš ï¸ ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ í™•ì¸ ì‹¤íŒ¨: {str(e)}")
        
        if vector_store is None:
            st.warning("ğŸ“‹ ë²¡í„° ìŠ¤í† ì–´ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            st.info("**ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”:**\n"
                     "1. **ğŸ“š ë¬¸ì„œ ë¡œë”©** íƒ­ì—ì„œ ë¬¸ì„œë¥¼ ë¡œë“œí•œ í›„ **ğŸ” ë²¡í„° ìŠ¤í† ì–´** íƒ­ì—ì„œ ìƒˆ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\n"
                     "2. **ğŸ” ë²¡í„° ìŠ¤í† ì–´** íƒ­ì—ì„œ ê¸°ì¡´ì— ì €ì¥ëœ ë²¡í„° ìŠ¤í† ì–´ ë¡œë”©")
            return False
        
        st.success("âœ… ë²¡í„° ìŠ¤í† ì–´ ì¤€ë¹„ ì™„ë£Œ!")
        RAGExperimentUI._display_vector_store_status(vector_store)
        return True
    
    @staticmethod
    def _display_vector_store_status(vector_store):
        """Display current vector store status with detailed information."""
        with st.expander("ğŸ“Š í˜„ì¬ ë²¡í„° ìŠ¤í† ì–´ ì •ë³´", expanded=True):
            try:
                # Get vector store manager from session state
                vector_store_manager = st.session_state.get("vector_store_manager")
                
                if vector_store_manager:
                    # Get collection stats
                    stats = vector_store_manager.get_collection_stats()
                    
                    # Get vector store metadata from both sources
                    manager_metadata = getattr(vector_store_manager, '_metadata', {})
                    session_metadata = st.session_state.get('vector_store_metadata', {})
                    
                    # Merge metadata (session state takes precedence for newer info)
                    metadata = {**manager_metadata, **session_metadata}
                    
                    # Display basic stats in columns
                    col1, col2, col3, col4, col5 = st.columns(5)
                    
                    with col1:
                        doc_count = stats.get("document_count", metadata.get("document_count", "N/A"))
                        st.metric("ğŸ“„ ë¬¸ì„œ ìˆ˜", doc_count)
                    
                    with col2:
                        vs_type = getattr(vector_store_manager, 'vector_store_type', 'Unknown')
                        st.metric("ğŸ”§ ë²¡í„° ìŠ¤í† ì–´ íƒ€ì…", vs_type.upper())
                    
                    with col3:
                        collection_name = getattr(vector_store_manager, 'collection_name', 'N/A')
                        st.metric("ğŸ—‚ï¸ ì»¬ë ‰ì…˜", collection_name)
                    
                    with col4:
                        source = st.session_state.get('vector_store_source', 'unknown')
                        source_emoji = {"created": "ğŸ†•", "loaded": "ğŸ“¥", "manual_loaded": "ğŸ”§", "auto_created": "âš™ï¸"}.get(source, "â“")
                        st.metric("ğŸ“ ì¶œì²˜", f"{source_emoji} {source}")
                    
                    with col5:
                        telemetry_status = stats.get("telemetry_status", "ë¹„í™œì„±í™”")
                        st.metric("ğŸš« í…”ë ˆë©”íŠ¸ë¦¬", telemetry_status)
                    
                    # Show vector store sync status
                    current_vs_id = st.session_state.get("vector_store_id")
                    rag_vs_id = st.session_state.get("last_rag_vector_store_id")
                    
                    if current_vs_id and rag_vs_id:
                        if current_vs_id == rag_vs_id:
                            st.success("ğŸ”„ **RAG ì‹œìŠ¤í…œê³¼ ë²¡í„° ìŠ¤í† ì–´ê°€ ë™ê¸°í™”ë¨**")
                        else:
                            st.warning("âš ï¸ **ë²¡í„° ìŠ¤í† ì–´ê°€ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.** ì‹¤í—˜ì„ ë‹¤ì‹œ ì‹œì‘í•˜ë©´ ìƒˆë¡œìš´ ë²¡í„° ìŠ¤í† ì–´ê°€ ì ìš©ë©ë‹ˆë‹¤.")
                    else:
                        st.info("â„¹ï¸ **RAG ì‹œìŠ¤í…œì´ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.** ì‹¤í—˜ì„ ì‹œì‘í•˜ë©´ í˜„ì¬ ë²¡í„° ìŠ¤í† ì–´ë¡œ ì´ˆê¸°í™”ë©ë‹ˆë‹¤.")
                    
                    # Detailed information table
                    st.write("### ğŸ“‹ ìƒì„¸ ì •ë³´")
                    
                    # Check if we have stored metadata from vector store creation/loading
                    vector_store_info = []
                    
                    # Try to get metadata from different sources
                    if metadata:
                        # From stored metadata
                        vector_store_info.extend([
                            ("ğŸ“… ìƒì„± ì‹œê°„", metadata.get('created_at', 'N/A')),
                            ("ğŸ·ï¸ ì €ì¥ ì´ë¦„", metadata.get('store_name', 'N/A')),
                            ("ğŸ“Š ì´ ë¬¸ì ìˆ˜", f"{metadata.get('total_characters', 0):,}"),
                            ("ğŸ“š ì†ŒìŠ¤ íŒŒì¼ ìˆ˜", metadata.get('source_count', 'N/A')),
                            ("ğŸ“ í‰ê·  ì²­í¬ í¬ê¸°", f"{metadata.get('avg_chunk_size', 0):.0f}"),
                            ("ğŸ¤– ì„ë² ë”© ëª¨ë¸", metadata.get('embedding_model', 'N/A')),
                            ("ğŸ”ª ì²­í¬ í¬ê¸°", metadata.get('chunk_size', 'N/A')),
                            ("ğŸ”— ì²­í¬ ì˜¤ë²„ë©", metadata.get('chunk_overlap', 'N/A'))
                        ])
                    else:
                        # Basic information when no metadata available
                        from ..config import EMBEDDING_MODEL
                        
                        vector_store_info.extend([
                            ("ğŸ¤– ì„ë² ë”© ëª¨ë¸", EMBEDDING_MODEL),
                            ("ğŸ”§ ë²¡í„° ìŠ¤í† ì–´ ìƒíƒœ", stats.get("status", "í™œì„±")),
                            ("ğŸ”ª í˜„ì¬ ì²­í¬ í¬ê¸°", st.session_state.get("chunk_size", "N/A")),
                            ("ğŸ”— í˜„ì¬ ì²­í¬ ì˜¤ë²„ë©", st.session_state.get("chunk_overlap", "N/A")),
                            ("ğŸ” ê¸°ë³¸ ê²€ìƒ‰ ìˆ˜", st.session_state.get("top_k", "N/A"))
                        ])
                    
                    # Display information in a clean format
                    for i in range(0, len(vector_store_info), 2):
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            if i < len(vector_store_info):
                                key, value = vector_store_info[i]
                                st.write(f"**{key}:** {value}")
                        
                        with col2:
                            if i + 1 < len(vector_store_info):
                                key, value = vector_store_info[i + 1]
                                st.write(f"**{key}:** {value}")
                    
                    # Sample document information
                    st.write("### ğŸ” ìƒ˜í”Œ ë¬¸ì„œ ì •ë³´")
                    sample_docs = vector_store.similarity_search("test", k=3)
                    if sample_docs:
                        st.success(f"âœ… ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì„±ê³µ - {len(sample_docs)}ê°œ ë¬¸ì„œ ë°œê²¬")
                        
                        # Show sample documents in a table
                        sample_data = []
                        for i, doc in enumerate(sample_docs):
                            sample_data.append({
                                "ìˆœë²ˆ": i + 1,
                                "ì¶œì²˜": doc.metadata.get('source', 'Unknown'),
                                "í˜ì´ì§€": doc.metadata.get('page_number', 'N/A'),
                                "ë¬¸ì ìˆ˜": len(doc.page_content),
                                "ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°": doc.page_content[:100] + "..." if len(doc.page_content) > 100 else doc.page_content
                            })
                        
                        import pandas as pd
                        df_samples = pd.DataFrame(sample_data)
                        st.dataframe(df_samples, use_container_width=True)
                        
                        # Show unique sources
                        unique_sources = set(doc.metadata.get('source', 'Unknown') for doc in sample_docs)
                        if len(unique_sources) > 1:
                            st.info(f"ğŸ“š **ë°œê²¬ëœ ì†ŒìŠ¤ íŒŒì¼:** {', '.join(sorted(unique_sources))}")
                        else:
                            st.info(f"ğŸ“š **ì£¼ìš” ì†ŒìŠ¤ íŒŒì¼:** {list(unique_sources)[0]}")
                    else:
                        st.warning("âš ï¸ ë²¡í„° ìŠ¤í† ì–´ê°€ ë¹„ì–´ìˆê±°ë‚˜ ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        
                else:
                    st.error("âŒ ë²¡í„° ìŠ¤í† ì–´ ë§¤ë‹ˆì €ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    
            except Exception as e:
                st.error(f"âŒ ë²¡í„° ìŠ¤í† ì–´ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {str(e)}")
                
                # Show basic fallback information
                st.write("### âš ï¸ ê¸°ë³¸ ì •ë³´")
                st.write(f"**ğŸ¤– ì„ë² ë”© ëª¨ë¸:** {st.session_state.get('selected_embedding_model', 'sentence-transformers/all-MiniLM-L6-v2')}")
                st.write(f"**ğŸ”§ ë²¡í„° ìŠ¤í† ì–´ íƒ€ì…:** {st.session_state.get('vector_store_type', 'faiss').upper()}")
                st.write(f"**ğŸ”ª ì²­í¬ í¬ê¸°:** {st.session_state.get('chunk_size', 'N/A')}")
                st.write(f"**ğŸ”— ì²­í¬ ì˜¤ë²„ë©:** {st.session_state.get('chunk_overlap', 'N/A')}")
    
    @staticmethod
    def _initialize_rag_systems():
        """Initialize RAG systems if not already done."""
        # Check if vector store has changed
        current_vector_store_id = st.session_state.get("vector_store_id")
        last_rag_vector_store_id = st.session_state.get("last_rag_vector_store_id")
        
        # Get existing RAG systems safely
        existing_rag_systems = st.session_state.get("rag_systems", {})
        
        # Reset RAG systems if vector store changed or if systems don't exist
        if (not existing_rag_systems or 
            current_vector_store_id != last_rag_vector_store_id):
            
            # Clear existing systems if vector store changed
            if (current_vector_store_id != last_rag_vector_store_id and 
                existing_rag_systems):
                st.info("ğŸ”„ **ë²¡í„° ìŠ¤í† ì–´ê°€ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.** RAG ì‹œìŠ¤í…œì„ ì¬ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
                # Clear previous results safely
                if "experiment_results" not in st.session_state:
                    st.session_state.experiment_results = []
                else:
                    st.session_state.experiment_results = []
            
            selected_model = st.session_state.get("selected_llm_model", DEFAULT_LLM_MODEL)
            llm_temperature = st.session_state.get("llm_temperature", 0.1)
            llm_manager = LLMManager(selected_model, OLLAMA_BASE_URL, temperature=llm_temperature)
            
            vector_store_manager = st.session_state.vector_store_manager
            
            st.session_state.rag_systems = {
                "Naive RAG": get_naive_rag_system_info(),
                "Advanced RAG": get_advanced_rag_system_info(),
                "Modular RAG": get_modular_rag_system_info()
            }
            
            # Track the vector store ID used for RAG systems
            st.session_state.last_rag_vector_store_id = current_vector_store_id
    
    @staticmethod
    def _display_experiment_interface(is_ready: bool):
        """Display the main experiment interface."""
        # System selection
        st.subheader("ğŸ¯ ì‹¤í—˜ ì„¤ì •")

        # Disable inputs if the system is not ready
        with st.container(border=True):
            if not is_ready:
                st.info("ì‹¤í—˜ì„ ì§„í–‰í•˜ë ¤ë©´ ë¨¼ì € ìœ„ì˜ ì•ˆë‚´ì— ë”°ë¼ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì¤€ë¹„í•´ì£¼ì„¸ìš”.")

            rag_systems = st.session_state.get("rag_systems", {
                "Naive RAG": get_naive_rag_system_info(),
                "Advanced RAG": get_advanced_rag_system_info(),
                "Modular RAG": get_modular_rag_system_info()
            })

            col1, col2 = st.columns(2)
            with col1:
                selected_systems = st.multiselect(
                    "í…ŒìŠ¤íŠ¸í•  RAG ì‹œìŠ¤í…œ ì„ íƒ:",
                    list(rag_systems.keys()),
                    default=list(rag_systems.keys()),
                    disabled=not is_ready
                )
            
            with col2:
                retrieval_k = st.slider("ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜ (k):", 1, 15, st.session_state.get("top_k", 5), disabled=not is_ready)
            
            # BM25 Indexing for Modular RAG
            if "Modular RAG" in selected_systems:
                st.subheader("ğŸ”‘ Modular RAG: BM25 ì¸ë±ì‹±")
                if is_ready:
                    RAGExperimentUI._manage_bm25_indexing()
                else:
                    st.info("BM25 ì¸ë±ì‹±ì€ ë²¡í„° ìŠ¤í† ì–´ê°€ ì¤€ë¹„ëœ í›„ì— ê°€ëŠ¥í•©ë‹ˆë‹¤.")

            # Display sample queries
            RAGExperimentUI._display_sample_queries()
            
            # Query input
            query = RAGExperimentUI._display_query_input()
            
            # Run experiment
            if st.button("ğŸš€ ì‹¤í—˜ ì‹¤í–‰", type="primary", use_container_width=True):
                if not is_ready:
                    st.error("âŒ ë²¡í„° ìŠ¤í† ì–´ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‹¤í—˜ì„ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                elif not query:
                    st.warning("âš ï¸ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                elif not selected_systems:
                    st.warning("âš ï¸ í•˜ë‚˜ ì´ìƒì˜ RAG ì‹œìŠ¤í…œì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
                else:
                    RAGExperimentUI._run_experiment(query, selected_systems, retrieval_k)

    @staticmethod
    def _display_sample_queries():
        """Display sample queries categorized by type."""
        st.write("**ìƒ˜í”Œ ì§ˆë¬¸ (ì§ˆë¬¸ ìœ í˜•ë³„):**")
        sample_queries = [
            "2025ë…„ AI íŠ¸ë Œë“œëŠ” ë¬´ì—‡ì¸ê°€ìš”?",                    # factual
            "ì§ì¥ì—ì„œ AIë¥¼ ì–´ë–»ê²Œ í™œìš©í•  ìˆ˜ ìˆë‚˜ìš”?",            # procedural
            "ì™œ AIê°€ ì—…ë¬´ ìƒì‚°ì„±ì— ì¤‘ìš”í•œê°€ìš”?",                # causal
            "AI ê¸°ìˆ ì€ ì–¸ì œë¶€í„° ë°œì „í•˜ê¸° ì‹œì‘í–ˆë‚˜ìš”?",          # temporal
            "ìƒì„±í˜• AIì™€ ê¸°ì¡´ AIì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",        # comparative
            "AI ì‹œì¥ ê·œëª¨ëŠ” ì–¼ë§ˆë‚˜ ë˜ë‚˜ìš”?",                   # quantitative
            "ì¸ê³µì§€ëŠ¥ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”.",                     # general
            "ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì€?",                # procedural
            "AI ê°œë°œì—ëŠ” ì–´ë–¤ ë¹„ìš©ì´ ë“œë‚˜ìš”?",                 # quantitative
            "ë”¥ëŸ¬ë‹ì´ ì£¼ëª©ë°›ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"              # causal
        ]
        
        # Display categorized sample questions
        st.write("**ğŸ·ï¸ ì§ˆë¬¸ ìœ í˜• ì˜ˆì‹œ:**")
        st.markdown("""
        - **ì‚¬ì‹¤í˜•(factual)**: "ë¬´ì—‡", "ì–´ë–¤" â†’ ì •í™•í•œ ì •ë³´ ìœ„ì£¼
        - **ë°©ë²•í˜•(procedural)**: "ì–´ë–»ê²Œ", "ë°©ë²•" â†’ ë‹¨ê³„ë³„ ì„¤ëª…  
        - **ì›ì¸í˜•(causal)**: "ì™œ", "ì´ìœ " â†’ ë…¼ë¦¬ì  ë¶„ì„
        - **ì‹œê°„í˜•(temporal)**: "ì–¸ì œ", "ì‹œì " â†’ ì‹œê°„ìˆœ ì •ë¦¬
        - **ë¹„êµí˜•(comparative)**: "ì°¨ì´", "ë¹„êµ" â†’ ë¹„êµ ë¶„ì„
        - **ìˆ˜ì¹˜í˜•(quantitative)**: "ì–¼ë§ˆ", "ê·œëª¨" â†’ ë°ì´í„° ê¸°ë°˜
        - **ì¼ë°˜í˜•(general)**: ê¸°íƒ€ â†’ ì¢…í•©ì  ì„¤ëª…
        """)
        
        st.write("**ğŸ“ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ëª©ë¡:**")
        
        # Sample query types for reference
        query_types = ["factual", "procedural", "causal", "temporal", "comparative", "quantitative", "general", "procedural", "quantitative", "causal"]
        
        cols = st.columns(2)
        for i, sample_query in enumerate(sample_queries):
            col = cols[i % 2]
            query_type = query_types[i] if i < len(query_types) else "general"
            type_emoji = {
                "factual": "ğŸ¯", "procedural": "ğŸ“‹", "causal": "ğŸ¤”", 
                "temporal": "â°", "comparative": "âš–ï¸", "quantitative": "ğŸ“Š", "general": "ğŸ“–"
            }
            emoji = type_emoji.get(query_type, "ğŸ“")
            
            if col.button(f"{emoji} {sample_query}", key=f"sample_{i}", help=f"ì§ˆë¬¸ ìœ í˜•: {query_type}"):
                st.session_state.text_area_value = sample_query
                st.rerun()
    
    @staticmethod
    def _display_query_input():
        """Display query input area."""
        # Query input - ìƒ˜í”Œ ì§ˆë¬¸ ì„ íƒ ì‹œ í•´ë‹¹ ì§ˆë¬¸ì´ ì…ë ¥ì°½ì— í‘œì‹œë¨
        query = st.text_area(
            "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:",
            value=st.session_state.get("text_area_value", ""),
            placeholder="ì˜ˆ: 2025ë…„ AI íŠ¸ë Œë“œëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
            height=100,
            key="query_input"
        )
        return query
    
    @staticmethod
    def _run_experiment(query, selected_systems, retrieval_k):
        """Run the RAG experiment with selected systems."""
        # This check is now the primary guard before running.
        if "vector_store_manager" not in st.session_state or not st.session_state.vector_store_manager.get_vector_store():
            st.error("âŒ ë²¡í„° ìŠ¤í† ì–´ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±í•˜ê±°ë‚˜ ë¡œë“œí•´ì£¼ì„¸ìš”.")
            return

        results = []
        
        # Get RAG systems safely
        rag_systems = st.session_state.get("rag_systems", {})
        
        if not rag_systems:
            st.error("âŒ RAG ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        for system_name in selected_systems:
            st.write(f"## {system_name} ì‹¤í–‰ ì¤‘...")
            
            if system_name not in rag_systems:
                st.error(f"âŒ {system_name} ì‹œìŠ¤í…œì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                continue
                
            rag_system = rag_systems[system_name]
            
            try:
                with st.spinner(f"{system_name} ì‹¤í–‰ ì¤‘... ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤."):
                    if system_name == "Naive RAG":
                        selected_model = st.session_state.get("selected_llm_model", DEFAULT_LLM_MODEL)
                        llm_temperature = st.session_state.get("llm_temperature", 0.1)
                        llm_manager = LLMManager(selected_model, OLLAMA_BASE_URL, temperature=llm_temperature)
                        vector_store_manager = st.session_state.vector_store_manager
                        
                        result = RAGExperimentUI._run_naive_rag(query, retrieval_k, llm_manager, vector_store_manager)

                    elif system_name == "Advanced RAG":
                        selected_model = st.session_state.get("selected_llm_model", DEFAULT_LLM_MODEL)
                        llm_temperature = st.session_state.get("llm_temperature", 0.1)
                        llm_manager = LLMManager(selected_model, OLLAMA_BASE_URL, temperature=llm_temperature)
                        vector_store_manager = st.session_state.vector_store_manager

                        result = RAGExperimentUI._run_advanced_rag(query, retrieval_k * 2, RERANK_TOP_K, llm_manager, vector_store_manager)
                        
                    elif system_name == "Modular RAG":
                        if "bm25_index" not in st.session_state or "bm25_documents" not in st.session_state:
                            st.error("âŒ Modular RAGë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ ë¨¼ì € BM25 ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.")
                            continue

                        selected_model = st.session_state.get("selected_llm_model", DEFAULT_LLM_MODEL)
                        llm_temperature = st.session_state.get("llm_temperature", 0.1)
                        llm_manager = LLMManager(selected_model, OLLAMA_BASE_URL, temperature=llm_temperature)
                        vector_store_manager = st.session_state.vector_store_manager
                        bm25_index = st.session_state.bm25_index
                        bm25_documents = st.session_state.bm25_documents

                        result = RAGExperimentUI._run_modular_rag(
                            query, MAX_ITERATIONS, llm_manager, vector_store_manager, bm25_index, bm25_documents
                        )
                    else:
                        # Fallback for any other system that might still use the old class structure
                        result = rag_system.query(query, k=retrieval_k)
                
                results.append(result)
                
                # Display individual result
                st.write(f"**ë‹µë³€:**")
                st.info(result.get('answer', 'ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.'))
                st.write(f"**ì²˜ë¦¬ ì‹œê°„:** {result.get('total_time', 0):.2f}ì´ˆ")
                
                retrieved_docs = result.get("retrieved_docs", [])
                if retrieved_docs:
                    with st.expander(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ({len(retrieved_docs)}ê°œ)"):
                        for i, doc in enumerate(retrieved_docs):
                            st.write(f"**ë¬¸ì„œ {i+1} (ì¶œì²˜: {doc.metadata.get('source', 'Unknown')})**")
                            st.text(doc.page_content[:200] + "...")
                            st.divider()
                
            except Exception as e:
                st.error(f"{system_name} ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
                import traceback
                st.error(f"ìƒì„¸ ì˜¤ë¥˜ ì •ë³´: {traceback.format_exc()}")
                continue
            
            st.divider()
        
        # Store results
        if results:
            st.session_state.experiment_results = results
            st.success("âœ… ëª¨ë“  ì‹¤í—˜ ì™„ë£Œ!")
    
    @staticmethod
    def _manage_bm25_indexing():
        """UI for managing the BM25 index required for Modular RAG."""
        bm25_index = st.session_state.get("bm25_index")
        bm25_docs_count = len(st.session_state.get("bm25_documents", []))

        if bm25_index:
            st.success(f"âœ… BM25 ì¸ë±ìŠ¤ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. ({bm25_docs_count}ê°œ ë¬¸ì„œ ì¸ë±ì‹±ë¨)")
            if st.button("ğŸ”„ BM25 ì¸ë±ìŠ¤ ì¬ìƒì„±", key="experiment_regenerate_bm25"):
                st.session_state.pop("bm25_index", None)
                st.session_state.pop("bm25_documents", None)
                st.rerun()
        else:
            st.warning("BM25 í‚¤ì›Œë“œ ê²€ìƒ‰ì„ ìœ„í•´ ì¸ë±ìŠ¤ ìƒì„±ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            if st.button("ğŸš€ BM25 ì¸ë±ìŠ¤ ìƒì„±", key="experiment_create_bm25"):
                try:
                    with st.spinner("ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ì—¬ BM25 ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤..."):
                        vector_store_manager = st.session_state.get("vector_store_manager")
                        if not vector_store_manager:
                            st.error("ë²¡í„° ìŠ¤í† ì–´ ë§¤ë‹ˆì €ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                            return

                        vector_store = vector_store_manager.get_vector_store()
                        
                        stats = vector_store_manager.get_collection_stats()
                        total_docs = stats.get("document_count", 1000)
                        
                        if total_docs == 0:
                            st.error("ì¸ë±ì‹±í•  ë¬¸ì„œê°€ ë²¡í„° ìŠ¤í† ì–´ì— ì—†ìŠµë‹ˆë‹¤.")
                            return

                        docs = vector_store.similarity_search("", k=total_docs)
                        
                        if not docs:
                            st.error("ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ë¬¸ì„œë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                            return
                        
                        corpus = [doc.page_content for doc in docs]
                        st.session_state.bm25_index = BM25(corpus)
                        st.session_state.bm25_documents = docs
                    st.rerun()
                except Exception as e:
                    st.error(f"BM25 ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
                    if "bm25_index" in st.session_state:
                        del st.session_state.bm25_index
                    if "bm25_documents" in st.session_state:
                        del st.session_state.bm25_documents
    
    @staticmethod
    def _run_modular_rag(query: str, max_iterations: int, llm_manager: LLMManager, vector_store_manager: VectorStoreManager, bm25_index: BM25, bm25_docs: List[Any]) -> Dict[str, Any]:
        """Run the Modular RAG using the compiled graph and display progress via streaming."""
        start_time = time.time()
        
        modular_rag_graph = create_modular_rag_graph(llm_manager, vector_store_manager, bm25_index, bm25_docs)
        inputs = {"query": query, "max_iterations": max_iterations}
        
        st.subheader("ğŸ§© ëª¨ë“ˆí˜• RAG ì²˜ë¦¬ ê³¼ì •")
        
        # Placeholders for real-time updates
        preprocess_placeholder = st.empty()
        iteration_placeholder = st.empty()
        final_summary_placeholder = st.empty()
        
        final_state = {}
        with st.spinner("Modular RAG ê·¸ë˜í”„ ì‹¤í–‰ ì¤‘..."):
            for state in modular_rag_graph.stream(inputs, config={"callbacks": [langfuse_handler]}):
                node_name, node_output = list(state.items())[0]
                final_state.update(node_output)

                with preprocess_placeholder.container(border=True):
                    st.write("**1ë‹¨ê³„: ì‚¬ì „ ê²€ìƒ‰ ì²˜ë¦¬**")
                    st.info(f"ì¿¼ë¦¬ í™•ì¥: {final_state.get('expanded_query', '...')}")
                    st.info(f"ì¿¼ë¦¬ ìœ í˜•: {final_state.get('query_type', '...')} (ì‹ ë¢°ë„: {final_state.get('classification_confidence', 0):.2f})")

                with iteration_placeholder.container(border=True):
                    st.write(f"**2ë‹¨ê³„: ë°˜ë³µì  ê°œì„  (í˜„ì¬ {final_state.get('iteration', 0) + 1}ë²ˆì§¸ ì‹¤í–‰ ì¤‘)**")
                    if node_name == "retrieve_and_process":
                        st.success(f"ë¬¸ì„œ ê²€ìƒ‰ ë° ì²˜ë¦¬ ì™„ë£Œ: {len(final_state.get('retrieved_docs', []))}ê°œ ë¬¸ì„œ ì„ íƒë¨")
                    if node_name == "generate":
                        st.success("ë‹µë³€ ìƒì„± ì™„ë£Œ")
                        st.info(f"ì¤‘ê°„ ë‹µë³€: {final_state.get('answer', '')[:100]}...")
                        st.warning(f"í˜„ì¬ ì‹ ë¢°ë„: {final_state.get('final_confidence', 0):.2f}")

        # Final display after streaming is complete
        iteration_placeholder.empty()
        preprocess_placeholder.empty()

        st.write("**1ë‹¨ê³„: ì‚¬ì „ ê²€ìƒ‰ ì²˜ë¦¬**")
        st.info(f"ì¿¼ë¦¬ í™•ì¥: {final_state.get('expanded_query')}")
        st.info(f"ì¿¼ë¦¬ ìœ í˜•: {final_state.get('query_type')} (ì‹ ë¢°ë„: {final_state.get('classification_confidence', 0):.2f})")

        st.write(f"**2ë‹¨ê³„: ë°˜ë³µì  ê°œì„  (ì´ {final_state.get('iteration', 0) + 1}íšŒ ì‹¤í–‰)**")

        st.subheader("ğŸ¤– ìµœì¢… ë‹µë³€")
        answer = final_state.get("answer", "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        st.markdown(answer)
        
        st.subheader("ğŸ“ˆ ìµœì¢… ê²°ê³¼ ìš”ì•½")
        total_time = time.time() - start_time
        all_retrieved_docs = final_state.get("all_retrieved_docs", [])
        
        col1, col2, col3 = st.columns(3)
        col1.metric("ì´ ì²˜ë¦¬ ì‹œê°„", f"{total_time:.2f}ì´ˆ")
        col2.metric("ì´ ë°˜ë³µ íšŸìˆ˜", final_state.get('iteration', 0) + 1)
        col3.metric("ìµœì¢… ì‹ ë¢°ë„", f"{final_state.get('final_confidence', 0):.2f}")

        with st.expander(f"ìµœì¢… ê²€ìƒ‰ëœ ë¬¸ì„œ ë³´ê¸° ({len(all_retrieved_docs)}ê°œ)"):
            for i, doc in enumerate(all_retrieved_docs):
                st.write(f"**ë¬¸ì„œ {i+1} (ì¶œì²˜: {doc.metadata.get('source', 'Unknown')})**")
                st.text(doc.page_content[:200] + "...")
                st.divider()

        system_info = get_modular_rag_system_info()
        return {
            "question": query,
            "answer": answer,
            "retrieved_docs": all_retrieved_docs,
            "total_time": total_time,
            "rag_type": system_info["name"],
            "metadata": {
                "iterations": final_state.get("iteration", 0),
                "final_confidence": final_state.get("final_confidence", 0.0),
                "query_type": final_state.get("query_type", "general"),
                "total_retrieved": len(all_retrieved_docs),
                "expansion_terms": final_state.get("expansion_terms", [])
            }
        }

    @staticmethod
    def _run_advanced_rag(query: str, k: int, rerank_top_k: int, llm_manager: LLMManager, vector_store_manager: VectorStoreManager) -> Dict[str, Any]:
        """Run the Advanced RAG using the compiled graph and display progress."""
        start_time = time.time()
        
        advanced_rag_graph = create_advanced_rag_graph(llm_manager, vector_store_manager)
        
        inputs = {"query": query, "k": k, "rerank_top_k": rerank_top_k}
        
        final_state = {}
        with st.spinner("Advanced RAG ê·¸ë˜í”„ ì‹¤í–‰ ì¤‘..."):
            final_state = advanced_rag_graph.invoke(inputs, config={"callbacks": [langfuse_handler]})

        # 1. ì¿¼ë¦¬ ì „ì²˜ë¦¬ ê²°ê³¼ í‘œì‹œ
        st.subheader("ğŸ”§ 1ë‹¨ê³„: ì¿¼ë¦¬ ì „ì²˜ë¦¬")
        preprocess_details = final_state.get("preprocessing_details", {})
        optimized_query = final_state.get("optimized_query")

        if optimized_query != query:
            st.success(f"ì¿¼ë¦¬ ìµœì í™” ì™„ë£Œ")
            with st.expander("ì¿¼ë¦¬ í™•ì¥ ìƒì„¸ ì •ë³´ ë³´ê¸°"):
                st.write(f"- **ì›ë³¸ ì¿¼ë¦¬:** `{preprocess_details.get('original_query')}`")
                st.write(f"- **ì„ íƒëœ í™•ì¥ ìš©ì–´:** `{preprocess_details.get('selected_terms')}`")
                st.write(f"- **ìµœì¢… í™•ì¥ ì¿¼ë¦¬:** `{optimized_query}`")
        else:
            st.info("ì¿¼ë¦¬ ìµœì í™”: ë³€ê²½ì‚¬í•­ ì—†ìŒ")
        
        # 2. ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼
        st.subheader("ğŸ” 2ë‹¨ê³„: ë¬¸ì„œ ê²€ìƒ‰")
        docs_with_scores = final_state.get("docs_with_scores", [])
        st.success(f"ì´ˆê¸° ê²€ìƒ‰: {len(docs_with_scores)}ê°œ ë¬¸ì„œ")

        # 3. ë¬¸ì„œ ì¬ìˆœìœ„í™” ê²°ê³¼
        st.subheader("ğŸ“Š 3ë‹¨ê³„: ë¬¸ì„œ ì¬ìˆœìœ„í™”")
        reranked_docs = final_state.get("reranked_docs", [])
        st.success(f"ì¬ìˆœìœ„í™” ì™„ë£Œ: ìƒìœ„ {len(reranked_docs)}ê°œ ë¬¸ì„œ ì„ íƒ")

        with st.expander(f"ì¬ìˆœìœ„í™”ëœ ë¬¸ì„œ ({len(reranked_docs)}ê°œ)"):
            for i, doc in enumerate(reranked_docs):
                st.write(f"**ë¬¸ì„œ {i+1} (ì¶œì²˜: {doc.metadata.get('source', 'Unknown')})**")
                st.text(doc.page_content[:200] + "...")
                st.divider()

        # 4. ì»¨í…ìŠ¤íŠ¸ ì••ì¶• ê²°ê³¼
        st.subheader("ğŸ—œï¸ 4ë‹¨ê³„: ì»¨í…ìŠ¤íŠ¸ ì••ì¶•")
        compression_ratio = final_state.get("compression_ratio", 0)
        st.info(f"ì••ì¶•ë¥ : {compression_ratio:.2%}")
        
        # 5. ìµœì¢… ë‹µë³€
        st.subheader("ğŸ¤– 5ë‹¨ê³„: ë‹µë³€ ìƒì„±")
        answer = final_state.get("answer", "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        
        total_time = time.time() - start_time

        system_info = get_advanced_rag_system_info()
        return {
            "question": query,
            "answer": answer,
            "retrieved_docs": reranked_docs,
            "total_time": total_time,
            "rag_type": system_info["name"],
            "metadata": {
                "optimized_query": optimized_query,
                "initial_retrieved": len(docs_with_scores),
                "final_retrieved": len(reranked_docs),
                "compression_ratio": compression_ratio,
                "retrieval_method": "similarity_search + reranking",
                "generation_method": "reasoning-based"
            }
        }

    @staticmethod
    def _run_naive_rag(query: str, k: int, llm_manager: LLMManager, vector_store_manager: VectorStoreManager) -> Dict[str, Any]:
        """Run the Naive RAG using the compiled graph and return results."""
        start_time = time.time()
        
        naive_rag_graph = create_naive_rag_graph(llm_manager, vector_store_manager)
        
        inputs = {"query": query, "k": k}
        final_state = naive_rag_graph.invoke(inputs, config={"callbacks": [langfuse_handler]})

        end_time = time.time()
        total_time = end_time - start_time

        retrieved_docs = final_state.get("documents", [])
        system_info = get_naive_rag_system_info()
        
        return {
            "question": query,
            "answer": final_state.get("answer"),
            "retrieved_docs": retrieved_docs,
            "total_time": total_time,
            "rag_type": system_info["name"],
            "metadata": {
                "num_retrieved": len(retrieved_docs),
                "retrieval_method": "similarity_search",
                "generation_method": "simple"
            }
        }