"""RAGAS Evaluation UI module for the RAG application."""

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from datasets import Dataset

from ..config import *
from ..config.settings import (
    RAGAS_EVALUATION_METRICS,
    RAGAS_SAMPLE_SIZE,
    RAGAS_TIMEOUT,
    RAGAS_BATCH_SIZE,
    RAGAS_AVAILABLE_MODELS,
    RAGAS_RESULTS_DIR
)
from ..graphs.naive_rag_graph import create_naive_rag_graph
from ..graphs.advanced_rag_graph import create_advanced_rag_graph
from ..graphs.modular_rag_graph import create_modular_rag_graph
from ..rag_systems.modular_rag import BM25
from ..utils.vector_store import VectorStoreManager
from ..utils.llm_manager import LLMManager
from src.config import langfuse_handler

# ëª¨ë¸ ì´ë¦„ ë§¤í•‘ (ì„¤ì •ì˜ ì†Œë¬¸ì -> UI í‘œì‹œìš© ëŒ€ë¬¸ì)
MODEL_NAME_MAPPING = {
    "naive": "Naive RAG",
    "advanced": "Advanced RAG", 
    "modular": "Modular RAG"
}

# ì—­ë°©í–¥ ë§¤í•‘ (UI -> ì„¤ì •)
REVERSE_MODEL_MAPPING = {v: k for k, v in MODEL_NAME_MAPPING.items()}

class RagasEvaluationUI:
    """UI for RAG model evaluation using RAGAS."""

    def __init__(self):
        if "evaluation_results" not in st.session_state:
            st.session_state.evaluation_results = None
        if "evaluation_running" not in st.session_state:
            st.session_state.evaluation_running = False

    def display(self):
        """Display the RAGAS evaluation page."""
        st.title("ğŸ“Š RAGAS ê²°ê³¼ í‰ê°€")
        
        st.info(
            "ì´ í˜ì´ì§€ì—ì„œëŠ” Naive, Advanced, Modular RAG ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ RAGASë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê°€í•©ë‹ˆë‹¤.\n\n"
            "**í‰ê°€ ë°ì´í„°ì…‹ í˜•ì‹:**\n"
            "- `question`: ëª¨ë¸ì— ì§ˆì˜í•  ì§ˆë¬¸ (í•„ìˆ˜)\n"
            "- `ground_truth`: ì´ìƒì ì¸ ì •ë‹µ (í•„ìˆ˜)\n\n"
            "í‰ê°€ ë°ì´í„°ì…‹ì„ ì•„ë˜ í‘œì— ì§ì ‘ ì…ë ¥í•˜ê±°ë‚˜ CSV/JSON íŒŒì¼ë¡œ ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )

        self._display_dataset_input()
        
        if "eval_dataset" in st.session_state and st.session_state.eval_dataset:
            is_ready = self._check_vector_store()
            if is_ready:
                self._manage_bm25_for_modular_rag()
                self._display_model_selection_and_run()
        
        self._display_results()

    def _get_llm_manager(self):
        """Initializes and returns the LLMManager based on session state."""
        return LLMManager(
            model_name=st.session_state.get("selected_llm_model", "llama3.2:latest"),
            base_url=OLLAMA_BASE_URL,
            temperature=st.session_state.get("llm_temperature", 0.1)
        )

    def _check_vector_store(self):
        """Checks if the vector store is ready for evaluation."""
        if "vector_store_manager" not in st.session_state or not st.session_state.vector_store_manager.get_vector_store():
            st.warning("RAGAS í‰ê°€ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ ë¨¼ì € 'ë¬¸ì„œ ë¡œë”©' íƒ­ì—ì„œ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  'ë²¡í„° ìŠ¤í† ì–´' íƒ­ì—ì„œ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.")
            return False
        st.success("âœ… ë²¡í„° ìŠ¤í† ì–´ ì¤€ë¹„ ì™„ë£Œ. í‰ê°€ë¥¼ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return True

    def _manage_bm25_for_modular_rag(self):
        """UI for managing the BM25 index required for Modular RAG."""
        with st.container(border=True):
            st.subheader("ğŸ› ï¸ Modular RAG ì‚¬ì „ ì„¤ì •: BM25 ì¸ë±ìŠ¤")
            bm25_index = st.session_state.get("bm25_index")
            bm25_docs_count = len(st.session_state.get("bm25_documents", []))

            if bm25_index:
                st.success(f"âœ… BM25 ì¸ë±ìŠ¤ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. ({bm25_docs_count}ê°œ ë¬¸ì„œ ì¸ë±ì‹±ë¨)")
                if st.button("ğŸ”„ BM25 ì¸ë±ìŠ¤ ì¬ìƒì„±", key="ragas_regenerate_bm25"):
                    st.session_state.pop("bm25_index", None)
                    st.session_state.pop("bm25_documents", None)
                    st.rerun()
            else:
                st.warning("Modular RAGë¥¼ í‰ê°€í•˜ë ¤ë©´ BM25 í‚¤ì›Œë“œ ê²€ìƒ‰ì„ ìœ„í•œ ì¸ë±ìŠ¤ ìƒì„±ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                if st.button("ğŸš€ BM25 ì¸ë±ìŠ¤ ìƒì„±", key="create_bm25_index_ragas"):
                    self._create_bm25_index()

    def _create_bm25_index(self):
        """Creates and stores the BM25 index in the session state."""
        try:
            with st.spinner("ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ì—¬ BM25 ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤..."):
                vector_store_manager = st.session_state.get("vector_store_manager")
                if not vector_store_manager:
                    st.error("ë²¡í„° ìŠ¤í† ì–´ ë§¤ë‹ˆì €ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return

                vector_store = vector_store_manager.get_vector_store()
                stats = vector_store_manager.get_collection_stats()
                total_docs = stats.get("document_count", 1000)

                if total_docs == 0:
                    st.error("ì¸ë±ì‹±í•  ë¬¸ì„œê°€ ë²¡í„° ìŠ¤í† ì–´ì— ì—†ìŠµë‹ˆë‹¤.")
                    return

                docs = vector_store.similarity_search("", k=total_docs)
                
                if not docs:
                    st.error("ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ë¬¸ì„œë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    return
                
                corpus = [doc.page_content for doc in docs]
                st.session_state.bm25_index = BM25(corpus)
                st.session_state.bm25_documents = docs
            st.rerun()
        except Exception as e:
            st.error(f"BM25 ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            st.session_state.pop("bm25_index", None)
            st.session_state.pop("bm25_documents", None)

    def _display_dataset_input(self):
        """Handles the input for the evaluation dataset."""
        st.subheader("1. í‰ê°€ ë°ì´í„°ì…‹ ì¤€ë¹„")

        # Initialize with sample data if not present
        if "eval_df" not in st.session_state:
            sample_data = {
                "question": ["RAGì˜ í•œê³„ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"],
                "ground_truth": ["RAGëŠ” ê²€ìƒ‰ëœ ë¬¸ì„œì˜ í’ˆì§ˆì— ë”°ë¼ ë‹µë³€ì˜ ì •í™•ì„±ì´ í¬ê²Œ ì¢Œìš°ë˜ë©°, ê´€ë ¨ ì—†ëŠ” ì •ë³´ê°€ ê²€ìƒ‰ë˜ë©´ í™˜ê°(Hallucination) í˜„ìƒì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ì‹¤ì‹œê°„ ì •ë³´ë‚˜ ìµœì‹  ë°ì´í„°ë¥¼ ë°˜ì˜í•˜ëŠ” ë° í•œê³„ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."]
            }
            # ì„¤ì •ëœ ìƒ˜í”Œ í¬ê¸°ë§Œí¼ ë°ì´í„°ë¥¼ ìƒì„± (ê¸°ë³¸ê°’ ì‚¬ìš©)
            st.session_state.eval_df = pd.DataFrame(sample_data)

        # Display editable dataframe
        st.session_state.eval_df = st.data_editor(
            st.session_state.eval_df,
            num_rows="dynamic",
            use_container_width=True
        )

        # File uploader
        uploaded_file = st.file_uploader("CSV ë˜ëŠ” JSON íŒŒì¼ ì—…ë¡œë“œ", type=["csv", "json"])
        if uploaded_file:
            try:
                if uploaded_file.name.endswith('.csv'):
                    df = pd.read_csv(uploaded_file)
                else:
                    df = pd.read_json(uploaded_file)
                
                # Validate columns
                if "question" not in df.columns or "ground_truth" not in df.columns:
                    st.error("ì˜¤ë¥˜: ì—…ë¡œë“œëœ íŒŒì¼ì— 'question'ê³¼ 'ground_truth' ì»¬ëŸ¼ì´ ëª¨ë‘ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")
                else:
                    st.session_state.eval_df = df
                    st.success(f"âœ… '{uploaded_file.name}' íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                st.error(f"íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

        if not st.session_state.eval_df.empty:
            # Convert to Hugging Face Dataset
            dataset = Dataset.from_pandas(st.session_state.eval_df)
            st.session_state.eval_dataset = dataset
            st.write(f"**ë°ì´í„°ì…‹ ì •ë³´:** {len(dataset)}ê°œ ì§ˆë¬¸")

    def _display_model_selection_and_run(self):
        """Allows model selection and initiates the evaluation."""
        st.subheader("2. í‰ê°€ ëŒ€ìƒ RAG ëª¨ë¸ ì„ íƒ")
        
        col1, col2 = st.columns([1, 2])
        
        with col1:
            # ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¨ ëª¨ë¸ë“¤ì„ UIìš© ì´ë¦„ìœ¼ë¡œ ë³€í™˜
            available_model_names = [MODEL_NAME_MAPPING[model] for model in RAGAS_AVAILABLE_MODELS]
            models_to_evaluate = st.multiselect(
                "í‰ê°€í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”.",
                options=available_model_names,
                default=available_model_names
            )
        
        with col2:
            st.write(" ") # For alignment
            if st.button("ğŸš€ í‰ê°€ ì‹œì‘", type="primary", disabled=not models_to_evaluate or st.session_state.evaluation_running):
                self._run_evaluation(models_to_evaluate)

    def _run_evaluation(self, models_to_evaluate):
        """Executes the evaluation process."""
        st.session_state.evaluation_running = True
        st.session_state.evaluation_results = {}

        vector_store_manager = st.session_state.get("vector_store_manager")
        if not vector_store_manager:
            st.error("ì˜¤ë¥˜: ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'ë¬¸ì„œ ë¡œë”©' íƒ­ì—ì„œ ë¬¸ì„œë¥¼ ë¨¼ì € ì²˜ë¦¬í•´ì£¼ì„¸ìš”.")
            st.session_state.evaluation_running = False
            return
        
        llm_manager = self._get_llm_manager()

        # RAGAS can be slow, so let's provide a progress bar for the models
        progress_bar = st.progress(0)
        progress_text = st.empty()

        for i, model_name in enumerate(models_to_evaluate):
            progress_text.text(f"'{model_name}' í‰ê°€ ì§„í–‰ ì¤‘... ({i+1}/{len(models_to_evaluate)})")
            
            with st.spinner(f"â³ **{model_name}** í‰ê°€ ì§„í–‰ ì¤‘... (ì§ˆë¬¸ë“¤ì„ ì²˜ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë‹¤ì†Œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)"):
                try:
                    # Graph setup
                    graph = None
                    if model_name == "Naive RAG":
                        graph = create_naive_rag_graph(llm_manager, vector_store_manager)
                    
                    elif model_name == "Advanced RAG":
                        graph = create_advanced_rag_graph(llm_manager, vector_store_manager)

                    elif model_name == "Modular RAG":
                        if "bm25_index" not in st.session_state:
                            st.warning(f"BM25 ì¸ë±ìŠ¤ê°€ ì—†ì–´ '{model_name}' í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤. 'ì‚¬ì „ ì„¤ì •'ì—ì„œ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
                            continue
                        bm25_index = st.session_state.bm25_index
                        bm25_documents = st.session_state.bm25_documents
                        graph = create_modular_rag_graph(llm_manager, vector_store_manager, bm25_index, bm25_documents)

                    if not graph:
                        st.error(f"'{model_name}'ì— ëŒ€í•œ ê·¸ë˜í”„ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        continue
                        
                    # Run inference and collect results
                    results_list = []
                    for item in st.session_state.eval_dataset:
                        inputs = {"query": item["question"]}
                        # Add specific inputs for each graph type
                        if model_name == "Naive RAG":
                            inputs["k"] = 5
                        elif model_name == "Advanced RAG":
                            inputs.update({"k": 10, "rerank_top_k": 5})
                        elif model_name == "Modular RAG":
                            inputs["max_iterations"] = 2
                        
                        response = graph.invoke(inputs, config={"callbacks": [langfuse_handler]})
                        
                        # Extract answer and context based on graph's output keys
                        answer = response.get("answer", "")
                        if model_name == "Naive RAG":
                            contexts = response.get("documents", [])
                        elif model_name == "Advanced RAG":
                            contexts = response.get("reranked_docs", [])
                        elif model_name == "Modular RAG":
                            contexts = response.get("all_retrieved_docs", [])
                        else:
                            contexts = []
                            
                        results_list.append({
                            "question": item["question"],
                            "answer": answer,
                            "contexts": [doc.page_content for doc in contexts],
                            "ground_truth": item["ground_truth"],
                        })

                    result_dataset = Dataset.from_list(results_list)
                    
                    # Perform RAGAS evaluation
                    from ragas import evaluate
                    from ragas.metrics import faithfulness, answer_relevancy, context_recall, context_precision
                    
                    # ì„¤ì •ì—ì„œ ì§€ì •ëœ ë©”íŠ¸ë¦­ë“¤ì„ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°
                    metrics = []
                    metric_map = {
                        "faithfulness": faithfulness,
                        "answer_relevancy": answer_relevancy,
                        "context_recall": context_recall,
                        "context_precision": context_precision
                    }
                    
                    for metric_name in RAGAS_EVALUATION_METRICS:
                        if metric_name in metric_map:
                            metrics.append(metric_map[metric_name])
                    
                    score = evaluate(
                        result_dataset,
                        metrics=metrics,
                    )
                    
                    st.session_state.evaluation_results[model_name] = score.to_pandas()
                    st.success(f"âœ… **{model_name}** í‰ê°€ ì™„ë£Œ!")
                
                except Exception as e:
                    st.error(f"**{model_name}** í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            
            progress_bar.progress((i + 1) / len(models_to_evaluate))

        progress_text.text("ëª¨ë“  í‰ê°€ ì™„ë£Œ!")
        st.session_state.evaluation_running = False
        
        # í‰ê°€ ê²°ê³¼ ì €ì¥ (ì„ íƒì‚¬í•­)
        if st.session_state.evaluation_results:
            self._save_evaluation_results()

    def _save_evaluation_results(self):
        """í‰ê°€ ê²°ê³¼ë¥¼ ì„¤ì •ëœ ê²½ë¡œì— ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            import json
            import os
            from datetime import datetime
            
            # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
            RAGAS_RESULTS_DIR.mkdir(parents=True, exist_ok=True)
            
            # ê²°ê³¼ ë°ì´í„° ì¤€ë¹„
            results_data = {
                "timestamp": datetime.now().isoformat(),
                "evaluation_metrics": RAGAS_EVALUATION_METRICS,
                "results": {}
            }
            
            for model_name, result_df in st.session_state.evaluation_results.items():
                results_data["results"][model_name] = {
                    "detailed_scores": result_df.to_dict(),
                    "average_scores": result_df.mean(numeric_only=True).to_dict()
                }
            
            # JSON íŒŒì¼ë¡œ ì €ì¥
            with open(RAGAS_RESULTS_DIR / f"evaluation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json", 'w', encoding='utf-8') as f:
                json.dump(results_data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            st.warning(f"í‰ê°€ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def _display_results(self):
        """Displays the evaluation results with advanced Plotly visualizations."""
        if st.session_state.evaluation_results:
            st.subheader("3. í‰ê°€ ê²°ê³¼")
            
            # ê°œë³„ ëª¨ë¸ ê²°ê³¼ í‘œì‹œ
            for model_name, result_df in st.session_state.evaluation_results.items():
                with st.expander(f"#### {model_name} ìƒì„¸ ê²°ê³¼", expanded=False):
                    st.dataframe(result_df, use_container_width=True)
                    
                    # Display average scores
                    avg_scores = result_df.mean(numeric_only=True)
                    st.write("##### ğŸ“ˆ í‰ê·  ì ìˆ˜")
                    st.dataframe(avg_scores.to_frame("í‰ê·  ì ìˆ˜").T, use_container_width=True)
            
            # ì¢…í•© ë¹„êµ ì‹œê°í™”
            if len(st.session_state.evaluation_results) > 1:
                self._create_comprehensive_visualizations()
            else:
                # ë‹¨ì¼ ëª¨ë¸ ê²°ê³¼ ì‹œê°í™”
                self._create_single_model_visualization()

    def _create_comprehensive_visualizations(self):
        """ë‹¤ì¤‘ ëª¨ë¸ ë¹„êµë¥¼ ìœ„í•œ ì¢…í•© ì‹œê°í™”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        st.subheader("ğŸ“Š ëª¨ë¸ë³„ ì¢…í•© ë¹„êµ")
        
        # ë°ì´í„° ì¤€ë¹„
        summary_data = self._prepare_summary_data()
        summary_df = pd.DataFrame(summary_data)
        
        # ì‹œê°í™” ì˜µì…˜ ì„ íƒ
        viz_col1, viz_col2 = st.columns(2)
        
        with viz_col1:
            show_radar = st.checkbox("ğŸ¯ ë ˆì´ë” ì°¨íŠ¸", value=True)
            show_bar = st.checkbox("ğŸ“Š ë°” ì°¨íŠ¸", value=True)
        
        with viz_col2:
            show_heatmap = st.checkbox("ğŸ”¥ íˆíŠ¸ë§µ", value=True)
            show_line = st.checkbox("ğŸ“ˆ ë¼ì¸ ì°¨íŠ¸", value=False)
        
        # ìš”ì•½ í…Œì´ë¸”
        st.write("##### ğŸ“‹ ì¢…í•© ì ìˆ˜ í…Œì´ë¸”")
        st.dataframe(summary_df.set_index("Model"), use_container_width=True)
        
        # ì‹œê°í™” ìƒì„±
        if show_radar:
            self._create_radar_chart(summary_df)
        
        if show_bar:
            self._create_bar_chart(summary_df)
        
        if show_heatmap:
            self._create_heatmap(summary_df)
        
        if show_line:
            self._create_line_chart(summary_df)
        
        # ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸
        self._display_performance_insights(summary_df)

    def _create_single_model_visualization(self):
        """ë‹¨ì¼ ëª¨ë¸ ê²°ê³¼ì— ëŒ€í•œ ì‹œê°í™”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        model_name, result_df = list(st.session_state.evaluation_results.items())[0]
        
        st.subheader(f"ğŸ“Š {model_name} ìƒì„¸ ë¶„ì„")
        
        # ë©”íŠ¸ë¦­ë³„ ë¶„í¬ ì°¨íŠ¸
        metrics = [col for col in result_df.columns if col in ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision']]
        
        if metrics:
            fig = make_subplots(
                rows=2, cols=2,
                subplot_titles=metrics,
                specs=[[{"type": "histogram"}, {"type": "histogram"}],
                       [{"type": "histogram"}, {"type": "histogram"}]]
            )
            
            positions = [(1,1), (1,2), (2,1), (2,2)]
            colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
            
            for i, metric in enumerate(metrics[:4]):
                row, col = positions[i]
                fig.add_trace(
                    go.Histogram(
                        x=result_df[metric],
                        name=metric,
                        marker_color=colors[i],
                        opacity=0.7,
                        nbinsx=10
                    ),
                    row=row, col=col
                )
            
            fig.update_layout(
                title=f"{model_name} ë©”íŠ¸ë¦­ë³„ ì ìˆ˜ ë¶„í¬",
                height=600,
                showlegend=False
            )
            
            st.plotly_chart(fig, use_container_width=True)

    def _prepare_summary_data(self):
        """ìš”ì•½ ë°ì´í„°ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤."""
        summary_data = {
            "Model": [],
            "Faithfulness": [],
            "Answer Relevancy": [],
            "Context Recall": [],
            "Context Precision": [],
            "Overall Score": []
        }
        
        for model_name, result_df in st.session_state.evaluation_results.items():
            avg_scores = result_df.mean(numeric_only=True)
            summary_data["Model"].append(model_name)
            summary_data["Faithfulness"].append(avg_scores.get("faithfulness", 0))
            summary_data["Answer Relevancy"].append(avg_scores.get("answer_relevancy", 0))
            summary_data["Context Recall"].append(avg_scores.get("context_recall", 0))
            summary_data["Context Precision"].append(avg_scores.get("context_precision", 0))
            
            # ì „ì²´ ì ìˆ˜ ê³„ì‚° (í‰ê· )
            overall_score = avg_scores.mean()
            summary_data["Overall Score"].append(overall_score)
        
        return summary_data

    def _create_radar_chart(self, summary_df):
        """ë ˆì´ë” ì°¨íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        st.write("##### ğŸ¯ ë ˆì´ë” ì°¨íŠ¸ - ì¢…í•© ì„±ëŠ¥ ë¹„êµ")
        
        fig = go.Figure()
        
        metrics = ["Faithfulness", "Answer Relevancy", "Context Recall", "Context Precision"]
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FECA57']
        
        for i, model in enumerate(summary_df["Model"]):
            values = [summary_df.loc[i, metric] for metric in metrics]
            values.append(values[0])  # ì°¨íŠ¸ë¥¼ ë‹«ê¸° ìœ„í•´ ì²« ë²ˆì§¸ ê°’ ì¶”ê°€
            
            fig.add_trace(go.Scatterpolar(
                r=values,
                theta=metrics + [metrics[0]],
                fill='toself',
                name=model,
                line_color=colors[i % len(colors)],
                fillcolor=colors[i % len(colors)],
                opacity=0.3
            ))
        
        fig.update_layout(
            polar=dict(
                radialaxis=dict(
                    visible=True,
                    range=[0, 1]
                )
            ),
            showlegend=True,
            title="RAG ëª¨ë¸ ì„±ëŠ¥ ë ˆì´ë” ì°¨íŠ¸",
            height=500
        )
        
        st.plotly_chart(fig, use_container_width=True)

    def _create_bar_chart(self, summary_df):
        """ì¸í„°ë™í‹°ë¸Œ ë°” ì°¨íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        st.write("##### ğŸ“Š ë°” ì°¨íŠ¸ - ë©”íŠ¸ë¦­ë³„ ì„±ëŠ¥ ë¹„êµ")
        
        # ë©”íŠ¸ë¦­ ì„ íƒ
        metrics = ["Faithfulness", "Answer Relevancy", "Context Recall", "Context Precision", "Overall Score"]
        selected_metrics = st.multiselect(
            "í‘œì‹œí•  ë©”íŠ¸ë¦­ì„ ì„ íƒí•˜ì„¸ìš”:",
            metrics,
            default=metrics[:4],
            key="bar_chart_metrics"
        )
        
        if selected_metrics:
            # ë°ì´í„° ì¬êµ¬ì„±
            melted_df = summary_df.melt(
                id_vars=["Model"],
                value_vars=selected_metrics,
                var_name="Metric",
                value_name="Score"
            )
            
            fig = px.bar(
                melted_df,
                x="Model",
                y="Score",
                color="Metric",
                barmode="group",
                title="ë©”íŠ¸ë¦­ë³„ ì„±ëŠ¥ ë¹„êµ",
                color_discrete_sequence=px.colors.qualitative.Set3
            )
            
            fig.update_layout(
                height=500,
                xaxis_title="RAG ëª¨ë¸",
                yaxis_title="ì ìˆ˜",
                yaxis=dict(range=[0, 1])
            )
            
            st.plotly_chart(fig, use_container_width=True)

    def _create_heatmap(self, summary_df):
        """íˆíŠ¸ë§µì„ ìƒì„±í•©ë‹ˆë‹¤."""
        st.write("##### ğŸ”¥ íˆíŠ¸ë§µ - ëª¨ë¸ Ã— ë©”íŠ¸ë¦­ ì„±ëŠ¥ ë§¤íŠ¸ë¦­ìŠ¤")
        
        metrics = ["Faithfulness", "Answer Relevancy", "Context Recall", "Context Precision"]
        heatmap_data = summary_df[["Model"] + metrics].set_index("Model")
        
        fig = px.imshow(
            heatmap_data.T,
            labels=dict(x="RAG ëª¨ë¸", y="í‰ê°€ ë©”íŠ¸ë¦­", color="ì ìˆ˜"),
            x=heatmap_data.index,
            y=heatmap_data.columns,
            color_continuous_scale="RdYlGn",
            aspect="auto",
            title="ëª¨ë¸ë³„ ë©”íŠ¸ë¦­ ì„±ëŠ¥ íˆíŠ¸ë§µ"
        )
        
        # í…ìŠ¤íŠ¸ ì£¼ì„ ì¶”ê°€
        for i, metric in enumerate(heatmap_data.columns):
            for j, model in enumerate(heatmap_data.index):
                fig.add_annotation(
                    x=j, y=i,
                    text=f"{heatmap_data.loc[model, metric]:.3f}",
                    showarrow=False,
                    font=dict(color="black" if heatmap_data.loc[model, metric] > 0.5 else "white")
                )
        
        fig.update_layout(height=400)
        st.plotly_chart(fig, use_container_width=True)

    def _create_line_chart(self, summary_df):
        """ë¼ì¸ ì°¨íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        st.write("##### ğŸ“ˆ ë¼ì¸ ì°¨íŠ¸ - ë©”íŠ¸ë¦­ íŠ¸ë Œë“œ")
        
        metrics = ["Faithfulness", "Answer Relevancy", "Context Recall", "Context Precision"]
        
        fig = go.Figure()
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
        
        for i, model in enumerate(summary_df["Model"]):
            values = [summary_df.loc[i, metric] for metric in metrics]
            
            fig.add_trace(go.Scatter(
                x=metrics,
                y=values,
                mode='lines+markers',
                name=model,
                line=dict(color=colors[i % len(colors)], width=3),
                marker=dict(size=8)
            ))
        
        fig.update_layout(
            title="ë©”íŠ¸ë¦­ë³„ ì„±ëŠ¥ íŠ¸ë Œë“œ",
            xaxis_title="í‰ê°€ ë©”íŠ¸ë¦­",
            yaxis_title="ì ìˆ˜",
            height=400,
            yaxis=dict(range=[0, 1])
        )
        
        st.plotly_chart(fig, use_container_width=True)

    def _display_performance_insights(self, summary_df):
        """ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."""
        st.write("##### ğŸ¯ ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
        best_overall = summary_df.loc[summary_df["Overall Score"].idxmax(), "Model"]
        best_faithfulness = summary_df.loc[summary_df["Faithfulness"].idxmax(), "Model"]
        best_relevancy = summary_df.loc[summary_df["Answer Relevancy"].idxmax(), "Model"]
        best_recall = summary_df.loc[summary_df["Context Recall"].idxmax(), "Model"]
        best_precision = summary_df.loc[summary_df["Context Precision"].idxmax(), "Model"]
        
        insights_col1, insights_col2 = st.columns(2)
        
        with insights_col1:
            st.info(f"ğŸ† **ì¢…í•© ìµœê³  ì„±ëŠ¥**: {best_overall}")
            st.success(f"ğŸ¯ **ìµœê³  ì¶©ì‹¤ì„±**: {best_faithfulness}")
            st.success(f"ğŸ¯ **ìµœê³  ê´€ë ¨ì„±**: {best_relevancy}")
        
        with insights_col2:
            st.success(f"ğŸ¯ **ìµœê³  ì¬í˜„ìœ¨**: {best_recall}")
            st.success(f"ğŸ¯ **ìµœê³  ì •ë°€ë„**: {best_precision}")
        
        # ì„±ëŠ¥ ê²©ì°¨ ë¶„ì„
        st.write("##### ğŸ“Š ì„±ëŠ¥ ê²©ì°¨ ë¶„ì„")
        
        performance_gap = {}
        for metric in ["Faithfulness", "Answer Relevancy", "Context Recall", "Context Precision"]:
            max_score = summary_df[metric].max()
            min_score = summary_df[metric].min()
            gap = max_score - min_score
            performance_gap[metric] = gap
        
        gap_df = pd.DataFrame(list(performance_gap.items()), columns=["ë©”íŠ¸ë¦­", "ì„±ëŠ¥ ê²©ì°¨"])
        gap_df = gap_df.sort_values("ì„±ëŠ¥ ê²©ì°¨", ascending=False)
        
        fig = px.bar(
            gap_df,
            x="ë©”íŠ¸ë¦­",
            y="ì„±ëŠ¥ ê²©ì°¨",
            title="ë©”íŠ¸ë¦­ë³„ ëª¨ë¸ ê°„ ì„±ëŠ¥ ê²©ì°¨",
            color="ì„±ëŠ¥ ê²©ì°¨",
            color_continuous_scale="Reds"
        )
        
        fig.update_layout(height=300)
        st.plotly_chart(fig, use_container_width=True)
        
        # ê¶Œì¥ì‚¬í•­
        largest_gap_metric = gap_df.iloc[0]["ë©”íŠ¸ë¦­"]
        st.warning(f"ğŸ’¡ **ê°œì„  ê¶Œì¥ì‚¬í•­**: {largest_gap_metric}ì—ì„œ ê°€ì¥ í° ì„±ëŠ¥ ê²©ì°¨ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ì˜ì—­ì— ì§‘ì¤‘í•˜ì—¬ ëª¨ë¸ì„ ê°œì„ í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.") 