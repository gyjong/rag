"""RAGAS Evaluation UI module for the RAG application."""

import streamlit as st
import pandas as pd
from datasets import Dataset

from ..config import *
from ..graphs.naive_rag_graph import create_naive_rag_graph
from ..graphs.advanced_rag_graph import create_advanced_rag_graph
from ..graphs.modular_rag_graph import create_modular_rag_graph
from ..rag_systems.modular_rag import BM25
from ..utils.vector_store import VectorStoreManager
from ..utils.llm_manager import LLMManager
from src.config import langfuse_handler

class RagasEvaluationUI:
    """UI for RAG model evaluation using RAGAS."""

    def __init__(self):
        if "evaluation_results" not in st.session_state:
            st.session_state.evaluation_results = None
        if "evaluation_running" not in st.session_state:
            st.session_state.evaluation_running = False

    def display(self):
        """Display the RAGAS evaluation page."""
        st.title("üìä RAGAS Í≤∞Í≥º ÌèâÍ∞Ä")
        
        st.info(
            "Ïù¥ ÌéòÏù¥ÏßÄÏóêÏÑúÎäî Naive, Advanced, Modular RAG ÏãúÏä§ÌÖúÏùò ÏÑ±Îä•ÏùÑ RAGASÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ÌèâÍ∞ÄÌï©ÎãàÎã§.\n\n"
            "**ÌèâÍ∞Ä Îç∞Ïù¥ÌÑ∞ÏÖã ÌòïÏãù:**\n"
            "- `question`: Î™®Îç∏Ïóê ÏßàÏùòÌï† ÏßàÎ¨∏ (ÌïÑÏàò)\n"
            "- `ground_truth`: Ïù¥ÏÉÅÏ†ÅÏù∏ Ï†ïÎãµ (ÌïÑÏàò)\n\n"
            "ÌèâÍ∞Ä Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ ÏïÑÎûò ÌëúÏóê ÏßÅÏ†ë ÏûÖÎ†•ÌïòÍ±∞ÎÇò CSV/JSON ÌååÏùºÎ°ú ÏóÖÎ°úÎìúÌï† Ïàò ÏûàÏäµÎãàÎã§."
        )

        self._display_dataset_input()
        
        if "eval_dataset" in st.session_state and st.session_state.eval_dataset:
            is_ready = self._check_vector_store()
            if is_ready:
                self._manage_bm25_for_modular_rag()
                self._display_model_selection_and_run()
        
        self._display_results()

    def _get_llm_manager(self):
        """Initializes and returns the LLMManager based on session state."""
        return LLMManager(
            model_name=st.session_state.get("selected_llm_model", "llama3.2:latest"),
            base_url=OLLAMA_BASE_URL,
            temperature=st.session_state.get("llm_temperature", 0.1)
        )

    def _check_vector_store(self):
        """Checks if the vector store is ready for evaluation."""
        if "vector_store_manager" not in st.session_state or not st.session_state.vector_store_manager.get_vector_store():
            st.warning("RAGAS ÌèâÍ∞ÄÎ•º Ïã§ÌñâÌïòÎ†§Î©¥ Î®ºÏ†Ä 'Î¨∏ÏÑú Î°úÎî©' ÌÉ≠ÏóêÏÑú Î¨∏ÏÑúÎ•º Î°úÎìúÌïòÍ≥† 'Î≤°ÌÑ∞ Ïä§ÌÜ†Ïñ¥' ÌÉ≠ÏóêÏÑú Î≤°ÌÑ∞ Ïä§ÌÜ†Ïñ¥Î•º ÏÉùÏÑ±Ìï¥Ïïº Ìï©ÎãàÎã§.")
            return False
        st.success("‚úÖ Î≤°ÌÑ∞ Ïä§ÌÜ†Ïñ¥ Ï§ÄÎπÑ ÏôÑÎ£å. ÌèâÍ∞ÄÎ•º ÏßÑÌñâÌï† Ïàò ÏûàÏäµÎãàÎã§.")
        return True

    def _manage_bm25_for_modular_rag(self):
        """UI for managing the BM25 index required for Modular RAG."""
        with st.container(border=True):
            st.subheader("üõ†Ô∏è Modular RAG ÏÇ¨Ï†Ñ ÏÑ§Ï†ï: BM25 Ïù∏Îç±Ïä§")
            bm25_index = st.session_state.get("bm25_index")
            bm25_docs_count = len(st.session_state.get("bm25_documents", []))

            if bm25_index:
                st.success(f"‚úÖ BM25 Ïù∏Îç±Ïä§Í∞Ä Ï§ÄÎπÑÎêòÏóàÏäµÎãàÎã§. ({bm25_docs_count}Í∞ú Î¨∏ÏÑú Ïù∏Îç±Ïã±Îê®)")
                if st.button("üîÑ BM25 Ïù∏Îç±Ïä§ Ïû¨ÏÉùÏÑ±", key="ragas_regenerate_bm25"):
                    st.session_state.pop("bm25_index", None)
                    st.session_state.pop("bm25_documents", None)
                    st.rerun()
            else:
                st.warning("Modular RAGÎ•º ÌèâÍ∞ÄÌïòÎ†§Î©¥ BM25 ÌÇ§ÏõåÎìú Í≤ÄÏÉâÏùÑ ÏúÑÌïú Ïù∏Îç±Ïä§ ÏÉùÏÑ±Ïù¥ ÌïÑÏöîÌï©ÎãàÎã§.")
                if st.button("üöÄ BM25 Ïù∏Îç±Ïä§ ÏÉùÏÑ±", key="create_bm25_index_ragas"):
                    self._create_bm25_index()

    def _create_bm25_index(self):
        """Creates and stores the BM25 index in the session state."""
        try:
            with st.spinner("Î≤°ÌÑ∞ Ïä§ÌÜ†Ïñ¥ÏóêÏÑú Î¨∏ÏÑúÎ•º Î°úÎìúÌïòÏó¨ BM25 Ïù∏Îç±Ïä§Î•º ÏÉùÏÑ±Ìï©ÎãàÎã§..."):
                vector_store_manager = st.session_state.get("vector_store_manager")
                if not vector_store_manager:
                    st.error("Î≤°ÌÑ∞ Ïä§ÌÜ†Ïñ¥ Îß§ÎãàÏ†ÄÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
                    return

                vector_store = vector_store_manager.get_vector_store()
                stats = vector_store_manager.get_collection_stats()
                total_docs = stats.get("document_count", 1000)

                if total_docs == 0:
                    st.error("Ïù∏Îç±Ïã±Ìï† Î¨∏ÏÑúÍ∞Ä Î≤°ÌÑ∞ Ïä§ÌÜ†Ïñ¥Ïóê ÏóÜÏäµÎãàÎã§.")
                    return

                docs = vector_store.similarity_search("", k=total_docs)
                
                if not docs:
                    st.error("Î≤°ÌÑ∞ Ïä§ÌÜ†Ïñ¥ÏóêÏÑú Î¨∏ÏÑúÎ•º Í∞ÄÏ†∏Ïò§ÏßÄ Î™ªÌñàÏäµÎãàÎã§.")
                    return
                
                corpus = [doc.page_content for doc in docs]
                st.session_state.bm25_index = BM25(corpus)
                st.session_state.bm25_documents = docs
            st.rerun()
        except Exception as e:
            st.error(f"BM25 Ïù∏Îç±Ïä§ ÏÉùÏÑ± Ïã§Ìå®: {e}")
            st.session_state.pop("bm25_index", None)
            st.session_state.pop("bm25_documents", None)

    def _display_dataset_input(self):
        """Handles the input for the evaluation dataset."""
        st.subheader("1. ÌèâÍ∞Ä Îç∞Ïù¥ÌÑ∞ÏÖã Ï§ÄÎπÑ")

        # Initialize with sample data if not present
        if "eval_df" not in st.session_state:
            sample_data = {
                "question": ["RAGÏùò ÌïúÍ≥ÑÎäî Î¨¥ÏóáÏù∏Í∞ÄÏöî?"],
                "ground_truth": ["RAGÎäî Í≤ÄÏÉâÎêú Î¨∏ÏÑúÏùò ÌíàÏßàÏóê Îî∞Îùº ÎãµÎ≥ÄÏùò Ï†ïÌôïÏÑ±Ïù¥ ÌÅ¨Í≤å Ï¢åÏö∞ÎêòÎ©∞, Í¥ÄÎ†® ÏóÜÎäî Ï†ïÎ≥¥Í∞Ä Í≤ÄÏÉâÎêòÎ©¥ ÌôòÍ∞Å(Hallucination) ÌòÑÏÉÅÏù¥ Î∞úÏÉùÌï† Ïàò ÏûàÏäµÎãàÎã§. ÎòêÌïú, Ïã§ÏãúÍ∞Ñ Ï†ïÎ≥¥ÎÇò ÏµúÏã† Îç∞Ïù¥ÌÑ∞Î•º Î∞òÏòÅÌïòÎäî Îç∞ ÌïúÍ≥ÑÍ∞Ä ÏûàÏùÑ Ïàò ÏûàÏäµÎãàÎã§."]
            }
            st.session_state.eval_df = pd.DataFrame(sample_data)

        # Display editable dataframe
        st.session_state.eval_df = st.data_editor(
            st.session_state.eval_df,
            num_rows="dynamic",
            use_container_width=True
        )

        # File uploader
        uploaded_file = st.file_uploader("CSV ÎòêÎäî JSON ÌååÏùº ÏóÖÎ°úÎìú", type=["csv", "json"])
        if uploaded_file:
            try:
                if uploaded_file.name.endswith('.csv'):
                    df = pd.read_csv(uploaded_file)
                else:
                    df = pd.read_json(uploaded_file)
                
                # Validate columns
                if "question" not in df.columns or "ground_truth" not in df.columns:
                    st.error("Ïò§Î•ò: ÏóÖÎ°úÎìúÎêú ÌååÏùºÏóê 'question'Í≥º 'ground_truth' Ïª¨ÎüºÏù¥ Î™®Îëê Ìè¨Ìï®ÎêòÏñ¥Ïïº Ìï©ÎãàÎã§.")
                else:
                    st.session_state.eval_df = df
                    st.success(f"‚úÖ '{uploaded_file.name}' ÌååÏùºÏù¥ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Î°úÎìúÎêòÏóàÏäµÎãàÎã§.")
            except Exception as e:
                st.error(f"ÌååÏùºÏùÑ ÏùΩÎäî Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {e}")

        if not st.session_state.eval_df.empty:
            # Convert to Hugging Face Dataset
            dataset = Dataset.from_pandas(st.session_state.eval_df)
            st.session_state.eval_dataset = dataset
            st.write(f"**Îç∞Ïù¥ÌÑ∞ÏÖã Ï†ïÎ≥¥:** {len(dataset)}Í∞ú ÏßàÎ¨∏")

    def _display_model_selection_and_run(self):
        """Allows model selection and initiates the evaluation."""
        st.subheader("2. ÌèâÍ∞Ä ÎåÄÏÉÅ RAG Î™®Îç∏ ÏÑ†ÌÉù")
        
        col1, col2 = st.columns([1, 2])
        
        with col1:
            models_to_evaluate = st.multiselect(
                "ÌèâÍ∞ÄÌï† Î™®Îç∏ÏùÑ ÏÑ†ÌÉùÌïòÏÑ∏Ïöî.",
                options=["Naive RAG", "Advanced RAG", "Modular RAG"],
                default=["Naive RAG", "Advanced RAG", "Modular RAG"]
            )
        
        with col2:
            st.write(" ") # For alignment
            if st.button("üöÄ ÌèâÍ∞Ä ÏãúÏûë", type="primary", disabled=not models_to_evaluate or st.session_state.evaluation_running):
                self._run_evaluation(models_to_evaluate)

    def _run_evaluation(self, models_to_evaluate):
        """Executes the evaluation process."""
        st.session_state.evaluation_running = True
        st.session_state.evaluation_results = {}

        vector_store_manager = st.session_state.get("vector_store_manager")
        if not vector_store_manager:
            st.error("Ïò§Î•ò: Î≤°ÌÑ∞ Ïä§ÌÜ†Ïñ¥Í∞Ä Ï¥àÍ∏∞ÌôîÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§. 'Î¨∏ÏÑú Î°úÎî©' ÌÉ≠ÏóêÏÑú Î¨∏ÏÑúÎ•º Î®ºÏ†Ä Ï≤òÎ¶¨Ìï¥Ï£ºÏÑ∏Ïöî.")
            st.session_state.evaluation_running = False
            return
        
        llm_manager = self._get_llm_manager()

        # RAGAS can be slow, so let's provide a progress bar for the models
        progress_bar = st.progress(0)
        progress_text = st.empty()

        for i, model_name in enumerate(models_to_evaluate):
            progress_text.text(f"'{model_name}' ÌèâÍ∞Ä ÏßÑÌñâ Ï§ë... ({i+1}/{len(models_to_evaluate)})")
            
            with st.spinner(f"‚è≥ **{model_name}** ÌèâÍ∞Ä ÏßÑÌñâ Ï§ë... (ÏßàÎ¨∏Îì§ÏùÑ Ï≤òÎ¶¨ÌïòÍ≥† ÏûàÏäµÎãàÎã§. Îã§ÏÜå ÏãúÍ∞ÑÏù¥ Í±∏Î¶¥ Ïàò ÏûàÏäµÎãàÎã§.)"):
                try:
                    # Graph setup
                    graph = None
                    if model_name == "Naive RAG":
                        graph = create_naive_rag_graph(llm_manager, vector_store_manager)
                    
                    elif model_name == "Advanced RAG":
                        graph = create_advanced_rag_graph(llm_manager, vector_store_manager)

                    elif model_name == "Modular RAG":
                        if "bm25_index" not in st.session_state:
                            st.warning(f"BM25 Ïù∏Îç±Ïä§Í∞Ä ÏóÜÏñ¥ '{model_name}' ÌèâÍ∞ÄÎ•º Í±¥ÎÑàÎúÅÎãàÎã§. 'ÏÇ¨Ï†Ñ ÏÑ§Ï†ï'ÏóêÏÑú Ïù∏Îç±Ïä§Î•º ÏÉùÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.")
                            continue
                        bm25_index = st.session_state.bm25_index
                        bm25_documents = st.session_state.bm25_documents
                        graph = create_modular_rag_graph(llm_manager, vector_store_manager, bm25_index, bm25_documents)

                    if not graph:
                        st.error(f"'{model_name}'Ïóê ÎåÄÌïú Í∑∏ÎûòÌîÑÎ•º ÏÉùÏÑ±Ìï† Ïàò ÏóÜÏäµÎãàÎã§.")
                        continue
                        
                    # Run inference and collect results
                    results_list = []
                    for item in st.session_state.eval_dataset:
                        inputs = {"query": item["question"]}
                        # Add specific inputs for each graph type
                        if model_name == "Naive RAG":
                            inputs["k"] = 5
                        elif model_name == "Advanced RAG":
                            inputs.update({"k": 10, "rerank_top_k": 5})
                        elif model_name == "Modular RAG":
                            inputs["max_iterations"] = 2
                        
                        response = graph.invoke(inputs, config={"callbacks": [langfuse_handler]})
                        
                        # Extract answer and context based on graph's output keys
                        answer = response.get("answer", "")
                        if model_name == "Naive RAG":
                            contexts = response.get("documents", [])
                        elif model_name == "Advanced RAG":
                            contexts = response.get("reranked_docs", [])
                        elif model_name == "Modular RAG":
                            contexts = response.get("all_retrieved_docs", [])
                        else:
                            contexts = []
                            
                        results_list.append({
                            "question": item["question"],
                            "answer": answer,
                            "contexts": [doc.page_content for doc in contexts],
                            "ground_truth": item["ground_truth"],
                        })

                    result_dataset = Dataset.from_list(results_list)
                    
                    # Perform RAGAS evaluation
                    from ragas import evaluate
                    from ragas.metrics import faithfulness, answer_relevancy, context_recall, context_precision
                    
                    score = evaluate(
                        result_dataset,
                        metrics=[faithfulness, answer_relevancy, context_recall, context_precision],
                    )
                    
                    st.session_state.evaluation_results[model_name] = score.to_pandas()
                    st.success(f"‚úÖ **{model_name}** ÌèâÍ∞Ä ÏôÑÎ£å!")
                
                except Exception as e:
                    st.error(f"**{model_name}** ÌèâÍ∞Ä Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}")
            
            progress_bar.progress((i + 1) / len(models_to_evaluate))

        progress_text.text("Î™®Îì† ÌèâÍ∞Ä ÏôÑÎ£å!")
        st.session_state.evaluation_running = False

    def _display_results(self):
        """Displays the evaluation results."""
        if st.session_state.evaluation_results:
            st.subheader("3. ÌèâÍ∞Ä Í≤∞Í≥º")
            
            for model_name, result_df in st.session_state.evaluation_results.items():
                with st.expander(f"#### {model_name} ÏÉÅÏÑ∏ Í≤∞Í≥º", expanded=True):
                    st.dataframe(result_df, use_container_width=True)
                    
                    # Display average scores
                    avg_scores = result_df.mean(numeric_only=True)
                    st.write("##### üìà ÌèâÍ∑† Ï†êÏàò")
                    st.dataframe(avg_scores.to_frame("ÌèâÍ∑† Ï†êÏàò").T, use_container_width=True)
            
            # Comparative summary
            if len(st.session_state.evaluation_results) > 1:
                st.subheader("üìä Î™®Îç∏Î≥Ñ Ï¢ÖÌï© ÎπÑÍµê")
                summary_data = {
                    "Model": [],
                    "Faithfulness": [],
                    "Answer Relevancy": [],
                    "Context Recall": [],
                    "Context Precision": [],
                }
                for model_name, result_df in st.session_state.evaluation_results.items():
                    avg_scores = result_df.mean(numeric_only=True)
                    summary_data["Model"].append(model_name)
                    summary_data["Faithfulness"].append(avg_scores.get("faithfulness", 0))
                    summary_data["Answer Relevancy"].append(avg_scores.get("answer_relevancy", 0))
                    summary_data["Context Recall"].append(avg_scores.get("context_recall", 0))
                    summary_data["Context Precision"].append(avg_scores.get("context_precision", 0))
                
                summary_df = pd.DataFrame(summary_data)
                st.dataframe(summary_df.set_index("Model"), use_container_width=True)
                
                st.bar_chart(summary_df.set_index("Model")) 